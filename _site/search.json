[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Overview",
    "section": "",
    "text": "This collection of handouts provides a hands-on introduction to data analysis and statistical methods in quantitative corpus linguistics with R. It is designed with accessibility in mind, assuming no prior knowledge of programming or statistics. All you need to get started is a laptop; everything else will be explained within these pages.\nPrimarily, this reader is geared towards students attending the classes Language Variation (BA) and Statistics for Linguistics (MA) at the Catholic University of Eichstätt-Ingolstadt (Germany). However, it is also meant to equip students currently working on their BA/MA/PhD theses with the tools they need to conduct empirical studies on a wide array of linguistic phenomena. The methods presented here reflect the state-of-the-art in corpus-linguistic research, providing readers with current and relevant analytical techniques."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Exploring_RStudio.html",
    "href": "Exploring_RStudio.html",
    "title": "Exploring RStudio",
    "section": "",
    "text": "Once you’ve opened RStudio, you will see several empty panes similar to this:\n\n\n\n\n\n\n\nLet’s focus on the console window on the left. This is where we can directly communicate with R by entering “code”. The way this code is written follows certain arbitrary conventions – just like natural languages such as English or German. Here is an example in the R programming language:\n\nprint(\"Hello world!\")\n\n[1] \"Hello world!\"\n\n\nEntering this command into your console and pressing ENTER will display the sentence “Hello world!” in a new line.\nAs we can see, anything that R should understand as a simple sequence of letters or words must be enclosed by quotation marks \"...\". Anything inside them will be interpreted as a so-called string. Their counterpart are numbers or integers, as illustrated here:\n\n2 + 2 - 1\n\n[1] 3\n\n\nNaturally, you can use R for more sophisticated computations:\n\n\\(2 + (3 \\times 3)\\)\n\n2 + (3 * 3)\n\n[1] 11\n\n\n\\(\\sqrt{9}\\)\n\nsqrt(9)\n\n[1] 3\n\n\n\\(\\frac{16}{2^3}\\)\n\n16 / 2 ^ 3\n\n[1] 2\n\n\n\n\n\n\nWhile it is certainly useful to print text or numbers to your console, it may sometimes make more sense to (at least temporally) store them somewhere, so you can re-use them later. In fact, R gives us a way of storing data in virtual, container-like objects: variables. We can assign strings or numbers to a variable by using the assignment operator &lt;-.\nWhen you run the commands below, you will (hopefully) notice two items popping up in your Environment/Workspace tab in the top right corner.\n\ngreeting &lt;- \"Hello world!\"\n\nquick_math &lt;- 2 + 2 - 1\n\nIf we now want to display the content in the console, we can simply apply the print() function to the variable:\n\nprint(greeting)\n\n[1] \"Hello world!\"\n\nprint(quick_math)\n\n[1] 3\n\n\nWe can also embed variables in other statements. For example, let’s take the content of quick_math and multiply it with 2.\n\nhard_math &lt;- quick_math * 2\n\nprint(hard_math)\n\n[1] 6\n\n\n\n\n\nWorking with the console has a very plain, yet important disadvantage: Once we close RStudio, the console is wiped clean, erasing everything we’ve typed into it during our precious R session.\nThe remedy for this nuisance are scripts. Essentially, a script is the programmer’s equivalent of a Word document: It allows you to save all the code you’ve written to a file, which you can seamlessly continue working on the next time you open it.\n\nTo create a new R script you can either go to:\n\n“File” \\(\\rightarrow\\) “New” \\(\\rightarrow\\) “R Script” or …\nclick on the icon with the + sign and select “R Script” …\nor simply press Ctrl+Shift+N (MacOS: Cmd+Shift+N)\n\n\nDon’t forget to save your script with Ctrl+S (Cmd+S)! It is good practice to save your files regularly."
  },
  {
    "objectID": "Exploring_RStudio.html#the-rstudio-interface",
    "href": "Exploring_RStudio.html#the-rstudio-interface",
    "title": "Exploring RStudio",
    "section": "",
    "text": "Once you’ve opened RStudio, you will see several empty panes similar to this:\n\n\n\n\n\n\n\nLet’s focus on the console window on the left. This is where we can directly communicate with R by entering “code”. The way this code is written follows certain arbitrary conventions – just like natural languages such as English or German. Here is an example in the R programming language:\n\nprint(\"Hello world!\")\n\n[1] \"Hello world!\"\n\n\nEntering this command into your console and pressing ENTER will display the sentence “Hello world!” in a new line.\nAs we can see, anything that R should understand as a simple sequence of letters or words must be enclosed by quotation marks \"...\". Anything inside them will be interpreted as a so-called string. Their counterpart are numbers or integers, as illustrated here:\n\n2 + 2 - 1\n\n[1] 3\n\n\nNaturally, you can use R for more sophisticated computations:\n\n\\(2 + (3 \\times 3)\\)\n\n2 + (3 * 3)\n\n[1] 11\n\n\n\\(\\sqrt{9}\\)\n\nsqrt(9)\n\n[1] 3\n\n\n\\(\\frac{16}{2^3}\\)\n\n16 / 2 ^ 3\n\n[1] 2\n\n\n\n\n\n\nWhile it is certainly useful to print text or numbers to your console, it may sometimes make more sense to (at least temporally) store them somewhere, so you can re-use them later. In fact, R gives us a way of storing data in virtual, container-like objects: variables. We can assign strings or numbers to a variable by using the assignment operator &lt;-.\nWhen you run the commands below, you will (hopefully) notice two items popping up in your Environment/Workspace tab in the top right corner.\n\ngreeting &lt;- \"Hello world!\"\n\nquick_math &lt;- 2 + 2 - 1\n\nIf we now want to display the content in the console, we can simply apply the print() function to the variable:\n\nprint(greeting)\n\n[1] \"Hello world!\"\n\nprint(quick_math)\n\n[1] 3\n\n\nWe can also embed variables in other statements. For example, let’s take the content of quick_math and multiply it with 2.\n\nhard_math &lt;- quick_math * 2\n\nprint(hard_math)\n\n[1] 6\n\n\n\n\n\nWorking with the console has a very plain, yet important disadvantage: Once we close RStudio, the console is wiped clean, erasing everything we’ve typed into it during our precious R session.\nThe remedy for this nuisance are scripts. Essentially, a script is the programmer’s equivalent of a Word document: It allows you to save all the code you’ve written to a file, which you can seamlessly continue working on the next time you open it.\n\nTo create a new R script you can either go to:\n\n“File” \\(\\rightarrow\\) “New” \\(\\rightarrow\\) “R Script” or …\nclick on the icon with the + sign and select “R Script” …\nor simply press Ctrl+Shift+N (MacOS: Cmd+Shift+N)\n\n\nDon’t forget to save your script with Ctrl+S (Cmd+S)! It is good practice to save your files regularly."
  },
  {
    "objectID": "chapters/01-index.html",
    "href": "chapters/01-index.html",
    "title": "Preface",
    "section": "",
    "text": "This collection of handouts provides a hands-on introduction to data analysis and statistical methods in quantitative corpus linguistics with R. It is designed with accessibility in mind, assuming no prior knowledge of programming or statistics. All you need to get started is a laptop; everything else will be explained within these pages.\nPrimarily, this reader is geared towards students attending the classes Language Variation (BA) and Statistics for Linguistics (MA) at the Catholic University of Eichstätt-Ingolstadt (Germany). However, it is also meant to equip students currently working on their BA/MA/PhD theses with the tools they need to conduct empirical studies on a wide array of linguistic phenomena. The methods presented here reflect the state-of-the-art in corpus-linguistic research, providing readers with current and relevant analytical techniques."
  },
  {
    "objectID": "chapters/01-index.html#overview",
    "href": "chapters/01-index.html#overview",
    "title": "Preface",
    "section": "Overview",
    "text": "Overview\nWe begin by establishing the general structure of corpus-linguistic studies, accompanied by key theoretical and practical considerations. The second section introduces R as an analytical tool, covering its core functionality. With these fundamentals in place, we proceed to query corpora directly within R. Chapters four and five focus on describing discrete and continuous outputs, as well as identifying meaningful associations and differences in the data. Finally, to gain more nuanced insights into potential patterns, we apply common machine learning algorithms to fit, evaluate, and interpret statistical models.\nThroughout this journey, readers will develop the skills to conduct robust corpus-linguistic analyses, from basic querying to advanced statistical modeling."
  },
  {
    "objectID": "chapters/01-index.html#collaborators",
    "href": "chapters/01-index.html#collaborators",
    "title": "Preface",
    "section": "1 Collaborators",
    "text": "1 Collaborators\nVladimir Buskin, PhD student at the Department of English Language and Linguistics, Catholic University of Eichstätt-Ingolstadt\nThomas Brunner, Assistant Professor at the Department of English Language and Linguistics, Catholic University of Eichstätt-Ingolstadt\nPhilippa Adolf, PhD student at the Department of Romance Studies, University of Vienna"
  },
  {
    "objectID": "chapters/04-chi_square_test.html",
    "href": "chapters/04-chi_square_test.html",
    "title": "Chi-squared test",
    "section": "",
    "text": "For linguists:\n\nGries (2021): Chapter 4.1.2.1\n\nGeneral:\n\nBaguley (2012): Chapter 4\nAgresti and Kateri (2022): Chapter 5"
  },
  {
    "objectID": "chapters/04-chi_square_test.html#suggested-reading",
    "href": "chapters/04-chi_square_test.html#suggested-reading",
    "title": "Chi-squared test",
    "section": "",
    "text": "For linguists:\n\nGries (2021): Chapter 4.1.2.1\n\nGeneral:\n\nBaguley (2012): Chapter 4\nAgresti and Kateri (2022): Chapter 5"
  },
  {
    "objectID": "chapters/04-chi_square_test.html#preparation",
    "href": "chapters/04-chi_square_test.html#preparation",
    "title": "Chi-squared test",
    "section": "2 Preparation",
    "text": "2 Preparation\n\n\n\n\n\n\nScript\n\n\n\nYou can find the full R script associated with this unit here.\n\n\n\n#  Load libraries\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(confintr) # for effect size calculation\n\n# Load data\ncl.order &lt;- read_xlsx(\"../datasets/Paquot_Larsson_2020_data.xlsx\")"
  },
  {
    "objectID": "chapters/04-chi_square_test.html#the-pearson-chi2-test-of-independence",
    "href": "chapters/04-chi_square_test.html#the-pearson-chi2-test-of-independence",
    "title": "Chi-squared test",
    "section": "3 The Pearson \\(\\chi^2\\)-test of independence",
    "text": "3 The Pearson \\(\\chi^2\\)-test of independence\nThe first step of any significance test involves setting up the null and alternative hypothesis. In this unit, we will perform a sample analysis on the relationship between clause ORDER and the type of subordinate clause (SUBORDTYPE). Specifically, we will focus on the independence of these two discrete variables (i.e, the presence or absence of correlation).\n\n\\(H_0:\\) The variables ORDER and SUBORDTYPE are independent.\n\\(H_1:\\) The variables ORDER and SUBORDTYPE are not independent.\n\n\n\n\n\n\n\nWhat does independence really mean?\n\n\n\n\n\nThe core idea is “that the probability distribution of the response variable is the same for each group” (Agresti and Kateri 2022: 177). If clause ORDER is the response variable and SUBORDTYPE the explanatory variable, independence would entail that the probabilities of the outcomes of the response variable ORDER = \"mc-sc\" and ORDER = \"sc-mc\" are not influenced by whether they occur in the groups SUBORDTYPE = \"temp\" or SUBORDTYPE = \"caus\".\nThe term probability distribution refers to a mathematical function that assigns probabilities to the outcomes of a variable. If we consider two variables at the same time, such as \\(X\\) and \\(Y\\), they are said to have marginal probability functions \\(f_1(x)\\) and \\(f_2(y)\\). If we condition the outcomes of all values on each other, the following equivalence will hold:\n\\[\nf(x \\mid y) = f_1(x) \\text{ and } f(y \\mid x) = f_2(y).\n\\tag{1}\\]\nThus, the null hypothesis assumes that the probabilities of each combination of values (such as ORDER and SUBORDTYPE), denoted by \\(\\pi_{ij}\\), have the relationship in Equation 1. This can be stated succinctly as\n\\[\nH_0 : \\pi_{ij} = P(X = j)P(Y = i).\n\\tag{2}\\]\n\n\n\nNext, we compute a test statistic that indicates how strongly our data conforms to \\(H_0\\), such as Pearson’s \\(\\chi^2\\). To this end, we will need two types of values:\n\nthe observed frequencies \\(f_{ij}\\) present in our data set\nthe expected frequencies \\(e_{ij}\\), which we would expect to see if \\(H_0\\) were true,\n\nwhere \\(f, e \\in \\mathbb{N}\\). The indices \\(i\\) and \\(j\\) uniquely identify the cell counts in all column-row combinations of a contingency table.\n\n3.1 Observerd frequencies\nThe table below represents a generic contingency table where \\(Y\\) and \\(X\\) are categorical variables and have the values \\(Y = \\{y_1, y_2, \\dots, y_i \\}\\) and \\(X = \\{x_1, x_2, \\dots, x_j\\}\\). In the table, each cell indicates the count of observation \\(f_{ij}\\) corresponding to the \\(i\\)-th row and \\(j\\)-th column.\n\n\n\n\n\n\n\\(X\\)\n\n\n\n\n\n\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n…\n\\(x_j\\)\n\n\n\n\n\\(y_1\\)\n\\(f_{11}\\)\n\\(f_{12}\\)\n…\n\\(f_{1j}\\)\n\n\n\n\n\\(y_2\\)\n\\(f_{21}\\)\n\\(f_{22}\\)\n…\n\\(f_{2j}\\)\n\n\n\n\\(Y\\)\n…\n…\n…\n…\n…\n\n\n\n\n\\(y_i\\)\n\\(f_{i1}\\)\n\\(f_{i2}\\)\n…\n\\(f_{ij}\\)\n\n\n\n\n\nIn the cl.order data, the observed frequencies correspond to how often each ORDER value (i.e., mc-sc and sc-mc) is attested for a given SUBORDTYPE (i.e., temp and caus). This can be done in a very straightforward fashion using R’s table() function on the variables of interest.\n\nobserved_freqs &lt;- table(cl.order$ORDER, cl.order$SUBORDTYPE)\n\nprint(observed_freqs)\n\n       \n        caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n\n\n\n\n3.2 Expected frequencies\nThe expected frequencies require a few additional steps. Usually, these steps are performed automatically when conducting the chi-squared test in R, so you don’t have to worry about calculating them by hand. We will do it anyway to drive home the rationale of the test.\n\n\n\n\n\n\n\\(X\\)\n\n\n\n\n\n\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n…\n\\(x_j\\)\n\n\n\n\n\\(y_1\\)\n\\(e_{11}\\)\n\\(e_{12}\\)\n…\n\\(e_{1j}\\)\n\n\n\n\n\\(y_2\\)\n\\(e_{21}\\)\n\\(e_{22}\\)\n…\n\\(e_{2j}\\)\n\n\n\n\\(Y\\)\n…\n…\n…\n…\n…\n\n\n\n\n\\(y_i\\)\n\\(e_{i1}\\)\n\\(e_{i2}\\)\n…\n\\(e_{ij}\\)\n\n\n\n\n\nThe expected frequencies \\(e_{ij}\\) are given by the formula in Equation 3. In concrete terms, we go through each cell in the cross-table and multiply the corresponding row sums with the column sums, dividing the result by the total number of occurrences in the sample. For example, there are \\(184\\) occurrences of mc-sc clause orders where the subordinate clause is causal. The row sum is \\(184 + 91 = 275\\) and the column sum is \\(184 + 15 = 199\\). Next, we take their product \\(275 \\times 199\\) and divide it by the total number of observations, which is \\(184 + 91 + 15 + 113 = 403\\). Thus we obtain an expected frequency of \\(\\frac{275 \\times 199}{403} = 135.79\\) under the null hypothesis.\n\\[\ne_{ij} = \\frac{i\\textrm{th row sum} \\times j \\textrm{th column sum}}{\\textrm{number of observations}}\n\\tag{3}\\]\nThe expected frequencies for our combination of variables is shown below. In which cells can you see the greatest deviations between observed and expected frequencies?\n\n\nShow the code\n## Calculate row totals\nrow_totals &lt;- rowSums(observed_freqs)\n\n## Calculate column totals\ncol_totals &lt;- colSums(observed_freqs)\n\n## Total number of observations\ntotal_obs &lt;- sum(observed_freqs)\n\n## Calculate expected frequencies\nexpected_freqs &lt;- outer(row_totals, col_totals) / total_obs\n\nprint(expected_freqs)\n\n\n           caus      temp\nmc-sc 135.79404 139.20596\nsc-mc  63.20596  64.79404\n\n\n\n\n3.3 Conducting the test\nThe \\(\\chi^2\\)-test now offers a convenient way of quantifying the differences between the two tables above. It measures how much the observed frequencies deviate from the expected frequencies for each cell in a contingency table (cf. Heumann, Schomaker, and Shalabh 2022: 249-251). The gist of this procedure is summarised in Equation 4.\n\\[\n\\text{Chi-squared } \\chi^2 =\\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\n\\tag{4}\\]\n\n\n\n\n\n\nFormal definition of the \\(\\chi^2\\)-test\n\n\n\n\n\nGiven \\(n\\) observations and \\(k\\) degrees of freedom \\(df\\), the joint squared deviations between \\(f_{ij}\\) and \\(e_{ij}\\) contribute to the final \\(\\chi^2\\)-score, which is defined as\n\\[\n\\chi^2 = \\sum_{i=1}^{I}\\sum_{j=1}^{J}{\\frac{(f_{ij} - e_{ij})^2}{e_{ij}}}\n\\tag{5}\\]\nfor \\(i = 1, ..., I\\) and \\(j = 1, ..., J\\) and \\(df = (\\textrm{number of rows} -1) \\times (\\textrm{number of columns} - 1)\\).\nApplying the formula, the updated contingency would have the following form:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\)\n\n\n\n\n\n\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n…\n\\(x_j\\)\n\n\n\n\n\\(y_1\\)\n\\(\\frac{(f_{11} - e_{11})^2}{e_{11}}\\)\n\\(\\frac{(f_{12} - e_{12})^2}{e_{12}}\\)\n…\n\\(\\frac{(f_{1j} - e_{1j})^2}{e_{1j}}\\)\n\n\n\n\n\\(y_2\\)\n\\(\\frac{(f_{21} - e_{21})^2}{e_{21}}\\)\n\\(\\frac{(f_{22} - e_{22})^2}{e_{22}}\\)\n…\n\\(\\frac{(f_{2j} - e_{2j})^2}{e_{2j}}\\)\n\n\n\n\\(Y\\)\n…\n…\n…\n…\n…\n\n\n\n\n\\(y_i\\)\n\\(\\frac{(f_{i1} - e_{i1})^2}{e_{i1}}\\)\n\\(\\frac{(f_{i2} - e_{i1})^2}{e_{i2}}\\)\n…\n\\(\\frac{(f_{ij} - e_{ij})^2}{e_{ij}}\\)\n\n\n\n\n\n\n\nThe implementation in R is a simple one-liner. Keep in mind that we have to supply absolute frequencies to chisq.test() rather than percentages.\n\nfreqs_test &lt;- chisq.test(observed_freqs)\n\nprint(freqs_test)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  observed_freqs\nX-squared = 104.24, df = 1, p-value &lt; 2.2e-16\n\n\nQuite conveniently, the test object freqs_test stores the expected frequencies, which can be easily accessed via subsetting. Luckily, they are identical to what we calculated above!\n\nfreqs_test$expected\n\n       \n             caus      temp\n  mc-sc 135.79404 139.20596\n  sc-mc  63.20596  64.79404\n\n\n\n\n3.4 Assumptions of the chi-squared test\nThis \\(\\chi^2\\)-test comes with certain statistical assumptions. Violations of these assumptions decrease the validity of the result and could, therefore, lead to wrong conclusions about relationships in the data. In this case, other tests should be consulted.\n\n\n\n\n\n\nImportant\n\n\n\n\nAll observations are independent of each other.\n80% of the expected frequencies are \\(\\geq\\) 5.\nAll observed frequencies are \\(\\geq\\) 1.\n\n\n\nIn case of dependent observations (e.g., multiple measurements per participant), the default approach is to fit a multilevel model that can control for grouping factors (see mixed-effects regression in ?@sec-mer.)\nIf assumptions 2 and 3 are violated, it is recommended to use a more robust test such as the Fisher’s Exact Test or the log-likelihood ratio test (\\(G\\)-test).\n\n\n\n\n\n\nFisher’s Exact Test\n\n\n\n\n\nWhile the \\(\\chi^2\\)-test can only approximate the \\(p\\)-value, Fisher’s Exact Test can provide an exact solution. Note that for anything more complex than a \\(2 \\times 2\\) table, it becomes considerably more computationally expensive; if it takes too long, set simulate.p.value = TRUE.\nDrawing on the hypergeometric distribution (see ?dhyper()), it computes the probability of all frequency tables that are equal or more extreme than the one observed.\n\nfisher.test(observed_freqs)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  observed_freqs\np-value &lt; 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  8.216959 29.548120\nsample estimates:\nodds ratio \n  15.11768 \n\n\n\n\n\n\n\n\n\n\n\n\\(G\\)-test\n\n\n\n\n\nThe \\(G^2\\)-statistic is analogous to \\(\\chi^2\\), but it tends to be more robust for lower observed counts. It is defined as\n\\[\nG^2 = 2\\sum_{i=1}^{I}\\sum_{j=1}^{J} f_{ij} \\ln\\left({\\frac{f_{ij}}{e_{ij}}}\\right)\n\\tag{6}\\]\nand implemented in R via the DescTools package.\n\n# Load library (install if necessary)\nlibrary(DescTools)\n\n# Perform G-test (preferably for tables with more than 2 rows/columns)\nGTest(observed_freqs)\n\n\n    Log likelihood ratio (G-test) test of independence without correction\n\ndata:  observed_freqs\nG = 116.97, X-squared df = 1, p-value &lt; 2.2e-16\n\n\n\n\n\n\n\n3.5 How do I make sense of the test results?\nThe test output has three ‘ingredients’:\n\nthe chi-squared score (X-squared)\nthe degrees of freedom (df)\nthe p-value.\n\nIt is absolutely essential to report all three of those as they determine each other. Here a few possible wordings that could be used:\n\nAccording to a \\(\\chi^2\\)-test, there is a highly significant association between clause ORDER and SUBORDTYPE at \\(p &lt; 0.001\\) (\\(\\chi^2 = 106.44, df = 1\\)), thus justifying the rejection of \\(H_0\\).\n\n\nA \\(\\chi^2\\)-test revealed a highly significant association between clause ORDER and SUBORDTYPE (\\(\\chi^2(1) = 106.44\\), \\(p &lt; 0.001\\)), supporting the rejection of \\(H_0\\).\n\n\nThe \\(\\chi^2\\)-test results (\\(\\chi^2 = 106.44\\), \\(df = 1\\), \\(p &lt; 0.001\\)) provide strong evidence against the null hypothesis, demonstrating a significant association between clause ORDER and SUBORDTYPE.\n\nThe test results suggest that the dependent variable ORDER and the explanatory variable SUBORDTYPE are not independent of each other. The probability of randomly observing usage patterns such as those found in the cl.order data is lower than 0.001 \\(\\approx\\) 0.1%, which is enough to reject the null hypothesis at \\(\\alpha = 0.05\\).\nWe can infer that a speaker’s choice of clause ORDER is very likely influenced by the semantic type of subordinate clause; in other words, these two variables are correlated. However, there are still several things the test does not tell us:\n\nAre there certain variable combinations where the \\(\\chi^2\\)-scores are particularly high?\nHow strongly do ORDER and SUBORDTYPE influence each other?"
  },
  {
    "objectID": "chapters/04-chi_square_test.html#pearson-residuals",
    "href": "chapters/04-chi_square_test.html#pearson-residuals",
    "title": "Chi-squared test",
    "section": "4 Pearson residuals",
    "text": "4 Pearson residuals\nIf we’re interested in what cells show the greatest difference between observed and expected frequencies, an option would be to inspect the Pearson residuals (cf. Equation 7).\n\\[ \\text{residuals} = \\frac{\\text{observed} - \\text{expected}}{\\sqrt{\\text{expected}}}\n\\tag{7}\\]\nThese can be accessed via the test results stored freqs_test.\n\nfreqs_test$residuals\n\n       \n             caus      temp\n  mc-sc  4.136760 -4.085750\n  sc-mc -6.063476  5.988708\n\n\nThe function assocplot() can automatically compute the pearson residuals for any given contingency table and create a plot that highlights their contributions. If the bar is above the dashed line, it is black and indicates that a category is observed more frequently than expected (e.g., causal subordinate clauses in the mc-sc order). Conversely, bars are coloured grey if a category is considerably less frequent than expected, such as caus in sc-mc.\n\nassocplot(t(observed_freqs), col = c(\"black\", \"lightgrey\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting the residuals: Configural Frequency Analysis\n\n\n\n\n\nThe chi-squared test only provides a \\(p\\)-value for the entire contingency table. But what if we wanted to test the residuals for their significance as well? Configural Frequency Analysis (Krauth and Lienert 1973) allows us to do exactly that: It performs a significance test for all combinations of variable values in a cross-table. Moreover, CFA is not limited to two variables only. Technically, users can test for associations between arbitrary numbers of variables, but should be aware of the increasing complexity of interpretation.\n\nlibrary(cfa) # install library beforehand\n\n# Get the observed counts and convert them to a data frame\nconfig_df &lt;- as.data.frame(observed_freqs)\n\n# Convert to matrix\nconfigs &lt;- as.matrix(config_df[, 1:2])  # first two columns contain the configurations (= combinations of variable values)\ncounts &lt;- config_df$Freq # Freq column contains the corresponding counts\n\n# Perform CFA on configuarations and counts; apply Bonferroni correction for multiple testing\ncfa_output &lt;- cfa(configs, counts, bonferroni = TRUE)\n\n# Print output\nprint(cfa_output)\n\n\n*** Analysis of configuration frequencies (CFA) ***\n\n       label   n  expected         Q    chisq      p.chisq sig.chisq         z\n1 sc-mc caus  15  63.20596 0.1418682 36.76575 1.332103e-09      TRUE -6.671872\n2 sc-mc temp 113  64.79404 0.1425343 35.86463 2.115143e-09      TRUE  6.469444\n3 mc-sc caus 184 135.79404 0.1804075 17.11278 3.522441e-05      TRUE  5.027611\n4 mc-sc temp  91 139.20596 0.1827409 16.69335 4.393467e-05      TRUE -5.102385\n           p.z sig.z\n1 1.000000e+00  TRUE\n2 4.918210e-11  TRUE\n3 2.483137e-07  TRUE\n4 9.999998e-01  TRUE\n\n\nSummary statistics:\n\nTotal Chi squared         =  106.4365 \nTotal degrees of freedom  =  1 \np                         =  0 \nSum of counts             =  403 \n\nLevels:\n\nVar1 Var2 \n   2    2"
  },
  {
    "objectID": "chapters/04-chi_square_test.html#effect-size",
    "href": "chapters/04-chi_square_test.html#effect-size",
    "title": "Chi-squared test",
    "section": "5 Effect size",
    "text": "5 Effect size\nThe \\(p\\)-value only indicates the presence of correlation, but not its strength – regardless of how low it may be. It does not convey how much two variables determine each other. For this reason, it is highly recommended to report an effect size measure alongside the \\(p\\)-value. One such measure is Cramér’s \\(V\\), which takes values in the interval \\([0, 1]\\):\n\\[\nV = \\sqrt{\\frac{\\chi^2}{n \\times (min(nrow, ncol) - 1)}}.\n\\tag{8}\\]\nThe package confintr implements this in its cramersv() function:\n\ncramersv(freqs_test)\n\n[1] 0.5085863\n\n\nThe association between two categorical variables is stronger, the closer \\(V\\) approximates 1. Conversely, if \\(V = 0\\), then the variables are completely independent. There are various guidelines in the literature that provide thresholds for “small”, “moderate” and “large” effects, yet these are rarely justified on theoretical grounds and could be viewed as arbitrary."
  },
  {
    "objectID": "chapters/04-chi_square_test.html#only-one-variable-the-pearson-chi2-goodness-of-fit-test",
    "href": "chapters/04-chi_square_test.html#only-one-variable-the-pearson-chi2-goodness-of-fit-test",
    "title": "Chi-squared test",
    "section": "6 Only one variable? The Pearson \\(\\chi^2\\) goodness-of-fit test",
    "text": "6 Only one variable? The Pearson \\(\\chi^2\\) goodness-of-fit test\nThe \\(\\chi^2\\)-statistic can also be utilised in simpler scenarios that involve only one single categorical variable.1 Consider the small eat_obj_aspect.xlsx dataset, where the object_realisation column indicates whether the object was realised or dropped in a given observation.\n\n# Load data\neat &lt;- read_xlsx(\"../datasets/eat_obj_aspect.xlsx\")\n\n# Show object realisation pattern\neat_observed &lt;- table(eat$object_realisation)\n\nprint(eat_observed)\n\n\n no yes \n 57  45 \n\n\nWe can use the \\(\\chi^2\\) goddness-of-fit test “to compare an observed frequency distribution against its expected probability of occurrence” (Baguley 2012: 132). In other words, we can check if the observed object data matches the frequencies we’d expect to see if both outcomes of object realisation were equally likely and hence randomly distributed.\nOur hypotheses are thus\n\n\\(H_0:\\) Observed frequencies of object realisation \\(=\\) expected frequencies of object realisation\n\\(H_1:\\) Observed frequencies of object realisation \\(\\neq\\) expected frequencies of object realisation\n\nThe expected frequencies are given by\n\\[\ne_i = \\frac{\\text{number of observations}}{\\text{number of cells}},\n\\tag{9}\\]\nand the test statistic simplifies to\n\\[\n\\chi^2 = \\sum_{i=1}^{I}{\\frac{(f_{i} - e_{i})^2}{e_{i}}}.\n\\tag{10}\\]\nIn R:\n\n# Perform the chi-squared goodness-of-fit test\neat_gof &lt;-  chisq.test(eat_observed)\n\nprint(eat_gof)\n\n\n    Chi-squared test for given probabilities\n\ndata:  eat_observed\nX-squared = 1.4118, df = 1, p-value = 0.2348\n\n# Check expected frequencies (should be greater or equal to 5)\nprint(eat_gof$expected)\n\n no yes \n 51  51 \n\n\n\nA chi-squared goodness-of-fit test indicates that the distribution of object realisation for eat does not significantly differ from an equal distribution (\\(\\chi^2\\) = 1.41, \\(df\\) = 1, \\(p\\) = 0.23). In the sample at hand, the verb eat does not seem to prefer one variant or the other."
  },
  {
    "objectID": "chapters/04-chi_square_test.html#exercises",
    "href": "chapters/04-chi_square_test.html#exercises",
    "title": "Chi-squared test",
    "section": "7 Exercises",
    "text": "7 Exercises\n\n\n\n\n\n\nSolutions\n\n\n\nYou can find the solutions to the exercises here.\n\n\n\nExercise 1 Load the file eat_obj_aspect.xlsx into R. It contains hits for the verb lemma eat which were annotated for the presence or absence of a direct object (object_realisation column) and for the aspect of the verb (verb_aspect column).\n\nCreate a frequency table that cross-classifies object_realisation and verb_aspect.\nForward a set of statistical hypotheses.\nPerform a \\(\\chi^2\\)-test of independence and interpret the result.\n\n\n\nExercise 2 The chisq.test() returns a warning message: “Chi-squared approximation may be incorrect”. Examine the frequency table for potential violations of the test assumptions. How could you control for them?"
  },
  {
    "objectID": "chapters/03-exploring_rstudio.html",
    "href": "chapters/03-exploring_rstudio.html",
    "title": "Exploring RStudio",
    "section": "",
    "text": "Once you’ve opened RStudio, you will see several empty panes similar to this:\n\n\n\n\n\n\n\nLet’s focus on the console window on the left. This is where we can directly communicate with R by entering “code”. The way this code is written follows certain arbitrary conventions – just like natural languages such as English or German. Here is an example in the R programming language:\n\nprint(\"Hello world!\")\n\n[1] \"Hello world!\"\n\n\nEntering this command into your console and pressing ENTER will display the sentence “Hello world!” in a new line.\nAs we can see, anything that R should understand as a simple sequence of letters or words must be enclosed by quotation marks \"...\". Anything inside them will be interpreted as a so-called string. Their counterpart are numbers or integers, as illustrated here:\n\n2 + 2 - 1\n\n[1] 3\n\n\nNaturally, you can use R for more sophisticated computations:\n\n\\(2 + (3 \\times 3)\\)\n\n2 + (3 * 3)\n\n[1] 11\n\n\n\\(\\sqrt{9}\\)\n\nsqrt(9)\n\n[1] 3\n\n\n\\(\\frac{16}{2^3}\\)\n\n16 / 2 ^ 3\n\n[1] 2\n\n\n\n\n\n\nWhile it is certainly useful to print text or numbers to your console, it may sometimes make more sense to (at least temporally) store them somewhere, so you can re-use them later. In fact, R gives us a way of storing data in virtual, container-like objects: variables. We can assign strings or numbers to a variable by using the assignment operator &lt;-.\nWhen you run the commands below, you will (hopefully) notice two items popping up in your Environment/Workspace tab in the top right corner.\n\ngreeting &lt;- \"Hello world!\"\n\nquick_math &lt;- 2 + 2 - 1\n\nIf we now want to display the content in the console, we can simply apply the print() function to the variable:\n\nprint(greeting)\n\n[1] \"Hello world!\"\n\nprint(quick_math)\n\n[1] 3\n\n\nWe can also embed variables in other statements. For example, let’s take the content of quick_math and multiply it with 2.\n\nhard_math &lt;- quick_math * 2\n\nprint(hard_math)\n\n[1] 6\n\n\n\n\n\nWorking with the console has a very plain, yet important disadvantage: Once we close RStudio, the console is wiped clean, erasing everything we’ve typed into it during our precious R session.\nThe remedy for this nuisance are scripts. Essentially, a script is the programmer’s equivalent of a Word document: It allows you to save all the code you’ve written to a file, which you can seamlessly continue working on the next time you open it.\n\nTo create a new R script you can either go to:\n\n“File” \\(\\rightarrow\\) “New” \\(\\rightarrow\\) “R Script” or …\nclick on the icon with the + sign and select “R Script” …\nor simply press Ctrl+Shift+N (MacOS: Cmd+Shift+N)\n\n\nDon’t forget to save your script with Ctrl+S (Cmd+S)! It is good practice to save your files regularly."
  },
  {
    "objectID": "chapters/03-exploring_rstudio.html#the-rstudio-interface",
    "href": "chapters/03-exploring_rstudio.html#the-rstudio-interface",
    "title": "Exploring RStudio",
    "section": "",
    "text": "Once you’ve opened RStudio, you will see several empty panes similar to this:\n\n\n\n\n\n\n\nLet’s focus on the console window on the left. This is where we can directly communicate with R by entering “code”. The way this code is written follows certain arbitrary conventions – just like natural languages such as English or German. Here is an example in the R programming language:\n\nprint(\"Hello world!\")\n\n[1] \"Hello world!\"\n\n\nEntering this command into your console and pressing ENTER will display the sentence “Hello world!” in a new line.\nAs we can see, anything that R should understand as a simple sequence of letters or words must be enclosed by quotation marks \"...\". Anything inside them will be interpreted as a so-called string. Their counterpart are numbers or integers, as illustrated here:\n\n2 + 2 - 1\n\n[1] 3\n\n\nNaturally, you can use R for more sophisticated computations:\n\n\\(2 + (3 \\times 3)\\)\n\n2 + (3 * 3)\n\n[1] 11\n\n\n\\(\\sqrt{9}\\)\n\nsqrt(9)\n\n[1] 3\n\n\n\\(\\frac{16}{2^3}\\)\n\n16 / 2 ^ 3\n\n[1] 2\n\n\n\n\n\n\nWhile it is certainly useful to print text or numbers to your console, it may sometimes make more sense to (at least temporally) store them somewhere, so you can re-use them later. In fact, R gives us a way of storing data in virtual, container-like objects: variables. We can assign strings or numbers to a variable by using the assignment operator &lt;-.\nWhen you run the commands below, you will (hopefully) notice two items popping up in your Environment/Workspace tab in the top right corner.\n\ngreeting &lt;- \"Hello world!\"\n\nquick_math &lt;- 2 + 2 - 1\n\nIf we now want to display the content in the console, we can simply apply the print() function to the variable:\n\nprint(greeting)\n\n[1] \"Hello world!\"\n\nprint(quick_math)\n\n[1] 3\n\n\nWe can also embed variables in other statements. For example, let’s take the content of quick_math and multiply it with 2.\n\nhard_math &lt;- quick_math * 2\n\nprint(hard_math)\n\n[1] 6\n\n\n\n\n\nWorking with the console has a very plain, yet important disadvantage: Once we close RStudio, the console is wiped clean, erasing everything we’ve typed into it during our precious R session.\nThe remedy for this nuisance are scripts. Essentially, a script is the programmer’s equivalent of a Word document: It allows you to save all the code you’ve written to a file, which you can seamlessly continue working on the next time you open it.\n\nTo create a new R script you can either go to:\n\n“File” \\(\\rightarrow\\) “New” \\(\\rightarrow\\) “R Script” or …\nclick on the icon with the + sign and select “R Script” …\nor simply press Ctrl+Shift+N (MacOS: Cmd+Shift+N)\n\n\nDon’t forget to save your script with Ctrl+S (Cmd+S)! It is good practice to save your files regularly."
  },
  {
    "objectID": "chapters/02-basics.html",
    "href": "chapters/02-basics.html",
    "title": "1. An example from sociolinguistics",
    "section": "",
    "text": "New quotatives in American English: be like, go, be all:\n\n“I was like ‘well if it’s not leaking, I don’t care.’”\n“Do you, I asked I was running out and I go, ‘you’re trying to deliver a pizza here?’”\n“My sister’s all ‘excuse me would you mind if I gave you, if I want your autograph.’”\n\n\n\n\n\nPrevious results with regard to age: Be like is known to be preferred by younger speakers.\nPrevious results with regard to sex are inconclusive.\n\n\n\n\n\n“The study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all” (Barbieri 2007: 26).\n\n\n\n\n\nChoice of corpus: 450,000 words taken from the Conversation component of the Longman Spoken and Written English Corpus (all files where the age and gender of speakers were available), i.e., about 50 hours of casual conversation coded for speakers’ age and sex.\nData extraction: All forms of be like, be all, go, say extracted using corpus linguistic software; data entered into spreadsheet software; word counts of texts added via Delphi program.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuotative\nAge group\nMale use (per 100,000 words)\nFemale use (per 100,000 words)\n\n\n\n\nsay\n16–26\n115\n149\n\n\n\n27–40\n101\n117\n\n\n\n41–54\n55\n214\n\n\n\n55+\n67\n433\n\n\nbe like\n16–26\n32\n329\n\n\n\n27–40\n66\n25\n\n\n\n41–54\n-\n1\n\n\n\n55+\n-\n19\n\n\ngo\n16–26\n25\n113\n\n\n\n27–40\n61\n18\n\n\n\n41–54\n-\n15\n\n\n\n55+\n-\n7\n\n\nbe all\n16–26\n14\n33\n\n\n\n\n\n\n\n\nReproduction of Barbieri’s Figure 5 ‘Proportional quotative use by men and women aged 16–26 and 27–40’\n\n\n\n\n\n\n\n\n“In sum, the patterns of proportional use and the results of the statistical analyses show that there are striking differences in the way that men and women under the age of forty use quotative verbs. In contrast, in conforming to the traditional quotative say, men and women over forty displayed similar behavior” (Barbieri 2007: 38–39).\n\n\n\n\n\n“In favoring be like over other quotatives, men in their late 20s and in their 30s display an ‘affinity’ with slightly younger women, women in their early 20s, rather than with women of their same age” (Barbieri 2007: 41)."
  },
  {
    "objectID": "chapters/02-basics.html#barbierioldermenyounger2007-older-men-and-younger-women",
    "href": "chapters/02-basics.html#barbierioldermenyounger2007-older-men-and-younger-women",
    "title": "1. An example from sociolinguistics",
    "section": "",
    "text": "New quotatives in American English: be like, go, be all:\n\n“I was like ‘well if it’s not leaking, I don’t care.’”\n“Do you, I asked I was running out and I go, ‘you’re trying to deliver a pizza here?’”\n“My sister’s all ‘excuse me would you mind if I gave you, if I want your autograph.’”\n\n\n\n\n\nPrevious results with regard to age: Be like is known to be preferred by younger speakers.\nPrevious results with regard to sex are inconclusive.\n\n\n\n\n\n“The study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all” (Barbieri 2007: 26).\n\n\n\n\n\nChoice of corpus: 450,000 words taken from the Conversation component of the Longman Spoken and Written English Corpus (all files where the age and gender of speakers were available), i.e., about 50 hours of casual conversation coded for speakers’ age and sex.\nData extraction: All forms of be like, be all, go, say extracted using corpus linguistic software; data entered into spreadsheet software; word counts of texts added via Delphi program.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuotative\nAge group\nMale use (per 100,000 words)\nFemale use (per 100,000 words)\n\n\n\n\nsay\n16–26\n115\n149\n\n\n\n27–40\n101\n117\n\n\n\n41–54\n55\n214\n\n\n\n55+\n67\n433\n\n\nbe like\n16–26\n32\n329\n\n\n\n27–40\n66\n25\n\n\n\n41–54\n-\n1\n\n\n\n55+\n-\n19\n\n\ngo\n16–26\n25\n113\n\n\n\n27–40\n61\n18\n\n\n\n41–54\n-\n15\n\n\n\n55+\n-\n7\n\n\nbe all\n16–26\n14\n33\n\n\n\n\n\n\n\n\nReproduction of Barbieri’s Figure 5 ‘Proportional quotative use by men and women aged 16–26 and 27–40’\n\n\n\n\n\n\n\n\n“In sum, the patterns of proportional use and the results of the statistical analyses show that there are striking differences in the way that men and women under the age of forty use quotative verbs. In contrast, in conforming to the traditional quotative say, men and women over forty displayed similar behavior” (Barbieri 2007: 38–39).\n\n\n\n\n\n“In favoring be like over other quotatives, men in their late 20s and in their 30s display an ‘affinity’ with slightly younger women, women in their early 20s, rather than with women of their same age” (Barbieri 2007: 41)."
  },
  {
    "objectID": "chapters/02-basics.html#principles-of-empirical-linguistics",
    "href": "chapters/02-basics.html#principles-of-empirical-linguistics",
    "title": "1. An example from sociolinguistics",
    "section": "2 Principles of Empirical Linguistics",
    "text": "2 Principles of Empirical Linguistics\n\nObjectivity — Independence from researchers or devices (→ replicability!)\nReliability — Studies should be replicable.\nValidity — A study must actually address the problem formulated in the research question."
  },
  {
    "objectID": "chapters/02-basics.html#exercises",
    "href": "chapters/02-basics.html#exercises",
    "title": "1. An example from sociolinguistics",
    "section": "3 Exercises",
    "text": "3 Exercises\n\nExercise 1 As you read through the sections above, think about what might go wrong at each stage of the study!\n\n\nExercise 2 Read another sociolinguistic study (e.g., Unuabonah and Gut 2018). As you read, identify similar key sections or “building blocks” used in this study (e.g., introduction, research question etc.). What purpose does each section serve in the study’s overall structure?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "This collection of handouts provides a hands-on introduction to data analysis and statistical methods in quantitative corpus linguistics with R. It is designed with accessibility in mind, assuming no prior knowledge of programming or statistics. All you need to get started is a laptop; everything else will be explained within these pages.\nPrimarily, this reader is geared towards students attending the classes Language Variation (BA) and Statistics for Linguistics (MA) at the Catholic University of Eichstätt-Ingolstadt (Germany). However, it is also meant to equip students currently working on their BA/MA/PhD theses with the tools they need to conduct empirical studies on a wide array of linguistic phenomena. The methods presented here reflect the state-of-the-art in corpus-linguistic research, providing readers with current and relevant analytical techniques."
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Preface",
    "section": "",
    "text": "This collection of handouts provides a hands-on introduction to data analysis and statistical methods in quantitative corpus linguistics with R. It is designed with accessibility in mind, assuming no prior knowledge of programming or statistics. All you need to get started is a laptop; everything else will be explained within these pages.\nPrimarily, this reader is geared towards students attending the classes Language Variation (BA) and Statistics for Linguistics (MA) at the Catholic University of Eichstätt-Ingolstadt (Germany). However, it is also meant to equip students currently working on their BA/MA/PhD theses with the tools they need to conduct empirical studies on a wide array of linguistic phenomena. The methods presented here reflect the state-of-the-art in corpus-linguistic research, providing readers with current and relevant analytical techniques."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Preface",
    "section": "Overview",
    "text": "Overview\nWe begin by establishing the general structure of corpus-linguistic studies, accompanied by key theoretical and practical considerations. The second section introduces R as an analytical tool, covering its core functionality. With these fundamentals in place, we proceed to query corpora directly within R. Chapters four and five focus on describing discrete and continuous outputs, as well as identifying meaningful associations and differences in the data. Finally, to gain more nuanced insights into potential patterns, we apply common machine learning algorithms to fit, evaluate, and interpret statistical models.\nThroughout this journey, readers will develop the skills to conduct robust corpus-linguistic analyses, from basic querying to advanced statistical modeling."
  },
  {
    "objectID": "index.html#collaborators",
    "href": "index.html#collaborators",
    "title": "Preface",
    "section": "1 Collaborators",
    "text": "1 Collaborators\nVladimir Buskin, PhD student at the Department of English Language and Linguistics, Catholic University of Eichstätt-Ingolstadt\nThomas Brunner, Assistant Professor at the Department of English Language and Linguistics, Catholic University of Eichstätt-Ingolstadt\nPhilippa Adolf, PhD student at the Department of Romance Studies, University of Vienna"
  },
  {
    "objectID": "Basics.html",
    "href": "Basics.html",
    "title": "An example from sociolinguistics",
    "section": "",
    "text": "New quotatives in American English: be like, go, be all:\n\n“I was like ‘well if it’s not leaking, I don’t care.’”\n“Do you, I asked I was running out and I go, ‘you’re trying to deliver a pizza here?’”\n“My sister’s all ‘excuse me would you mind if I gave you, if I want your autograph.’”\n\n\n\n\n\nPrevious results with regard to age: Be like is known to be preferred by younger speakers.\nPrevious results with regard to sex are inconclusive.\n\n\n\n\n\n“The study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all” (Barbieri 2007: 26).\n\n\n\n\n\nChoice of corpus: 450,000 words taken from the Conversation component of the Longman Spoken and Written English Corpus (all files where the age and gender of speakers were available), i.e., about 50 hours of casual conversation coded for speakers’ age and sex.\nData extraction: All forms of be like, be all, go, say extracted using corpus linguistic software; data entered into spreadsheet software; word counts of texts added via Delphi program.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuotative\nAge group\nMale use (per 100,000 words)\nFemale use (per 100,000 words)\n\n\n\n\nsay\n16–26\n115\n149\n\n\n\n27–40\n101\n117\n\n\n\n41–54\n55\n214\n\n\n\n55+\n67\n433\n\n\nbe like\n16–26\n32\n329\n\n\n\n27–40\n66\n25\n\n\n\n41–54\n-\n1\n\n\n\n55+\n-\n19\n\n\ngo\n16–26\n25\n113\n\n\n\n27–40\n61\n18\n\n\n\n41–54\n-\n15\n\n\n\n55+\n-\n7\n\n\nbe all\n16–26\n14\n33\n\n\n\n\n\n\n\n\nReproduction of Barbieri’s Figure 5 ‘Proportional quotative use by men and women aged 16–26 and 27–40’\n\n\n\n\n\n\n\n\n“In sum, the patterns of proportional use and the results of the statistical analyses show that there are striking differences in the way that men and women under the age of forty use quotative verbs. In contrast, in conforming to the traditional quotative say, men and women over forty displayed similar behavior” (Barbieri 2007: 38–39).\n\n\n\n\n\n“In favoring be like over other quotatives, men in their late 20s and in their 30s display an ‘affinity’ with slightly younger women, women in their early 20s, rather than with women of their same age” (Barbieri 2007: 41)."
  },
  {
    "objectID": "Basics.html#barbierioldermenyounger2007-older-men-and-younger-women",
    "href": "Basics.html#barbierioldermenyounger2007-older-men-and-younger-women",
    "title": "An example from sociolinguistics",
    "section": "",
    "text": "New quotatives in American English: be like, go, be all:\n\n“I was like ‘well if it’s not leaking, I don’t care.’”\n“Do you, I asked I was running out and I go, ‘you’re trying to deliver a pizza here?’”\n“My sister’s all ‘excuse me would you mind if I gave you, if I want your autograph.’”\n\n\n\n\n\nPrevious results with regard to age: Be like is known to be preferred by younger speakers.\nPrevious results with regard to sex are inconclusive.\n\n\n\n\n\n“The study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all” (Barbieri 2007: 26).\n\n\n\n\n\nChoice of corpus: 450,000 words taken from the Conversation component of the Longman Spoken and Written English Corpus (all files where the age and gender of speakers were available), i.e., about 50 hours of casual conversation coded for speakers’ age and sex.\nData extraction: All forms of be like, be all, go, say extracted using corpus linguistic software; data entered into spreadsheet software; word counts of texts added via Delphi program.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuotative\nAge group\nMale use (per 100,000 words)\nFemale use (per 100,000 words)\n\n\n\n\nsay\n16–26\n115\n149\n\n\n\n27–40\n101\n117\n\n\n\n41–54\n55\n214\n\n\n\n55+\n67\n433\n\n\nbe like\n16–26\n32\n329\n\n\n\n27–40\n66\n25\n\n\n\n41–54\n-\n1\n\n\n\n55+\n-\n19\n\n\ngo\n16–26\n25\n113\n\n\n\n27–40\n61\n18\n\n\n\n41–54\n-\n15\n\n\n\n55+\n-\n7\n\n\nbe all\n16–26\n14\n33\n\n\n\n\n\n\n\n\nReproduction of Barbieri’s Figure 5 ‘Proportional quotative use by men and women aged 16–26 and 27–40’\n\n\n\n\n\n\n\n\n“In sum, the patterns of proportional use and the results of the statistical analyses show that there are striking differences in the way that men and women under the age of forty use quotative verbs. In contrast, in conforming to the traditional quotative say, men and women over forty displayed similar behavior” (Barbieri 2007: 38–39).\n\n\n\n\n\n“In favoring be like over other quotatives, men in their late 20s and in their 30s display an ‘affinity’ with slightly younger women, women in their early 20s, rather than with women of their same age” (Barbieri 2007: 41)."
  },
  {
    "objectID": "Basics.html#principles-of-empirical-linguistics",
    "href": "Basics.html#principles-of-empirical-linguistics",
    "title": "An example from sociolinguistics",
    "section": "2 Principles of Empirical Linguistics",
    "text": "2 Principles of Empirical Linguistics\n\nObjectivity — Independence from researchers or devices (→ replicability!)\nReliability — Studies should be replicable.\nValidity — A study must actually address the problem formulated in the research question."
  },
  {
    "objectID": "Basics.html#exercises",
    "href": "Basics.html#exercises",
    "title": "An example from sociolinguistics",
    "section": "3 Exercises",
    "text": "3 Exercises\n\nExercise 1 As you read through the sections above, think about what might go wrong at each stage of the study!\n\n\nExercise 2 Read another sociolinguistic study (e.g., Unuabonah and Gut 2018). As you read, identify similar key sections or “building blocks” used in this study (e.g., introduction, research question etc.). What purpose does each section serve in the study’s overall structure?"
  },
  {
    "objectID": "blog.html#foundations",
    "href": "blog.html#foundations",
    "title": "Contents",
    "section": "1 Foundations",
    "text": "1 Foundations\n\nIntroduction"
  },
  {
    "objectID": "blog.html#fundamentals-of-corpus-based-research",
    "href": "blog.html#fundamentals-of-corpus-based-research",
    "title": "Overview",
    "section": "1 Fundamentals of Corpus-based Research",
    "text": "1 Fundamentals of Corpus-based Research\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\n1.1 An example from sociolinguistics\n\n\n\n\n\n\n\n1.2 Research questions\n\n\n\n\n\n\n\n1.3 Linguistic variables\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html#introduction-to-r",
    "href": "blog.html#introduction-to-r",
    "title": "Overview",
    "section": "2 Introduction to R",
    "text": "2 Introduction to R"
  },
  {
    "objectID": "blog.html#corpus-linguistics-with-r",
    "href": "blog.html#corpus-linguistics-with-r",
    "title": "Overview",
    "section": "3 Corpus Linguistics with R",
    "text": "3 Corpus Linguistics with R"
  },
  {
    "objectID": "blog.html#descriptive-statistics",
    "href": "blog.html#descriptive-statistics",
    "title": "Overview",
    "section": "4 Descriptive Statistics",
    "text": "4 Descriptive Statistics\n\n4.1 Chi-squared test"
  },
  {
    "objectID": "blog.html#inferential-statistics",
    "href": "blog.html#inferential-statistics",
    "title": "Overview",
    "section": "5 Inferential Statistics",
    "text": "5 Inferential Statistics"
  },
  {
    "objectID": "blog.html#statistical-modelling-and-machine-learning",
    "href": "blog.html#statistical-modelling-and-machine-learning",
    "title": "Overview",
    "section": "6 Statistical Modelling and Machine Learning",
    "text": "6 Statistical Modelling and Machine Learning"
  },
  {
    "objectID": "blog.html#collaborators",
    "href": "blog.html#collaborators",
    "title": "Overview",
    "section": "1 Collaborators",
    "text": "1 Collaborators\nVladimir Buskin, PhD student at the Department of English Language and Linguistics, Catholic University of Eichstätt-Ingolstadt\nThomas Brunner, Assistant Professor at the Department of English Language and Linguistics, Catholic University of Eichstätt-Ingolstadt\nPhilippa Adolf, PhD student at the Department of Romance Studies, University of Vienna"
  },
  {
    "objectID": "chapters/01-rqs.html",
    "href": "chapters/01-rqs.html",
    "title": "1.2 Research questions",
    "section": "",
    "text": "Example: I am studying the pronunciation of [r] in New York because I want to find out whether different social groups differ with respect to their pronunciation, in order to help my readers understand how social factors influence our linguistic behavior.\n\nTopic: I am studying...\nIndirect question: Because I want to find out what, why, how...\n(General) linguistic significance: ... in order to help the readers understand how, why, or whether...\n\n\nCf. Booth, Colomb, and Williams (2008): 45–48\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nA research question must be simple, specific, and doable. It is better to make a small contribution to a particular problem than to aim at covering a whole subfield of sociolinguistics.\nKeep a research question stable in your paper, even if you come across interesting new ideas as you work on it.\nThe research question must be an explicit part of your paper. Mention it in the introduction and keep referring back to it from time to time.\nKeep in mind that the research question will determine both methodology and data.\n\n\nCf. Hazen (2015): 9–10\n\n\n\n\n\n\nResearch questions must be complemented by a set of falsifiable hypotheses. These will be covered in-depth in the chapter on hypothesis testing. In short,\n\nHypothesis \\(H_1\\) predicts a specific relationship between a dependent variable and an independent variable. If \\(H_1\\) holds, studies often speak of a correlation (or association) between variables.\nits opposite, Hypothesis \\(H_0\\), describes the state of affairs where the predicted relationship between the variables does not hold.\n\nThe predictions made by the hypotheses are based on previous research. Here is an example:\nResearch question: I am studying the pronunciation of [r] in New York because I want to find out whether different social groups differ with respect to their pronunciation, in order to help my readers understand how social factors influence our linguistic behavior.\n\nHypothesis \\(H_1\\): There is a difference in the use of postvocalic /r/ between upper-class and lower-class speakers.\nHypothesis \\(H_0\\): There is no difference in the use of postvocalic /r/ between upper-class and lower-class speakers.\n\n\n\n\n\nExercise 1 Assess the following research questions!\n\nI will try to find the specific linguistic features which a Shetlander uses when speaking Scottish standard English, i.e., Shetland accent in Scottish Standard English.\nIn this paper, I am going to analyze the use of American and Indian dialect features among Indian immigrants to the US because I want to find out whether they correlate with their willingness to integrate into American society; this will show the degree to which linguistic adaptation depends on cultural orientation.\nIn this paper, I am going to address the question of how gender influences language.\nThe study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all.\nIn this paper, adverts by Coca-Cola and Pepsi will be analyzed for the use of verb forms.\n\n\n\nExercise 2 Develop valid research questions on the basis of the following linguistic variables:\n\nSpeakers in bilingual conversations are especially likely to code-switch when there is a significant change of topic.\nWomen use more tentative language in conversations than men."
  },
  {
    "objectID": "chapters/01-rqs.html#what-makes-a-good-research-question",
    "href": "chapters/01-rqs.html#what-makes-a-good-research-question",
    "title": "1.2 Research questions",
    "section": "",
    "text": "Example: I am studying the pronunciation of [r] in New York because I want to find out whether different social groups differ with respect to their pronunciation, in order to help my readers understand how social factors influence our linguistic behavior.\n\nTopic: I am studying...\nIndirect question: Because I want to find out what, why, how...\n(General) linguistic significance: ... in order to help the readers understand how, why, or whether...\n\n\nCf. Booth, Colomb, and Williams (2008): 45–48\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nA research question must be simple, specific, and doable. It is better to make a small contribution to a particular problem than to aim at covering a whole subfield of sociolinguistics.\nKeep a research question stable in your paper, even if you come across interesting new ideas as you work on it.\nThe research question must be an explicit part of your paper. Mention it in the introduction and keep referring back to it from time to time.\nKeep in mind that the research question will determine both methodology and data.\n\n\nCf. Hazen (2015): 9–10\n\n\n\n\n\n\nResearch questions must be complemented by a set of falsifiable hypotheses. These will be covered in-depth in the chapter on hypothesis testing. In short,\n\nHypothesis \\(H_1\\) predicts a specific relationship between a dependent variable and an independent variable. If \\(H_1\\) holds, studies often speak of a correlation (or association) between variables.\nits opposite, Hypothesis \\(H_0\\), describes the state of affairs where the predicted relationship between the variables does not hold.\n\nThe predictions made by the hypotheses are based on previous research. Here is an example:\nResearch question: I am studying the pronunciation of [r] in New York because I want to find out whether different social groups differ with respect to their pronunciation, in order to help my readers understand how social factors influence our linguistic behavior.\n\nHypothesis \\(H_1\\): There is a difference in the use of postvocalic /r/ between upper-class and lower-class speakers.\nHypothesis \\(H_0\\): There is no difference in the use of postvocalic /r/ between upper-class and lower-class speakers.\n\n\n\n\n\nExercise 1 Assess the following research questions!\n\nI will try to find the specific linguistic features which a Shetlander uses when speaking Scottish standard English, i.e., Shetland accent in Scottish Standard English.\nIn this paper, I am going to analyze the use of American and Indian dialect features among Indian immigrants to the US because I want to find out whether they correlate with their willingness to integrate into American society; this will show the degree to which linguistic adaptation depends on cultural orientation.\nIn this paper, I am going to address the question of how gender influences language.\nThe study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all.\nIn this paper, adverts by Coca-Cola and Pepsi will be analyzed for the use of verb forms.\n\n\n\nExercise 2 Develop valid research questions on the basis of the following linguistic variables:\n\nSpeakers in bilingual conversations are especially likely to code-switch when there is a significant change of topic.\nWomen use more tentative language in conversations than men."
  },
  {
    "objectID": "chapters/01-linguistic_variables.html",
    "href": "chapters/01-linguistic_variables.html",
    "title": "1.3 Linguistic variables",
    "section": "",
    "text": "The classical view: Labov (1972) defines a linguistic variable as “two ways of saying the same thing.”\nA restriction: Meyerhoff (2009: 11) summarized, “In sum, a sociolinguistic variable can be defined as a linguistic variable that is constrained by social or non-linguistic factors…”\nA more open view: Kiesling (2011) argued, “Given the variability of what counts as a variable, we must define what counts as a variable more broadly than ‘two or more ways of saying the same thing’. We will simply say that a linguistic variable is a choice or option about speaking in a speech community… Note that this definition does not in any way require us to state that the meaning be the same, although there should be some kind of equivalence noted.”\n\n\n\n\n\n\n\nDiscussion\n\n\n\nWhich of the following variables are good sociolinguistic variables, and which of them are poor?\n\n/fɔːθ flɔː/ vs. /fɔːrθ flɔːr/\nThis enables him to preside over the process which I have described vs. This enables him to preside over the process that I have described vs. This enables him to preside over the process ∅ I have described.\nThe pair found the briefcase on a bus station bench at Bath central bus station. vs. The briefcase was found on a bus station bench at Bath central bus station by the pair.\nArt is after all the subject of attention for both critic and historian, even though the functions and methods of the two sorts of writer have drawn apart. vs. Art histories often make an attempt to keep to chronology, although the difficulties include the crucial fact that in art there is no clear sequence of events. vs. Many of his readers approved his sensitive and appreciative understanding of paintings, though without sharing his political views.\n/pleɪŋ/ vs. /pleɪn/\n[tʰ] in /tɔp/ vs. [t] in stop."
  },
  {
    "objectID": "chapters/01-linguistic_variables.html#what-is-a-linguistic-variable",
    "href": "chapters/01-linguistic_variables.html#what-is-a-linguistic-variable",
    "title": "1.3 Linguistic variables",
    "section": "",
    "text": "The classical view: Labov (1972) defines a linguistic variable as “two ways of saying the same thing.”\nA restriction: Meyerhoff (2009: 11) summarized, “In sum, a sociolinguistic variable can be defined as a linguistic variable that is constrained by social or non-linguistic factors…”\nA more open view: Kiesling (2011) argued, “Given the variability of what counts as a variable, we must define what counts as a variable more broadly than ‘two or more ways of saying the same thing’. We will simply say that a linguistic variable is a choice or option about speaking in a speech community… Note that this definition does not in any way require us to state that the meaning be the same, although there should be some kind of equivalence noted.”\n\n\n\n\n\n\n\nDiscussion\n\n\n\nWhich of the following variables are good sociolinguistic variables, and which of them are poor?\n\n/fɔːθ flɔː/ vs. /fɔːrθ flɔːr/\nThis enables him to preside over the process which I have described vs. This enables him to preside over the process that I have described vs. This enables him to preside over the process ∅ I have described.\nThe pair found the briefcase on a bus station bench at Bath central bus station. vs. The briefcase was found on a bus station bench at Bath central bus station by the pair.\nArt is after all the subject of attention for both critic and historian, even though the functions and methods of the two sorts of writer have drawn apart. vs. Art histories often make an attempt to keep to chronology, although the difficulties include the crucial fact that in art there is no clear sequence of events. vs. Many of his readers approved his sensitive and appreciative understanding of paintings, though without sharing his political views.\n/pleɪŋ/ vs. /pleɪn/\n[tʰ] in /tɔp/ vs. [t] in stop."
  },
  {
    "objectID": "chapters/01-linguistic_variables.html#the-principle-of-accountability",
    "href": "chapters/01-linguistic_variables.html#the-principle-of-accountability",
    "title": "1.3 Linguistic variables",
    "section": "The principle of accountability",
    "text": "The principle of accountability\n\n\n\n\n\n\nTask\n\n\n\nTwo linguists aim to study the preference for passives among men and women. They extract all the passives from 500,000 words of male speech and all passives from 500,000 words of female speech and report the results. What’s wrong?"
  },
  {
    "objectID": "chapters/01-linguistic_variables.html#subtypes-of-variables",
    "href": "chapters/01-linguistic_variables.html#subtypes-of-variables",
    "title": "1.3 Linguistic variables",
    "section": "Subtypes of variables",
    "text": "Subtypes of variables\n\nLinguistic perspective\n\nphonetic/phonological\nmorphological\nsyntactic\npragmatic\n\n\n\nSociolinguistic perspective\nSociolinguistic variables also differ with regard to their salience in society.\n\nStereotypes are strongly socially marked and part of popular discourse about language.\n\nh-dropping in Cockney\nCanadian eh at the end of sentences\nAustralian dinkum: I was fair dinkum about my interest in their culture ‘authentic, genuine’\n\nMarkers show both social and style stratification; all members of a society react similarly in taking care to avoid the pattern in formal registers.\n\n(r)\n(th)\n\nIndicators differentiate social groups. However, people are not aware of them and therefore do not avoid them in formal registers.\n\nSame vowel in God and Guard in New York City\n\n\n\nCf. Mesthrie (2011)."
  },
  {
    "objectID": "chapters/01-linguistic_variables.html#many-morphosyntactic-variables-in-english",
    "href": "chapters/01-linguistic_variables.html#many-morphosyntactic-variables-in-english",
    "title": "1.3 Linguistic variables",
    "section": "Many morphosyntactic variables in English",
    "text": "Many morphosyntactic variables in English\n\n\n\n\n\n\n\nVariable\nExample\n\n\n\n\nIndefinite Pronouns\neverybody vs. everyone\n\n\nCase and order of coordinated pronouns\nmy husband and I vs. my husband and me vs. me and my husband\n\n\nthat vs. zero complementation\nI don’t think that/Ø it’s a problem.\n\n\nthat vs. gerundial complementation\nremember that vs. remember V-ing; try to vs. try and vs. try V-ing\n\n\nParticle placement\nset the computer up vs. set up the computer\n\n\nThe dative alternation\ngive the book to John vs. give John the book\n\n\nThe genitive alternation\nJohn’s house vs. the house of John\n\n\nRelativization strategies\nwh-word vs. that vs. Ø\n\n\nAnalytic vs. synthetic comparatives\nwarmer vs. more scary\n\n\nPlural existentials\nthere are some places vs. there’s some places\n\n\nFuture temporal reference\nwill vs. going to vs. progressive etc.\n\n\nDeontic modality\nmust vs. have to vs. need to vs. got to etc.\n\n\nStative possession\nhave vs. have got vs. got\n\n\nQuotatives\nsay vs. be like vs. go etc.\n\n\nnot vs. no\nnot anybody vs. nobody; not anyone vs. no one; not anything vs. nothing\n\n\nNOT vs. AUX contraction\nthat’s not vs. that isn’t etc.\n\n\n\n\nCf. Gardner et al. (2021)."
  },
  {
    "objectID": "chapters/02-libraries.html",
    "href": "chapters/02-libraries.html",
    "title": "Libraries",
    "section": "",
    "text": "Winter (2020): Chapter 1.13"
  },
  {
    "objectID": "chapters/02-libraries.html#recommended-reading",
    "href": "chapters/02-libraries.html#recommended-reading",
    "title": "Libraries",
    "section": "",
    "text": "Winter (2020): Chapter 1.13"
  },
  {
    "objectID": "chapters/02-libraries.html#working-with-packages-in-r",
    "href": "chapters/02-libraries.html#working-with-packages-in-r",
    "title": "Libraries",
    "section": "2 Working with packages in R",
    "text": "2 Working with packages in R\nPackages expand the basic functionality of R by providing numerous quality-of-life improvements that not only considerably simplify common data wrangling tasks but which also provide frameworks for state-of-the-art methods for statistical analysis and natural language processing (NLP), among many other things.\n\n2.1 Installation\n\n\n\n\n\n\nHow do I install a library?\n\n\n\n\n\nNavigate to Packages &gt; Install and verify that the pop-up window says Install from: Repository (CRAN). You can now type in the name of the package you would like to install under Packages.\nVideo tutorial on YouTube\n\n\n\nThis reader will use functions from a variety of R packages. Please install the following ones:\n\n\n\nPackage\nPurpose\nSession\n\n\n\n\nreadxl\nImporting Microsoft Excel files\nImporting/exporting\n\n\nwritexl\nExporting Microsoft Excel files\nImporting/exporting\n\n\nquanteda\nAnalysis of text data\nConcordancing\n\n\nlattice\nData visualisation\nConcordancing\n\n\ntidyverse\nFramework for data manipulation and visualisation\nCategorical data, Continuous data\n\n\ncrosstable\nCreating contingency tables\nCategorical data\n\n\nflextable\nExporting contingency tables\nCategorical data\n\n\nconfintr\nEffect size measure for categorical data\nChi-squared test\n\n\n\n\n\n2.2 Loading packages\nOnce the installation has been completed, you can proceed to load the libraries using the code below. You can ignore the warning messages.\n\nlibrary(readxl)\nlibrary(writexl)\nlibrary(quanteda)\n\nPackage version: 3.3.1\nUnicode version: 14.0\nICU version: 70.1\n\n\nParallel computing: 8 of 8 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(lattice)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(crosstable)\n\n\nAttaching package: 'crosstable'\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\nlibrary(flextable)\n\n\nAttaching package: 'flextable'\n\nThe following object is masked from 'package:purrr':\n\n    compose\n\nlibrary(confintr)\n\n\n\n\n\n\n\nActivating libraries\n\n\n\n\n\nWhenever you start a new R session (i.e., open RStudio), your libraries and their respective functions will be inactive. To re-activate a library, either use the library() function or simply select it in the Packages tab.\n\n\n\nIt is good practice to only activate those packages that are necessary for your analysis. While it won’t be a problem for the small set of packages as shown here, loading dozens of packages increases the risk of obtaining “homonymous” functions which have the same name but perform different operations. In this case, it might be helpful to “disambiguate” them by directly indicating which package a function is from:\n\nreadxl::read_xlsx(...)\n\n\n\n2.3 Citing R and R packages\nWhenever we draw on ideas other than our own, we give credit to the respective source by citing it appropriately. The same applies to R, RStudio as well as all the packages we rely on throughout our analyses.\nFor R, an up-to-date citation can be generated as follows:\n\ncitation()\n\n\nTo cite R in publications use:\n\n  R Core Team (2023). R: A language and environment for statistical\n  computing. R Foundation for Statistical Computing, Vienna, Austria.\n  URL https://www.R-project.org/.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2023},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nTo cite a specific package, simply supply the package name as an argument.\n\ncitation(\"quanteda\")\n\n\nTo cite package 'quanteda' in publications use:\n\n  Benoit K, Watanabe K, Wang H, Nulty P, Obeng A, Müller S, Matsuo A\n  (2018). \"quanteda: An R package for the quantitative analysis of\n  textual data.\" _Journal of Open Source Software_, *3*(30), 774.\n  doi:10.21105/joss.00774 &lt;https://doi.org/10.21105/joss.00774&gt;,\n  &lt;https://quanteda.io&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {quanteda: An R package for the quantitative analysis of textual data},\n    journal = {Journal of Open Source Software},\n    author = {Kenneth Benoit and Kohei Watanabe and Haiyan Wang and Paul Nulty and Adam Obeng and Stefan Müller and Akitaka Matsuo},\n    doi = {10.21105/joss.00774},\n    url = {https://quanteda.io},\n    volume = {3},\n    number = {30},\n    pages = {774},\n    year = {2018},\n  }"
  },
  {
    "objectID": "chapters/02-importing_exporting.html",
    "href": "chapters/02-importing_exporting.html",
    "title": "Import/export data",
    "section": "",
    "text": "Script\n\n\n\nYou can find the full R script associated with this unit here."
  },
  {
    "objectID": "chapters/02-importing_exporting.html#recommended-reading",
    "href": "chapters/02-importing_exporting.html#recommended-reading",
    "title": "Import/export data",
    "section": "1 Recommended reading",
    "text": "1 Recommended reading\n\nWinter (2020): Chapter 1.11"
  },
  {
    "objectID": "chapters/02-importing_exporting.html#preparation",
    "href": "chapters/02-importing_exporting.html#preparation",
    "title": "Import/export data",
    "section": "2 Preparation",
    "text": "2 Preparation\nThe first section of an R script should always specify the libraries that are needed for executing the code to follow. In this unit, we will need readxl and writexl to aid us with importing MS Excel files.\n\nlibrary(readxl)\nlibrary(writexl)\n\nIf you haven’t installed them yet, the R console will throw an error message. For instructions on how to install an R package, consult the unit on Libraries.\n\n2.1 Exporting data\nAssume we’d like to export our data frame with word frequencies to a local file on our system. Let’s briefly regenerate the data frame:\n\n# Generate data frame\ndata &lt;- data.frame(lemma = c(\"start\", \"enjoy\", \"begin\", \"help\"), \n                   frequency = c(418, 139, 337, 281))\n\n# Print contents\nprint(data)\n\n  lemma frequency\n1 start       418\n2 enjoy       139\n3 begin       337\n4  help       281\n\n\nThere are two common formats in which tabular data can be stored:\n\nas .csv-files (‘comma-separated values’; native format of LibreOffice Calc)\nas .xls/.xlsx-files (Microsoft Excel)\n\n\n\n\n\n\n\nExport to CSV\n\n\n\n\n\nTo save our data data frame in .csv-format, we can use the write_table() function:\n\nwrite.csv(data, \"frequency_data.csv\")\n\nThe file is now stored at the location of your current R script. You can open this file …\n\nin LibreOffice\nin Microsoft Excel via File &gt; Import &gt; CSV file &gt; Select the file &gt; Delimited and then Next &gt; Comma and Next &gt; General and Finish.\n\nClearly, opening CSV files in MS Excel is quite cumbersome, which is why it’s better to export it as an Excel file directly.\n\n\n\n\n\n\n\n\n\nExport to Excel\n\n\n\n\n\nWe use the write_xlsx() function provided by the package writexl.\n\nwrite_xlsx(data, \"frequency_data.xlsx\")\n\nThe file is now stored at the location of your currently active R script. You should now be able to open it in MS Excel without any issues.\n\n\n\n\n\n2.2 Importing data\nLet’s read the two files back into R.\n\n\n\n\n\n\nImport from CSV\n\n\n\n\n\nTo import the CSV file, we can use the read.csv() function:\n\nimported_csv &lt;- read.csv(\"frequency_data.csv\")\nprint(imported_csv)\n\n  X lemma frequency\n1 1 start       418\n2 2 enjoy       139\n3 3 begin       337\n4 4  help       281\n\n\nIt appears that read.csv() has also written the row numbers to the file. This is not the desired outcome and can be prevented by adding an additional argument:\n\nimported_csv &lt;- read.csv(\"frequency_data.csv\", row.names = 1)\nprint(imported_csv) # Problem solved!\n\n  lemma frequency\n1 start       418\n2 enjoy       139\n3 begin       337\n4  help       281\n\n\n\n\n\n\n\n\nA note on file encodings and separators\n\n\n\n\n\nWhen working with CSV files, you may encounter issues with character encodings and separators, especially when:\n\nworking with files from different operating systems,\ndealing with text containing special characters (é, ü, ñ, etc.), or\nimporting files created in different regions (e.g., European vs. US).\n\nThe most common encoding-related parameters for read.csv() are:\n\n# For files with special characters (recommended default)\ndata &lt;- read.csv(\"myfile.csv\", encoding = \"UTF-8\")\n\n# For files from Windows systems\ndata &lt;- read.csv(\"myfile.csv\", encoding = \"latin1\")\n\n# For files using semicolons and commas as decimal points\ndata &lt;- read.csv(\"myfile.csv\", sep = \";\", dec = \",\")\n\n\nIf you see garbled text like Ã© instead of é, try specifying encoding = \"UTF-8\".\nIf your data appears in a single column, check if your file uses semicolons (;) instead of commas (,) as separators.\nIf numeric values are incorrect, verify whether the file uses commas or periods as decimal separators.\n\n\n\n\n\n\n\n\n\n\n\n\n\nImport from Excel\n\n\n\n\n\nFor importing the Excel file, we’ll use the read_xlsx() function from the readxl package:\n\nimported_excel &lt;- read_xlsx(\"frequency_data.xlsx\")\nprint(imported_excel)\n\n# A tibble: 4 × 2\n  lemma frequency\n  &lt;chr&gt;     &lt;dbl&gt;\n1 start       418\n2 enjoy       139\n3 begin       337\n4 help        281\n\n\n\n\n\nThat’s it! Nevertheless, remember to always check your imported data to ensure it has been read in correctly, especially when working with CSV files."
  },
  {
    "objectID": "chapters/02-importing_exporting.html#a-convenient-alternative-rds-files",
    "href": "chapters/02-importing_exporting.html#a-convenient-alternative-rds-files",
    "title": "Import/export data",
    "section": "3 A convenient alternative: RDS files",
    "text": "3 A convenient alternative: RDS files\nIf the main goal is to save an intermediary result and make it available for later use, the most efficient solution is to save the object to a local R data file ending in .RDS. Since it compressed data, .RDS files can be considered analogous to .zip files, which are very commonly used for other data types.\nIn practice, we use the saveRDS() function and supply it with …\n\n… an R object (e.g., a vector, data frame, matrix, graphs, statistical models – anything goes!) as well as\n… the desired name of the file.\n\n\n# Save data frame \"data\" to the file \"frequency_data.RDS\"\nsaveRDS(data, \"frequency_data.RDS\")\n\nTo read a file back in, we need to indicate the file name (or the full file path if the file is located in a different folder).\n\n# Read in \"frequency_data.RDS\" and assign the contents to \"data2\"\ndata2 &lt;- readRDS(\"frequency_data.RDS\")\n\n# Verify contents\nprint(data2)\n\n  lemma frequency\n1 start       418\n2 enjoy       139\n3 begin       337\n4  help       281"
  },
  {
    "objectID": "chapters/02-importing_exporting.html#troubleshooting",
    "href": "chapters/02-importing_exporting.html#troubleshooting",
    "title": "Import/export data",
    "section": "4 Troubleshooting",
    "text": "4 Troubleshooting\nImporting data into R is often a challenging and error-prone task, made more difficult by the wide range of potential issues. AI tools can be immensely helpful for identifying and addressing problems specific to your use case. This handout provides some practical guidance, including tips for resolving file-loading errors."
  },
  {
    "objectID": "chapters/02-importing_exporting.html#exercises",
    "href": "chapters/02-importing_exporting.html#exercises",
    "title": "Import/export data",
    "section": "5 Exercises",
    "text": "5 Exercises\n\n\n\n\n\n\nSolutions\n\n\n\nYou can find the solutions to the exercises here.\n\n\n\nExercise 1 Download the file SCOPE_reduced.RDS from this repository and read it into a variable named SCOPE. It contains data from the the South Carolina Psycholinguistic Metabase (Gao, Shinkareva, and Desai 2022), specifically:\n\nNumber of meanings (Nsenses_WordNet)\nEmotional valence ratings, which describe the pleasantness of a lexical stimulus on a scale from 1 to 9 (Valence_Warr)\nData for nearly 200,000 words\n\n\n\nExercise 2 Using this database, retrieve\n\nthe number of meanings for the verbs start, enjoy, begin, help. Store them in a data frame with the name senses_df.\nemotional valence ratings for the words fun, love, vacation, war, politics, failure, table. Store them in a data frame, and name it valence_df.\n\nWhat do you notice about the valence ratings? Do they align with your intuitions about these words’ emotional content?\n\n\n\n\n\n\nTip\n\n\n\nThis task is very similar to ?@exr-df-3!\n\n\n\n\nExercise 3 Export senses_df and valence_df both as .csv and .xlsx files, and read them back into R.\n\n\nExercise 4 Separators determine how tabular data is stored internally. Investigate what happens when you read in frequency_data.csv with different separator settings:\n\n# Comma separator (default)\nimported_csv1 &lt;- read.csv(\"frequency_data.csv\", sep = \",\")\nprint(imported_csv1)\n\n# Tab separator\nimported_csv2 &lt;- read.csv(\"frequency_data.csv\", sep = \"\\t\")\nprint(imported_csv2)\n\n# Semi-colon separator\nimported_csv3 &lt;- read.csv(\"frequency_data.csv\", sep = \";\")\nprint(imported_csv3)"
  },
  {
    "objectID": "chapters/01-basics.html",
    "href": "chapters/01-basics.html",
    "title": "1.1 An example from sociolinguistics",
    "section": "",
    "text": "New quotatives in American English: be like, go, be all:\n\n“I was like ‘well if it’s not leaking, I don’t care.’”\n“Do you, I asked I was running out and I go, ‘you’re trying to deliver a pizza here?’”\n“My sister’s all ‘excuse me would you mind if I gave you, if I want your autograph.’”\n\n\n\n\n\nPrevious results with regard to age: Be like is known to be preferred by younger speakers.\nPrevious results with regard to sex are inconclusive.\n\n\n\n\n\n“The study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all” (Barbieri 2007: 26).\n\n\n\n\n\nChoice of corpus: 450,000 words taken from the Conversation component of the Longman Spoken and Written English Corpus (all files where the age and gender of speakers were available), i.e., about 50 hours of casual conversation coded for speakers’ age and sex.\nData extraction: All forms of be like, be all, go, say extracted using corpus linguistic software; data entered into spreadsheet software; word counts of texts added via Delphi program.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuotative\nAge group\nMale use (per 100,000 words)\nFemale use (per 100,000 words)\n\n\n\n\nsay\n16–26\n115\n149\n\n\n\n27–40\n101\n117\n\n\n\n41–54\n55\n214\n\n\n\n55+\n67\n433\n\n\nbe like\n16–26\n32\n329\n\n\n\n27–40\n66\n25\n\n\n\n41–54\n-\n1\n\n\n\n55+\n-\n19\n\n\ngo\n16–26\n25\n113\n\n\n\n27–40\n61\n18\n\n\n\n41–54\n-\n15\n\n\n\n55+\n-\n7\n\n\nbe all\n16–26\n14\n33\n\n\n\n\n\n\n\n\nReproduction of Barbieri’s Figure 5 ‘Proportional quotative use by men and women aged 16–26 and 27–40’\n\n\n\n\n\n\n\n\n“In sum, the patterns of proportional use and the results of the statistical analyses show that there are striking differences in the way that men and women under the age of forty use quotative verbs. In contrast, in conforming to the traditional quotative say, men and women over forty displayed similar behavior” (Barbieri 2007: 38–39).\n\n\n\n\n\n“In favoring be like over other quotatives, men in their late 20s and in their 30s display an ‘affinity’ with slightly younger women, women in their early 20s, rather than with women of their same age” (Barbieri 2007: 41)."
  },
  {
    "objectID": "chapters/01-basics.html#barbierioldermenyounger2007-older-men-and-younger-women",
    "href": "chapters/01-basics.html#barbierioldermenyounger2007-older-men-and-younger-women",
    "title": "1.1 An example from sociolinguistics",
    "section": "",
    "text": "New quotatives in American English: be like, go, be all:\n\n“I was like ‘well if it’s not leaking, I don’t care.’”\n“Do you, I asked I was running out and I go, ‘you’re trying to deliver a pizza here?’”\n“My sister’s all ‘excuse me would you mind if I gave you, if I want your autograph.’”\n\n\n\n\n\nPrevious results with regard to age: Be like is known to be preferred by younger speakers.\nPrevious results with regard to sex are inconclusive.\n\n\n\n\n\n“The study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all” (Barbieri 2007: 26).\n\n\n\n\n\nChoice of corpus: 450,000 words taken from the Conversation component of the Longman Spoken and Written English Corpus (all files where the age and gender of speakers were available), i.e., about 50 hours of casual conversation coded for speakers’ age and sex.\nData extraction: All forms of be like, be all, go, say extracted using corpus linguistic software; data entered into spreadsheet software; word counts of texts added via Delphi program.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuotative\nAge group\nMale use (per 100,000 words)\nFemale use (per 100,000 words)\n\n\n\n\nsay\n16–26\n115\n149\n\n\n\n27–40\n101\n117\n\n\n\n41–54\n55\n214\n\n\n\n55+\n67\n433\n\n\nbe like\n16–26\n32\n329\n\n\n\n27–40\n66\n25\n\n\n\n41–54\n-\n1\n\n\n\n55+\n-\n19\n\n\ngo\n16–26\n25\n113\n\n\n\n27–40\n61\n18\n\n\n\n41–54\n-\n15\n\n\n\n55+\n-\n7\n\n\nbe all\n16–26\n14\n33\n\n\n\n\n\n\n\n\nReproduction of Barbieri’s Figure 5 ‘Proportional quotative use by men and women aged 16–26 and 27–40’\n\n\n\n\n\n\n\n\n“In sum, the patterns of proportional use and the results of the statistical analyses show that there are striking differences in the way that men and women under the age of forty use quotative verbs. In contrast, in conforming to the traditional quotative say, men and women over forty displayed similar behavior” (Barbieri 2007: 38–39).\n\n\n\n\n\n“In favoring be like over other quotatives, men in their late 20s and in their 30s display an ‘affinity’ with slightly younger women, women in their early 20s, rather than with women of their same age” (Barbieri 2007: 41)."
  },
  {
    "objectID": "chapters/01-basics.html#principles-of-empirical-linguistics",
    "href": "chapters/01-basics.html#principles-of-empirical-linguistics",
    "title": "1.1 An example from sociolinguistics",
    "section": "Principles of Empirical Linguistics",
    "text": "Principles of Empirical Linguistics\n\nObjectivity — Independence from researchers or devices (→ replicability!)\nReliability — Studies should be replicable.\nValidity — A study must actually address the problem formulated in the research question."
  },
  {
    "objectID": "chapters/01-basics.html#exercises",
    "href": "chapters/01-basics.html#exercises",
    "title": "1.1 An example from sociolinguistics",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1 As you read through the sections above, think about what might go wrong at each stage of the study!\n\n\nExercise 2 Read another sociolinguistic study (e.g., Unuabonah and Gut 2018). As you read, identify similar key sections or “building blocks” used in this study (e.g., introduction, research question etc.). What purpose does each section serve in the study’s overall structure?"
  },
  {
    "objectID": "chapters/02-exploring_rstudio.html",
    "href": "chapters/02-exploring_rstudio.html",
    "title": "Exploring RStudio",
    "section": "",
    "text": "Once you’ve opened RStudio, you will see several empty panes similar to this:\n\n\n\n\n\n\n\nLet’s focus on the console window on the left. This is where we can directly communicate with R by entering “code”. The way this code is written follows certain arbitrary conventions – just like natural languages such as English or German. Here is an example in the R programming language:\n\nprint(\"Hello world!\")\n\n[1] \"Hello world!\"\n\n\nEntering this command into your console and pressing ENTER will display the sentence “Hello world!” in a new line.\nAs we can see, anything that R should understand as a simple sequence of letters or words must be enclosed by quotation marks \"...\". Anything inside them will be interpreted as a so-called string. Their counterpart are numbers or integers, as illustrated here:\n\n2 + 2 - 1\n\n[1] 3\n\n\nNaturally, you can use R for more sophisticated computations:\n\n\\(2 + (3 \\times 3)\\)\n\n2 + (3 * 3)\n\n[1] 11\n\n\n\\(\\sqrt{9}\\)\n\nsqrt(9)\n\n[1] 3\n\n\n\\(\\frac{16}{2^3}\\)\n\n16 / 2 ^ 3\n\n[1] 2\n\n\n\n\n\n\nWhile it is certainly useful to print text or numbers to your console, it may sometimes make more sense to (at least temporally) store them somewhere, so you can re-use them later. In fact, R gives us a way of storing data in virtual, container-like objects: variables. We can assign strings or numbers to a variable by using the assignment operator &lt;-.\nWhen you run the commands below, you will (hopefully) notice two items popping up in your Environment/Workspace tab in the top right corner.\n\ngreeting &lt;- \"Hello world!\"\n\nquick_math &lt;- 2 + 2 - 1\n\nIf we now want to display the content in the console, we can simply apply the print() function to the variable:\n\nprint(greeting)\n\n[1] \"Hello world!\"\n\nprint(quick_math)\n\n[1] 3\n\n\nWe can also embed variables in other statements. For example, let’s take the content of quick_math and multiply it with 2.\n\nhard_math &lt;- quick_math * 2\n\nprint(hard_math)\n\n[1] 6\n\n\n\n\n\nWorking with the console has a very plain, yet important disadvantage: Once we close RStudio, the console is wiped clean, erasing everything we’ve typed into it during our precious R session.\nThe remedy for this nuisance are scripts. Essentially, a script is the programmer’s equivalent of a Word document: It allows you to save all the code you’ve written to a file, which you can seamlessly continue working on the next time you open it.\n\nTo create a new R script you can either go to:\n\n“File” \\(\\rightarrow\\) “New” \\(\\rightarrow\\) “R Script” or …\nclick on the icon with the + sign and select “R Script” …\nor simply press Ctrl+Shift+N (MacOS: Cmd+Shift+N)\n\n\nDon’t forget to save your script with Ctrl+S (Cmd+S)! It is good practice to save your files regularly."
  },
  {
    "objectID": "chapters/02-exploring_rstudio.html#the-rstudio-interface",
    "href": "chapters/02-exploring_rstudio.html#the-rstudio-interface",
    "title": "Exploring RStudio",
    "section": "",
    "text": "Once you’ve opened RStudio, you will see several empty panes similar to this:\n\n\n\n\n\n\n\nLet’s focus on the console window on the left. This is where we can directly communicate with R by entering “code”. The way this code is written follows certain arbitrary conventions – just like natural languages such as English or German. Here is an example in the R programming language:\n\nprint(\"Hello world!\")\n\n[1] \"Hello world!\"\n\n\nEntering this command into your console and pressing ENTER will display the sentence “Hello world!” in a new line.\nAs we can see, anything that R should understand as a simple sequence of letters or words must be enclosed by quotation marks \"...\". Anything inside them will be interpreted as a so-called string. Their counterpart are numbers or integers, as illustrated here:\n\n2 + 2 - 1\n\n[1] 3\n\n\nNaturally, you can use R for more sophisticated computations:\n\n\\(2 + (3 \\times 3)\\)\n\n2 + (3 * 3)\n\n[1] 11\n\n\n\\(\\sqrt{9}\\)\n\nsqrt(9)\n\n[1] 3\n\n\n\\(\\frac{16}{2^3}\\)\n\n16 / 2 ^ 3\n\n[1] 2\n\n\n\n\n\n\nWhile it is certainly useful to print text or numbers to your console, it may sometimes make more sense to (at least temporally) store them somewhere, so you can re-use them later. In fact, R gives us a way of storing data in virtual, container-like objects: variables. We can assign strings or numbers to a variable by using the assignment operator &lt;-.\nWhen you run the commands below, you will (hopefully) notice two items popping up in your Environment/Workspace tab in the top right corner.\n\ngreeting &lt;- \"Hello world!\"\n\nquick_math &lt;- 2 + 2 - 1\n\nIf we now want to display the content in the console, we can simply apply the print() function to the variable:\n\nprint(greeting)\n\n[1] \"Hello world!\"\n\nprint(quick_math)\n\n[1] 3\n\n\nWe can also embed variables in other statements. For example, let’s take the content of quick_math and multiply it with 2.\n\nhard_math &lt;- quick_math * 2\n\nprint(hard_math)\n\n[1] 6\n\n\n\n\n\nWorking with the console has a very plain, yet important disadvantage: Once we close RStudio, the console is wiped clean, erasing everything we’ve typed into it during our precious R session.\nThe remedy for this nuisance are scripts. Essentially, a script is the programmer’s equivalent of a Word document: It allows you to save all the code you’ve written to a file, which you can seamlessly continue working on the next time you open it.\n\nTo create a new R script you can either go to:\n\n“File” \\(\\rightarrow\\) “New” \\(\\rightarrow\\) “R Script” or …\nclick on the icon with the + sign and select “R Script” …\nor simply press Ctrl+Shift+N (MacOS: Cmd+Shift+N)\n\n\nDon’t forget to save your script with Ctrl+S (Cmd+S)! It is good practice to save your files regularly."
  },
  {
    "objectID": "chapters/02-first_steps.html",
    "href": "chapters/02-first_steps.html",
    "title": "First steps",
    "section": "",
    "text": "When it comes to data analysis, learning R offers an overwhelming number of short- and long-term advantages over conventional spreadsheet software such as Microsoft Excel or LibreOffice Calc:\n\nFirst of all, it’s completely free. There’s no need to obtain any expensive licenses, as it is the case for commercial software such as SPSS or MS Excel.\nR makes it very easy to document and share every step of the analysis, thereby facilitating reproducible workflows.\nLarge (and by that I mean extremely large!) datasets pose no problems whatsoever. Loading tabular data with hundreds of thousands (or even millions) of rows only takes a few seconds, whereas most other software would crash.\nThere are numerous extensions that provide tailored functions for corpus linguistics that aren’t available in general-purpose spreadsheet software. This allows us to work with corpora, use complex search expression, perform part-of-speech annotation, dependency parsing, and much more – all from within R.\nR’s ggplot2 offers an incredibly powerful framework for data visualisation. Don’t believe it? Check out the ggplot2 gallery.\nThe CRAN repository features more than 20,000 packages that can be installed to expand the functionality of R almost indefinitely. Should none of them meet your needs, R gives you the tools to comfortably write and share your own functions and packages."
  },
  {
    "objectID": "chapters/02-first_steps.html#why-learn-r-to-begin-with",
    "href": "chapters/02-first_steps.html#why-learn-r-to-begin-with",
    "title": "First steps",
    "section": "",
    "text": "When it comes to data analysis, learning R offers an overwhelming number of short- and long-term advantages over conventional spreadsheet software such as Microsoft Excel or LibreOffice Calc:\n\nFirst of all, it’s completely free. There’s no need to obtain any expensive licenses, as it is the case for commercial software such as SPSS or MS Excel.\nR makes it very easy to document and share every step of the analysis, thereby facilitating reproducible workflows.\nLarge (and by that I mean extremely large!) datasets pose no problems whatsoever. Loading tabular data with hundreds of thousands (or even millions) of rows only takes a few seconds, whereas most other software would crash.\nThere are numerous extensions that provide tailored functions for corpus linguistics that aren’t available in general-purpose spreadsheet software. This allows us to work with corpora, use complex search expression, perform part-of-speech annotation, dependency parsing, and much more – all from within R.\nR’s ggplot2 offers an incredibly powerful framework for data visualisation. Don’t believe it? Check out the ggplot2 gallery.\nThe CRAN repository features more than 20,000 packages that can be installed to expand the functionality of R almost indefinitely. Should none of them meet your needs, R gives you the tools to comfortably write and share your own functions and packages."
  },
  {
    "objectID": "chapters/02-first_steps.html#installing-r",
    "href": "chapters/02-first_steps.html#installing-r",
    "title": "First steps",
    "section": "2 Installing R",
    "text": "2 Installing R\nThe first step involves downloading the R programming language itself. The link will take you to the homepage of the Comprehensive R Archive Network (CRAN) where you can download the binary distribution. Choose the one that corresponds to your operating system (Windows/MAC/Linux).\n\n\n\n\n\n\nInstallation instructions for Windows users\n\n\n\n\n\nClick “Download R for Windows” \\(\\rightarrow\\) Select “base” \\(\\rightarrow\\) Click on “Download R-4.4.1 for Windows” (or whatever most recent version is currently displayed).\nOpen the set-up file you’ve just downloaded and simply follow the instructions on screen. It’s fine to go with the default options.\nVideo tutorial on YouTube\n\n\n\n\n\n\n\n\n\nInstallation instructions for MacOS users\n\n\n\n\n\nClick “Download R for macOS” \\(\\rightarrow\\) Select the latest release for your OS\nOpen the downloaded .pkg file and follow the instructions in the installation window.\nVideo tutorial on YouTube"
  },
  {
    "objectID": "chapters/02-first_steps.html#installing-rstudio",
    "href": "chapters/02-first_steps.html#installing-rstudio",
    "title": "First steps",
    "section": "3 Installing RStudio",
    "text": "3 Installing RStudio\nYou can now download and install RStudio. RStudio is a so-called “Integrated Development Environment” (IDE), which will provide us with a variety of helpful tools to write and edit code comfortably. If R was a musical instrument, then RStudio would be the recording studio, so-to-speak."
  }
]