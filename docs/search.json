[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Overview",
    "section": "",
    "text": "This collection of handouts provides a hands-on introduction to data analysis and statistical methods in quantitative corpus linguistics with R. It is designed with accessibility in mind, assuming no prior knowledge of programming or statistics. All you need to get started is a laptop; everything else will be explained within these pages.\nPrimarily, this reader is geared towards students attending the classes Language Variation (BA) and Statistics for Linguistics (MA) at the Catholic University of Eichstätt-Ingolstadt (Germany). However, it is also meant to equip students currently working on their BA/MA/PhD theses with the tools they need to conduct empirical studies on a wide array of linguistic phenomena. The methods presented here reflect the state-of-the-art in corpus-linguistic research, providing readers with current and relevant analytical techniques.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "blog.html#fundamentals-of-corpus-based-research",
    "href": "blog.html#fundamentals-of-corpus-based-research",
    "title": "Overview",
    "section": "Fundamentals of Corpus-based Research",
    "text": "Fundamentals of Corpus-based Research\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\n1.1 Basics\n\n\nA short introduction to the basic structure of a sociolinguistic study.\n\n\n\n\n1.2 Research questions\n\n\nWhat is a research question and do I write a good one?\n\n\n\n\n1.3 Linguistic variables\n\n\nThis handout introduces linguistic variables from classical and sociolinguistic perspectives, explores their subtypes and salience, discusses the principle of accountability, and provides examples of morphosyntactic variation in English.\n\n\n\n\n1.4 Set theory and mathematical notation\n\n\nThis unit introduces key concepts from set theory and mathematical notation – including sets, subsets, unions, intersections, sums, and products—to build foundational skills for formal reasoning in corpus linguistics.\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "blog.html#introduction-to-r",
    "href": "blog.html#introduction-to-r",
    "title": "Overview",
    "section": "Introduction to R",
    "text": "Introduction to R\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\n2.1 First steps\n\n\n\n\n\n\n\n2.2 Exploring RStudio\n\n\nIntroduction to the RStudio interface, illustrating how to interact with R via the console, store and reuse values using variables, and preserve your work using R scripts.\n\n\n\n\n2.3 Vectors\n\n\nWe introduce vectors as our first data structure in R and explore some common applications and manipulations.\n\n\n\n\n2.4 Data frames\n\n\nThis unit introduces data frames in R, covering their creation, subsetting, filtering (including with base R and tidyverse), and includes practice exercises on accessing and manipulating structured linguistic data.\n\n\n\n\n2.5 Libraries\n\n\n\n\n\n\n\n2.6 Import/export data\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "blog.html#nlp-with-r",
    "href": "blog.html#nlp-with-r",
    "title": "Overview",
    "section": "NLP with R",
    "text": "NLP with R\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\n3.1 Concordancing\n\n\n\n\n\n\n\n3.2 Regular expressions\n\n\n\n\n\n\n\n3.3 Data annotation\n\n\nExemplifying the data collection and annotation workflow.\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "blog.html#statistics",
    "href": "blog.html#statistics",
    "title": "Overview",
    "section": "Statistics",
    "text": "Statistics\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\n4.1 Data, variables, samples\n\n\nThis handout introduces key statistical concepts—such as samples, populations, variables, datasets, and data types—with a focus on their application to empirical linguistic research, using R to explore and illustrate these ideas. \n\n\n\n\n4.2 Probability theory\n\n\nThis section covers some essential concepts from probability theory, such as the concept of probability, probability distributions, and exepectations.\n\n\n\n\n4.3 Descriptive statistics\n\n\n\n\n\n\n\n4.4 Continuous data\n\n\n\n\n\n\n\n4.4 Hypothesis testing\n\n\n\n\n\n\n\n4.5 Binomial test\n\n\nA binomial distribution is a probability distribution which can be found in an experiment with only two possible outcomes (0 and 1, also referred to as failure and success, respectively) and which are independent of each other in case of repetition. \n\n\n\n\n4.6 Chi-squared test\n\n\nThe chi-squared (\\(\\chi^2\\)) test helps determine if there is a statistically significant association between two categorical variables. It compares the observed frequencies of categories with those expected under the null hypothesis. The \\(\\chi^2\\) (chi-squared) score quantifies the difference between observed and expected frequencies for every cell in a contingency table. The greater the difference between observed and expected, the higher the \\(\\chi^2\\) score and the lower the \\(p\\)-value, given the degrees of freedom. It is recommended to compute effect size measures and inspect the residuals to assess the nature of the association. \n\n\n\n\n4.7 t-test\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "blog.html#models",
    "href": "blog.html#models",
    "title": "Overview",
    "section": "Models",
    "text": "Models\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\n6.1 Linear regression\n\n\nModelling continuous response variables.\n\n\n\n\n6.2 Logistic Regression\n\n\nModelling categorical (binary) response variables.\n\n\n\n\n6.3 Mixed-effects regression\n\n\n\n\n\n\n\n6.4 Poisson regression\n\n\nModelling count data.\n\n\n\n\n6.5 Ordinal regression\n\n\nModelling categorical (ordinal) response variables.\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "blog.html#machine-learning",
    "href": "blog.html#machine-learning",
    "title": "Overview",
    "section": "Machine Learning",
    "text": "Machine Learning\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\n7.1 Tree-based methods\n\n\nWe familiarise ourselves with powerful non-parametric models based on recursive partitioning.\n\n\n\n\n7.2 Gradient boosting\n\n\nGradient boosting constitutes a powerful extension of tree-based methods and is generally appreciated for its high predictive performance. Nevertheless, this family of methods, which includes implementations such as AdaBoost, XGBoost, and CatBoost, among many others, is not yet established in corpus-linguistic statistics. A practical scenario is presented to introduce the core ideas of gradient boosting, demonstrate its application to linguistic data as well as point out its advantages and drawbacks. \n\n\n\n\n7.3 Principal Components Analysis\n\n\n\n\n\n\n\n7.4 Exploratory Factor Analysis\n\n\n\n\n\n\n\n7.5 Clustering\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Exploring_RStudio.html",
    "href": "Exploring_RStudio.html",
    "title": "Exploring RStudio",
    "section": "",
    "text": "Once you’ve opened RStudio, you will see several empty panes similar to this:\n\n\n\n\n\n\n\nLet’s focus on the console window on the left. This is where we can directly communicate with R by entering “code”. The way this code is written follows certain arbitrary conventions – just like natural languages such as English or German. Here is an example in the R programming language:\n\nprint(\"Hello world!\")\n\n[1] \"Hello world!\"\n\n\nEntering this command into your console and pressing ENTER will display the sentence “Hello world!” in a new line.\nAs we can see, anything that R should understand as a simple sequence of letters or words must be enclosed by quotation marks \"...\". Anything inside them will be interpreted as a so-called string. Their counterpart are numbers or integers, as illustrated here:\n\n2 + 2 - 1\n\n[1] 3\n\n\nNaturally, you can use R for more sophisticated computations:\n\n\\(2 + (3 \\times 3)\\)\n\n2 + (3 * 3)\n\n[1] 11\n\n\n\\(\\sqrt{9}\\)\n\nsqrt(9)\n\n[1] 3\n\n\n\\(\\frac{16}{2^3}\\)\n\n16 / 2 ^ 3\n\n[1] 2\n\n\n\n\n\n\nWhile it is certainly useful to print text or numbers to your console, it may sometimes make more sense to (at least temporally) store them somewhere, so you can re-use them later. In fact, R gives us a way of storing data in virtual, container-like objects: variables. We can assign strings or numbers to a variable by using the assignment operator &lt;-.\nWhen you run the commands below, you will (hopefully) notice two items popping up in your Environment/Workspace tab in the top right corner.\n\ngreeting &lt;- \"Hello world!\"\n\nquick_math &lt;- 2 + 2 - 1\n\nIf we now want to display the content in the console, we can simply apply the print() function to the variable:\n\nprint(greeting)\n\n[1] \"Hello world!\"\n\nprint(quick_math)\n\n[1] 3\n\n\nWe can also embed variables in other statements. For example, let’s take the content of quick_math and multiply it with 2.\n\nhard_math &lt;- quick_math * 2\n\nprint(hard_math)\n\n[1] 6\n\n\n\n\n\nWorking with the console has a very plain, yet important disadvantage: Once we close RStudio, the console is wiped clean, erasing everything we’ve typed into it during our precious R session.\nThe remedy for this nuisance are scripts. Essentially, a script is the programmer’s equivalent of a Word document: It allows you to save all the code you’ve written to a file, which you can seamlessly continue working on the next time you open it.\n\nTo create a new R script you can either go to:\n\n“File” \\(\\rightarrow\\) “New” \\(\\rightarrow\\) “R Script” or …\nclick on the icon with the + sign and select “R Script” …\nor simply press Ctrl+Shift+N (MacOS: Cmd+Shift+N)\n\n\nDon’t forget to save your script with Ctrl+S (Cmd+S)! It is good practice to save your files regularly."
  },
  {
    "objectID": "Exploring_RStudio.html#the-rstudio-interface",
    "href": "Exploring_RStudio.html#the-rstudio-interface",
    "title": "Exploring RStudio",
    "section": "",
    "text": "Once you’ve opened RStudio, you will see several empty panes similar to this:\n\n\n\n\n\n\n\nLet’s focus on the console window on the left. This is where we can directly communicate with R by entering “code”. The way this code is written follows certain arbitrary conventions – just like natural languages such as English or German. Here is an example in the R programming language:\n\nprint(\"Hello world!\")\n\n[1] \"Hello world!\"\n\n\nEntering this command into your console and pressing ENTER will display the sentence “Hello world!” in a new line.\nAs we can see, anything that R should understand as a simple sequence of letters or words must be enclosed by quotation marks \"...\". Anything inside them will be interpreted as a so-called string. Their counterpart are numbers or integers, as illustrated here:\n\n2 + 2 - 1\n\n[1] 3\n\n\nNaturally, you can use R for more sophisticated computations:\n\n\\(2 + (3 \\times 3)\\)\n\n2 + (3 * 3)\n\n[1] 11\n\n\n\\(\\sqrt{9}\\)\n\nsqrt(9)\n\n[1] 3\n\n\n\\(\\frac{16}{2^3}\\)\n\n16 / 2 ^ 3\n\n[1] 2\n\n\n\n\n\n\nWhile it is certainly useful to print text or numbers to your console, it may sometimes make more sense to (at least temporally) store them somewhere, so you can re-use them later. In fact, R gives us a way of storing data in virtual, container-like objects: variables. We can assign strings or numbers to a variable by using the assignment operator &lt;-.\nWhen you run the commands below, you will (hopefully) notice two items popping up in your Environment/Workspace tab in the top right corner.\n\ngreeting &lt;- \"Hello world!\"\n\nquick_math &lt;- 2 + 2 - 1\n\nIf we now want to display the content in the console, we can simply apply the print() function to the variable:\n\nprint(greeting)\n\n[1] \"Hello world!\"\n\nprint(quick_math)\n\n[1] 3\n\n\nWe can also embed variables in other statements. For example, let’s take the content of quick_math and multiply it with 2.\n\nhard_math &lt;- quick_math * 2\n\nprint(hard_math)\n\n[1] 6\n\n\n\n\n\nWorking with the console has a very plain, yet important disadvantage: Once we close RStudio, the console is wiped clean, erasing everything we’ve typed into it during our precious R session.\nThe remedy for this nuisance are scripts. Essentially, a script is the programmer’s equivalent of a Word document: It allows you to save all the code you’ve written to a file, which you can seamlessly continue working on the next time you open it.\n\nTo create a new R script you can either go to:\n\n“File” \\(\\rightarrow\\) “New” \\(\\rightarrow\\) “R Script” or …\nclick on the icon with the + sign and select “R Script” …\nor simply press Ctrl+Shift+N (MacOS: Cmd+Shift+N)\n\n\nDon’t forget to save your script with Ctrl+S (Cmd+S)! It is good practice to save your files regularly."
  },
  {
    "objectID": "chapters/03-concordancing.html",
    "href": "chapters/03-concordancing.html",
    "title": "3.1 Concordancing",
    "section": "",
    "text": "In-depth introduction to concordancing with R:\n\n\nSchweinberger (2024)\n\n\nNatural Language Processing (NLP) with quanteda:\n\n\nBenoit et al. (2018)\nOnline reference\n\n\nOn corpus-linguistic theory:\n\n\nWulff and Baker (2020)\nLange and Leuckert (2020)\nMcEnery, Xiao, and Yukio (2006)",
    "crumbs": [
      "3. NLP with R",
      "3.1 Concordancing"
    ]
  },
  {
    "objectID": "chapters/03-concordancing.html#recommended-reading",
    "href": "chapters/03-concordancing.html#recommended-reading",
    "title": "3.1 Concordancing",
    "section": "",
    "text": "In-depth introduction to concordancing with R:\n\n\nSchweinberger (2024)\n\n\nNatural Language Processing (NLP) with quanteda:\n\n\nBenoit et al. (2018)\nOnline reference\n\n\nOn corpus-linguistic theory:\n\n\nWulff and Baker (2020)\nLange and Leuckert (2020)\nMcEnery, Xiao, and Yukio (2006)",
    "crumbs": [
      "3. NLP with R",
      "3.1 Concordancing"
    ]
  },
  {
    "objectID": "chapters/03-concordancing.html#preparation",
    "href": "chapters/03-concordancing.html#preparation",
    "title": "3.1 Concordancing",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nScript\n\n\n\nYou can find the full R script associated with this unit here.\n\n\n\n\n\n\n\n\nWorking directory\n\n\n\nIn order for R to be able to recognise the data, it is crucial to set up the working directory correctly.\n\nMake sure your R-script and the corpus (e.g., ‘ICE-GB’) are stored in the same folder on your computer.\nIn RStudio, go to the Files pane (usually in the bottom-right corner) and navigate to the location of your script. Alternatively, you can click on the three dots ... and use the file browser instead.\nOnce you’re in the correct folder, click on the blue ⚙️ icon.\nSelect Set As Working Directory. This action will update your working directory to the folder where the file is located.\n\n\n\nIn addition, make sure you have installed quanteda. Load it at the beginning of your script:\n\nlibrary(quanteda) # Package for Natural Language Processing in R\nlibrary(lattice) # for dotplots\n\nTo load a corpus object into R, place it in your working directory and read it into your working environment with readRDS().1\n1 The ICE-GB.RDS file you’ve been provided with has been pre-processed and saved in this specific format for practical reasons.\n# Load corpus from directory\nICE_GB &lt;- readRDS(\"../datasets/ICE_GB.RDS\")\n\nIf you encounter any error messages at this stage, ensure you followed steps 1 and 2 in the callout box above.",
    "crumbs": [
      "3. NLP with R",
      "3.1 Concordancing"
    ]
  },
  {
    "objectID": "chapters/03-concordancing.html#concordancing",
    "href": "chapters/03-concordancing.html#concordancing",
    "title": "3.1 Concordancing",
    "section": "Concordancing",
    "text": "Concordancing\nA core task in corpus-linguistic research involves finding occurrences of a single word or multi-word sequence in the corpus. Lange & Leuckert (2020: 55) explain that specialised software typically “provide[s] the surrounding context as well as the name of the file in which the word could be identified.” Inspecting the context is particularly important in comparative research, as it may be indicative of distinct usage patterns.\n\nSimple queries\nTo obtain such a keyword in context (KWIC) in R, we use the kwic() function. We supply the corpus as well as the keyword we’re interested in:\n\nquery1 &lt;- kwic(ICE_GB, \"eat\")\n\nThe output in query1 contains concordance lines that list all occurrences of the keyword, including the document, context to the left, the keyword itself, and the context to the right. The final column reiterates our search expression.\n\nhead(query1)\n\nKeyword-in-context with 6 matches.                                                            \n  [ICE_GB/S1A-006.txt, 785]           So I' d rather | eat |\n [ICE_GB/S1A-009.txt, 1198]              I must &lt;, &gt; | eat |\n  [ICE_GB/S1A-010.txt, 958]         to &lt;, &gt; actually | eat |\n  [ICE_GB/S1A-018.txt, 455] order one first and then | eat |\n  [ICE_GB/S1A-018.txt, 498]  A &gt; The bargain hunting | eat |\n [ICE_GB/S1A-023.txt, 1853]       B &gt; Oh name please | eat |\n                            \n beforehand just to avoid uh\n them &lt; ICE-GB:S1A-009#71:  \n it for one' s              \n it and then sort of        \n &lt; ICE-GB:S1A-018#29: 1     \n something &lt;,, &gt;            \n\n\nFor a full screen display of the KWIC data frame, try View():\n\nView(query1)\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nICE_GB/S1A-006.txt\n785\n785\nSo I ' d rather\neat\nbeforehand just to avoid uh\neat\n\n\nICE_GB/S1A-009.txt\n1198\n1198\nI must &lt; , &gt;\neat\nthem &lt; ICE-GB:S1A-009 #71 :\neat\n\n\nICE_GB/S1A-010.txt\n958\n958\nto &lt; , &gt; actually\neat\nit for one ' s\neat\n\n\nICE_GB/S1A-018.txt\n455\n455\norder one first and then\neat\nit and then sort of\neat\n\n\nICE_GB/S1A-018.txt\n498\n498\nA &gt; The bargain hunting\neat\n&lt; ICE-GB:S1A-018 #29 : 1\neat\n\n\nICE_GB/S1A-023.txt\n1853\n1853\nB &gt; Oh name please\neat\nsomething &lt; , , &gt;\neat\n\n\n\n\n\n\n\n\n\nMulti-word queries\nIf the search expression exceeds a single word, we need to mark it as a multi-word sequence by means of the phrase() function. For instance, if we were interested in the pattern eat a, we’d have to adjust the code as follows:\n\nquery2 &lt;- kwic(ICE_GB, phrase(\"eat a\"))\n\n\nView(query2)\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nICE_GB/S1A-059.txt\n2230\n2231\n1 : B &gt; I\neat a\n&lt; , &gt; very balanced\neat a\n\n\nICE_GB/W2B-014.txt\n1045\n1046\n: 1 &gt; We can't\neat a\nlot of Welsh or Scottish\neat a\n\n\nICE_GB/W2B-022.txt\n589\n590\nhave few labour-saving devices ,\neat a\ndiet low in protein ,\neat a\n\n\n\n\n\n\n\n\n\nMultiple simultaneous queries\nA very powerful advantage of quanteda over traditional corpus software is that we can query a corpus for a multitude of keywords at the same time. Say, we need our output to contain hits for eat, drink as well as sleep. Instead of a single keyword, we supply a character vector containing the strings of interest.\n\nquery3 &lt;- kwic(ICE_GB, c(\"eat\", \"drink\", \"sleep\"))\n\n\nView(query3)\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nICE_GB/S1A-006.txt\n785\n785\nSo I ' d rather\neat\nbeforehand just to avoid uh\neat\n\n\nICE_GB/S1A-009.txt\n869\n869\n: A &gt; Do you\ndrink\nquite a lot of it\ndrink\n\n\nICE_GB/S1A-009.txt\n1198\n1198\nI must &lt; , &gt;\neat\nthem &lt; ICE-GB:S1A-009 #71 :\neat\n\n\nICE_GB/S1A-010.txt\n958\n958\nto &lt; , &gt; actually\neat\nit for one ' s\neat\n\n\nICE_GB/S1A-014.txt\n3262\n3262\nyou were advised not to\ndrink\nwater in Leningrad because they\ndrink\n\n\nICE_GB/S1A-016.txt\n3290\n3290\n&gt; I couldn't I couldn't\nsleep\nif I didn't read &lt;\nsleep\n\n\n\n\n\n\n\n\n\nWindow size\nSome studies require more detailed examination of the preceding or following context of the keyword. We can easily adjust the window size to suit our needs:\n\nquery4 &lt;- kwic(ICE_GB, \"eat\", window = 20) \n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nICE_GB/S1A-006.txt\n785\n785\n#49 : 1 : A &gt; Yeah &lt; ICE-GB:S1A-006 #50 : 1 : A &gt; So I ' d rather\neat\nbeforehand just to avoid uh &lt; , , &gt; any problems there &lt; ICE-GB:S1A-006 #51 : 1 : B &gt;\neat\n\n\nICE_GB/S1A-009.txt\n1198\n1198\n&lt; , &gt; in in the summer &lt; ICE-GB:S1A-009 #70 : 1 : A &gt; I must &lt; , &gt;\neat\nthem &lt; ICE-GB:S1A-009 #71 : 1 : A &gt; Yes &lt; ICE-GB:S1A-009 #72 : 1 : B &gt; You ought\neat\n\n\nICE_GB/S1A-010.txt\n958\n958\n1 : B &gt; You know I mean it would seem to be squandering it to &lt; , &gt; actually\neat\nit for one ' s own enjoyment &lt; , , &gt; &lt; ICE-GB:S1A-010 #49 : 1 : A &gt; Mm\neat\n\n\nICE_GB/S1A-018.txt\n455\n455\ns so &lt; ICE-GB:S1A-018 #27 : 1 : A &gt; What you should do is order one first and then\neat\nit and then sort of carry on from there &lt; laughter &gt; &lt; , &gt; by which time you wouldn't\neat\n\n\nICE_GB/S1A-018.txt\n498\n498\nsecond anyway so &lt; laugh &gt; &lt; , &gt; &lt; ICE-GB:S1A-018 #28 : 1 : A &gt; The bargain hunting\neat\n&lt; ICE-GB:S1A-018 #29 : 1 : B &gt; So all right what did I have &lt; ICE-GB:S1A-018 #30 : 1\neat\n\n\nICE_GB/S1A-023.txt\n1853\n1853\n&gt; I can't bear it &lt; , , &gt; &lt; ICE-GB:S1A-023 #121 : 1 : B &gt; Oh name please\neat\nsomething &lt; , , &gt; &lt; ICE-GB:S1A-023 #122 : 1 : A &gt; Oh actually Dad asked me if &lt;\neat\n\n\n\n\n\n\n\n\n\nSaving your output\nYou can store your results in a spreadsheet file just as described in the unit on importing and exporting data.\n\nMicrosoft Excel (.xlsx)\n\n\nlibrary(writexl) # required for writing files to MS Excel\n\nwrite_xlsx(query1, \"myresults1.xlsx\")\n\n\nLibreOffice (.csv)\n\n\nwrite.csv(query1, \"myresults1.csv\")\n\nAs soon as you have annotated your data, you can load .xlsx files back into R with read_xlsx() from the readxl package and .csv files using the Base R function read.csv().",
    "crumbs": [
      "3. NLP with R",
      "3.1 Concordancing"
    ]
  },
  {
    "objectID": "chapters/03-concordancing.html#characterising-the-output",
    "href": "chapters/03-concordancing.html#characterising-the-output",
    "title": "3.1 Concordancing",
    "section": "Characterising the output",
    "text": "Characterising the output\nRecall our initial query of the eat, whose output we stored in query1:\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nICE_GB/S1A-006.txt\n785\n785\nSo I ' d rather\neat\nbeforehand just to avoid uh\neat\n\n\nICE_GB/S1A-009.txt\n1198\n1198\nI must &lt; , &gt;\neat\nthem &lt; ICE-GB:S1A-009 #71 :\neat\n\n\nICE_GB/S1A-010.txt\n958\n958\nto &lt; , &gt; actually\neat\nit for one ' s\neat\n\n\nICE_GB/S1A-018.txt\n455\n455\norder one first and then\neat\nit and then sort of\neat\n\n\nICE_GB/S1A-018.txt\n498\n498\nA &gt; The bargain hunting\neat\n&lt; ICE-GB:S1A-018 #29 : 1\neat\n\n\nICE_GB/S1A-023.txt\n1853\n1853\nB &gt; Oh name please\neat\nsomething &lt; , , &gt;\neat\n\n\n\n\n\n\n\nFirst, we may be interested in obtaining some general information on our results, such as …\n\n… how many tokens (= individual hits) does the query return?\n\nThe nrow() function counts the number of rows in a data frame — these always correspond to the number of observations in our sample (here: 53).\n\nnrow(query1)\n\n[1] 53\n\n\n\n… how many types (= distinct hits) does the query return?\n\nApparently, there are 52 counts of eat in lower case and 1 in upper case. Their sum corresponds to our 53 observations in total.\n\ntable(query1$keyword)\n\n\neat Eat \n 52   1 \n\n\n\n… how is the keyword distributed across corpus files?\n\nThis question relates to the notion of dispersion: Is a keyword spread relatively evenly across corpus files or does it only occur in specific ones?\n\n# Frequency of keyword by docname\nquery_distrib &lt;- table(query1$docname, query1$keyword)\n\n# Show first few rows\nhead(query_distrib)\n\n                    \n                     eat Eat\n  ICE_GB/S1A-006.txt   1   0\n  ICE_GB/S1A-009.txt   1   0\n  ICE_GB/S1A-010.txt   1   0\n  ICE_GB/S1A-018.txt   2   0\n  ICE_GB/S1A-023.txt   1   0\n  ICE_GB/S1A-025.txt   1   0\n\n# Create a simple dot plot\ndotplot(query_distrib, auto.key = list(columns = 2, title = \"Tokens\", cex.title = 1))\n\n\n\n\n\n\n\n# Create a fancy plot (requires tidyverse)\nggplot(query1, aes(x = keyword)) + \n  geom_bar() +\n  facet_wrap(~docname)\n\n\n\n\n\n\n\n\nIt seems that eat occurs at least once in most text categories (both spoken and written), but seems to be much more common in face-to-face conversations (S1A). This is not surprising: It is certainly more common to discuss food in a casual chat with friends than in an academic essay (unless, of course, its main subject matter is food). Dispersion measures can thus be viewed as indicators of contextual preferences associated with lexemes or more grammatical patterns.\n\n\n\n\n\n\nAdvanced: More on dispersion\n\n\n\n\n\nThe empirical study of dispersion has attracted a lot of attention in recent years Gries (2020). A reason for this is the necessity of finding a dispersion measure that is minimally correlated with token frequency. One such measure is the Kullback-Leibler divergence \\(KLD\\), which comes from the field of information theory and is closely related to entropy.\nMathematically, \\(KLD\\) measures the difference between two probability distributions \\(p\\) and \\(q\\).\n\\[ KLD(p \\parallel q) = \\sum\\limits_{x \\in X} p(x) \\log \\frac{p(x)}{q(x)}\n\\tag{1}\\]\nLet \\(f\\) denote the overall frequency of a keyword in the corpus, \\(v\\) its frequency in each corpus part, \\(s\\) the sizes of each corpus part (as fractions) and \\(n\\) the total number of corpus parts. We thus compare the posterior (= “actual”) distribution of keywords \\(\\frac{v_i}{f}\\) for \\(i = 1, ..., n\\) with their prior distribution, which assumes all words are spread evenly across corpus parts (hence the division by \\(s_i\\)).\n\\[ KLD = \\sum\\limits_{i=1}^n \\frac{v_i}{f} \\times \\log_2\\left({\\frac{v_i}{f} \\times \\frac{1}{s_i}}\\right)\n\\tag{2}\\]\nIn R, let’s calculate the dispersion of the verbs eat, drink, and sleep from query3.\n\n# Let's filter out the upper-case variants:\nquery3_reduced &lt;- query3[query3$keyword %in% c(\"eat\", \"drink\", \"sleep\"),]\ntable(query3_reduced$keyword)\n\n\ndrink   eat sleep \n   48    52    41 \n\n# Extract text categories\nquery_registers &lt;- separate_wider_delim(query3_reduced, cols = docname, delim = \"-\", names = c(\"Text_category\", \"File_number\"))\n\n# Get separate data frames for each verb\neat &lt;- filter(query_registers, keyword == \"eat\")\ndrink &lt;- filter(query_registers, keyword == \"drink\")\nsleep &lt;- filter(query_registers, keyword == \"sleep\")\n\n## Get frequency distribution across files\nv_eat &lt;- table(eat$Text_category)\nv_drink &lt;- table(drink$Text_category)\nv_sleep &lt;- table(sleep$Text_category)\n\n## Get total frequencies\nf_eat &lt;- nrow(eat)\nf_drink &lt;- nrow(drink)\nf_sleep &lt;- nrow(sleep)\n\n# The next step is a little trickier. First we need to find out how many distinct corpus parts there are in the ICE corpus.\n\n## Check ICE-corpus structure and convert to data frame\nICE_GB_str &lt;- as.data.frame(summary(ICE_GB))\n\n## Separate files from text categores\nICE_GB_texts &lt;- separate_wider_delim(ICE_GB_str, cols = Var1, delim = \"-\", names = c(\"Text_category\", \"File\"))\n\n## Get number of distinct text categories\nn &lt;- length(unique(ICE_GB_texts$Text_category))\n\n## Get proportions of distinct text categories (s)\ns &lt;- table(ICE_GB_texts$Text_category)/sum(table(ICE_GB_texts$Text_category))\n\n## Unfortunately not all of these corpus parts are represented in our queries. We need to correct the proportions in s for the missing ones!\n\n## Store unique ICE text categories \nICE_unique_texts &lt;- unique(ICE_GB_texts$Text_category)\n\n## Make sure only those text proportions are included where the keywords actually occur\ns_eat &lt;- s[match(names(v_eat), ICE_unique_texts)]\ns_drink &lt;- s[match(names(v_drink), ICE_unique_texts)]\ns_sleep &lt;- s[match(names(v_sleep), ICE_unique_texts)]\n\n# Compute KLD for each verb\nkld_eat &lt;- sum(v_eat/f_eat * log2(v_eat/f_eat * 1/s_eat)); kld_eat\n\n[1] 0.6747268\n\nkld_drink &lt;- sum(v_drink/f_drink * log2(v_drink/f_drink * 1/s_drink)); kld_drink\n\n[1] 0.8463608\n\nkld_sleep &lt;- sum(v_sleep/f_sleep * log2(v_sleep/f_sleep * 1/s_sleep)); kld_sleep\n\n[1] 0.7047421\n\n# Plot\nkld_df &lt;- data.frame(kld_eat, kld_drink, kld_sleep)\n\nbarplot(as.numeric(kld_df), names.arg = names(kld_df), col = \"steelblue\",\n        xlab = \"Variable\", ylab = \"KLD Value (= deviance from even distribution)\", main = \"Dispersion of 'eat', 'drink', and 'sleep'\")\n\n\n\n\n\n\n\n\nThe plot indicates that drink is the most unevenly distributed verb out of the three considered (high KDL \\(\\sim\\) low dispersion), whereas eat appears to be slightly more evenly distributed across corpus files. The verb sleep assumes an intermediary position.",
    "crumbs": [
      "3. NLP with R",
      "3.1 Concordancing"
    ]
  },
  {
    "objectID": "chapters/03-concordancing.html#alternative-concordancing-software",
    "href": "chapters/03-concordancing.html#alternative-concordancing-software",
    "title": "3.1 Concordancing",
    "section": "Alternative concordancing software",
    "text": "Alternative concordancing software\nThere is a wide variety of concordancing software available, both free and paid. Among the most popular options are AntConc (Anthony 2020) and SketchEngine (Kilgarriff et al. 2004). However, as Schweinberger (2024) notes, the exact processes these tools use to generate output are not always fully transparent, making them something of a “black box.” In contrast, programming languages like R or Python allow researchers to document each step of their analysis clearly, providing full transparency from start to finish.\nThe following apps attempt to reconcile the need for an intuitive user interface with transparent data handling. The full source code is documented in the respective GitHub repositories.\n\nQuantedaApp is an interface for the R package quanteda (Benoit et al. 2018).\nPyConc is an interface for the Python package spaCy (Honnibal and Montani 2017).",
    "crumbs": [
      "3. NLP with R",
      "3.1 Concordancing"
    ]
  },
  {
    "objectID": "chapters/05-hypothesis_testing.html",
    "href": "chapters/05-hypothesis_testing.html",
    "title": "4.4 Hypothesis testing",
    "section": "",
    "text": "For linguists:\n\nGries (2021): Chapter 1.3.2\n\nGeneral:\n\nBaguley (2012): Chapter 4\nAgresti and Kateri (2022): Chapter 5\nDienes (2008)",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.4 Hypothesis testing"
    ]
  },
  {
    "objectID": "chapters/05-hypothesis_testing.html#suggested-reading",
    "href": "chapters/05-hypothesis_testing.html#suggested-reading",
    "title": "4.4 Hypothesis testing",
    "section": "",
    "text": "For linguists:\n\nGries (2021): Chapter 1.3.2\n\nGeneral:\n\nBaguley (2012): Chapter 4\nAgresti and Kateri (2022): Chapter 5\nDienes (2008)",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.4 Hypothesis testing"
    ]
  },
  {
    "objectID": "chapters/05-hypothesis_testing.html#sec-hyp",
    "href": "chapters/05-hypothesis_testing.html#sec-hyp",
    "title": "4.4 Hypothesis testing",
    "section": "On scientific inference",
    "text": "On scientific inference\nScience begins and ends with theory, and statistics acts as the “go-between”. Regardless of the discipline, solid research is characterised by a robust theoretical foundation that gives rise to substantive hypotheses, i.e., theory-driven predictions about a population of interest. From this rather concrete hypothesis, it should be possible to derive a statistical hypothesis that re-states the prediction in more formal/mathematical terms. After checking it against real-world data, researchers can either confirm or reject their hypothesis, after which they may decide to amend (or even abandon) their theory – or keep it as is.",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.4 Hypothesis testing"
    ]
  },
  {
    "objectID": "chapters/05-hypothesis_testing.html#null-hypothesis-significance-testing-nhst",
    "href": "chapters/05-hypothesis_testing.html#null-hypothesis-significance-testing-nhst",
    "title": "4.4 Hypothesis testing",
    "section": "Null hypothesis significance testing (NHST)",
    "text": "Null hypothesis significance testing (NHST)\nThe NHST framework offers researchers a convenient way of testing their theoretical assumptions. This chiefly involves setting up a set of (ideally) falsifiable statistical hypotheses, gathering evidence from the observed data and computing the (in)famous ‘\\(p\\)-value’ to determine “statistical significance” – a notion that is frequently misinterpreted in scientific studies.\n\n\n\n\n\n\nIs this the only way of testing hypotheses?\n\n\n\nThe answer is a resounding no. Despite its immense popularity, NHST is problematic in many respects and hence subject to heavy criticism (cf. Dienes (2008): 76; Baguley (2012): 143-144). There are other statistical schools that can remedy many of its shortcomings and come with distinct advantages, such as those relying on likelihood-based inference and Bayesian principles. Although these are also becoming increasingly common in linguistics, they are still restricted to very few sub-disciplines and journals (mostly in the area of psycholinguistics).\n\n\n\n\\(H_0\\) vs. \\(H_1\\)\nStatistical hypotheses always come in pairs: A null hypothesis is accompanied by an alternative hypothesis. They are set up before (!) seeing the data and justified by previous research.\n\nThe null hypothesis \\(H_0\\) describes the “default state of the world” (James et al. 2021: 555). It claims there is no noteworthy effect to be observed in the data.\nThe alternative hypothesis \\(H_1\\) (or \\(H_a\\)) plainly states that \\(H_0\\) is false, suggesting that there is an effect of some kind.\n\n\nCategorical data\nWe are interested in finding out whether the Type of an English genitive (‘s’ or ‘of-sc’) depends on Possessor Animacy (‘animate’ vs. ‘inanimate’). Our hypotheses are:\n\n\\(H_0:\\) Genitive Type and Possessor Animancy are independent.\n\\(H_1:\\) Genitive Type and Possessor Animancy are not independent.\n\n\nAccording to a \\(\\chi^2\\)-test of independence, there is a statistically significant relationship between genitive type and possessor animacy (\\(p &lt; 0.001\\), \\(\\chi^2 = 106.44\\), \\(df = 1\\)).\n\n\n\n\n\n\n\nWhat does independence really mean?\n\n\n\n\n\nThe core idea is “that the probability distribution of the response variable is the same for each group” (Agresti and Kateri 2022: 177). Assume genitive Type is the dependent variable and Possessor Animacy the independent variable. Then independence would entail that the probabilities of the outcomes of the response variable Type = \"s\" and Type = \"of\" are not influenced by whether they occur in the groups Possessor Animacy = \"animate\" or Possessor Animacy = \"inanimate\".\nIf we consider two variables at the same time, such as \\(X\\) and \\(Y\\), they are said to have marginal probability functions, which we can call \\(f_1(\\text{Type})\\) and \\(f_2(\\text{Animacy})\\) here. If we condition the outcomes of both variables on each other, the following equivalence will hold:\n\\[\nf(\\text{Type} \\mid \\text{Animacy}) = f_1(\\text{Type}) \\text{ and } f(\\text{Animacy} \\mid \\text{Type}) = f_2(\\text{Animacy}).\n\\tag{1}\\]\nThus, the null hypothesis assumes that the probabilities of each combination of values (such as Type and Possessor Animacy), denoted by \\(\\pi_{ij}\\), have the relationship in Equation 1. This can be stated succinctly as\n\\[\nH_0 : \\pi_{ij} = P(X = j)P(Y = i),\n\\tag{2}\\]\nwhere \\(P(X = j)\\) denotes the the probability that the random variable \\(X\\) takes the value \\(j\\) and \\(P(Y = i)\\) the probability that the random variable \\(Y\\) takes the value \\(i\\).\nFor illustration, consider the data below:\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\text{Possessor Animacy}\\)\n\n\n\n\n\n\n\\(\\text{animate}\\)\n\\(\\text{inanimate}\\)\n\n\n\\(\\text{Genitive}\\)\n\\(\\text{s}\\)\n\\(\\pi_{11}\\)\n\\(\\pi_{12}\\)\n\n\n\n\\(of\\)\n\\(\\pi_{21}\\)\n\\(\\pi_{22}\\)\n\n\n\nIndependence would be given if for each cell the following relationships would hold:\n\\[\n\\begin{align}\n\\pi_{11} &= P(X = \\text{animate})P(Y = \\text{s}) \\\\\n\\pi_{12} &= P(X = \\text{inanimate})P(Y = \\text{s}) \\\\\n\\pi_{21} &= P(X = \\text{animate})P(Y = \\text{of}) \\\\\n\\pi_{22} &= P(X = \\text{inanimate})P(Y = \\text{of}) \\\\\n\\end{align}\n\\]\n\n\n\n\n\nContinuous data\nAs part of a phonetic study, we compare the base frequencies of the F1 formants of vowels (in Hz) for male and female speakers of Apache. We forward the following hypotheses:\n\n\\(H_0:\\) mean F1 frequency of men \\(=\\) mean F1 frequency of women.\n\\(H_1:\\) mean F1 frequency of men \\(\\ne\\) mean F1 frequency of women.\n\n\nAccording to a two-sample \\(t\\)-test, there is a significant difference between the mean F1 frequencies of male and female speakers of Apache (\\(t(112.19) = 2.44\\), \\(p &lt; 0.05\\)).\n\n\n\nGeneralisation\nTo be precise, we use the hypotheses to make statements about a population parameter \\(\\theta\\), which can be a mean \\(\\mu\\) for continuous data or a proportion \\(\\pi\\) for categorical data. Formally, the null and alternative hypotheses can be restated as in Equation 3.\n\\[\n\\begin{align}\nH_0: \\theta = 0  \\\\\nH_1: \\theta \\neq 0\n\\end{align}\n\\tag{3}\\]\nIn the NHST world, we’re dealing with a “This town ain’t big enough for the both of us” situation: While we have to state both \\(H_0\\) and \\(H_1\\), only one of them can remain at the end of the day. But how do we decide between these two?\n\n\n\nTest statistics\nTo facilitate the decision-making process, we proceed to gather statistical evidence from the observed data. Since NHST primarily revolves around \\(H_0\\) (and not \\(H_1\\)!), we need to review the evidence the data provides against or in favour \\(H_0\\). This is done via a test statistic \\(T\\) that characterises the sample at hand. Agresti & Kateri (2022: 163) describe test statistics as indicators of how strongly a point estimate (e.g., a mean) deviates from its expected value under \\(H_0\\).\nThere are many possible test statistics out there:\n\nFor instance, if the data are discrete, the \\(\\chi^2\\) measure is used to compute differences between observed and expected frequencies in the entire sample.\nIn the case of continuos data, it is common to rely on \\(t\\) for quantifying differences between sample means.\nOther possible test statistics include the correlation coefficient \\(r\\), \\(z\\)-scores, the \\(F\\)-statistic, and many others.\n\n\n\nSampling distributions\nProbability distributions of sample statistics are called sampling distributions. A fundamental statistical insight is that sample statistics (specifically means and sums of continuous random variables) converge onto a standard normal distribution with \\(\\mu = 0\\) and \\(\\sigma = 1\\). This proven statement is known as the Central Limit Theorem (Heumann, Schomaker, and Shalabh 2022: 547). However, if the population standard deviation is unknown, the Student \\(t\\) distribution provides a reasonable approximation. It has a single parameter \\(v\\) determining its shape, and it stands for the degrees of freedom.\n\n\n\n\n\n\n\\(t\\) distribution\n\n\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\n\n# Define the degrees of freedom\ndf_t &lt;- 10\n\n# Create a sequence of x values\nx_t &lt;- seq(-4, 4, length.out = 1000)\n\n# Compute the t-distribution density\ny_t &lt;- dt(x_t, df = df_t)\n\n# Create a data frame\nt_distribution_data &lt;- data.frame(x = x_t, y = y_t)\n\n# Generate the plot\nggplot(t_distribution_data, aes(x = x, y = y)) +\n  geom_line(color = \"steelblue\", size = 1) + # Line for the density curve\n  labs(\n    title = \"t-Distribution\",\n    subtitle = \"Probability density function with 10 degrees of freedom\",\n    x = \"t value\",\n    y = \"Probability density\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(ylim = c(0, 0.4), xlim = c(-4, 4)) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.title = element_text(size = 12)\n  )\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\nThe \\(\\chi^2\\) distribution is closely related to the \\(t\\) distribution, sharing the degrees of freedom parameter \\(v\\). In essence, it is the distribution of squared (standard) normally distributed random variables \\(Z^2\\). Another example are sums of squares \\(\\sum_{i=1}^n(x_i - \\mu)^2\\).\n\n\n\n\n\n\n\\(\\chi^2\\) distribution\n\n\n\n\n\n\n\nShow the code\n# Load ggplot2\nlibrary(ggplot2)\n\n# Define the degrees of freedom\ndf &lt;- 2\n\n# Create a sequence of x values\nx &lt;- seq(0, 30, length.out = 1000)\n\n# Compute the chi-squared density\ny &lt;- dchisq(x, df = df)\n\n# Create a data frame\nchi_squared_data &lt;- data.frame(x = x, y = y)\n\n# Generate the plot\nggplot(chi_squared_data, aes(x = x, y = y)) +\n  geom_line(color = \"steelblue\", size = 1) + # Line for the density curve\n  labs(\n    title = \"Chi-Squared Distribution\",\n    subtitle = \"Probability density function with 2 degrees of freedom\",\n    x = \"Chi-squared value\",\n    y = \"Probability density\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(ylim = c(0, 0.05), xlim = c(0, 30)) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.title = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical significance\nThe final rejection of \\(H_0\\) is determined by the significance probability \\(p\\). Due to the frequency and ferocity with which statistical significance is misinterpreted in the research literature, we will begin by reviewing its technical definition:\n\n“The \\(p\\)-value is the probabilty, presuming that \\(H_0\\) is true, that the test statistic equals the observed value or a value even more extreme in the direction predicted by \\(H_a\\)” (Agresti and Kateri 2022: 163).\n\nIn compact notation, it is equivalent to the conditional probability\n\\[\nP(T \\geq \\text{observed value} \\mid H_0 \\text{ is true}).\n\\] If \\(p\\) is lower than a pre-defined threshold (typically \\(0.05\\)), also known as the significance level \\(\\alpha\\), we can reject \\(H_0\\). However, if \\(p \\geq\\) 0.05, this neither justifies rejecting nor accepting the null hypothesis (Baguley 2012: 121).\n\n\nWhat could go wrong? Type I and Type II errors\nThere is always a chance that we accept or reject the wrong hypothesis; the four possible constellations are summarised in the table below (cf. Heumann, Schomaker, and Shalabh 2022: 223):\n\n\n\n\n\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is not true\n\n\n\n\n\\(H_0\\) is not rejected\n\\(\\color{green}{\\text{Correct decision}}\\)\n\\(\\color{red}{\\text{Type II } (\\beta)\\text{-error}}\\)\n\n\n\\(H_0\\) is rejected\n\\(\\color{red}{\\text{Type I } (\\alpha)\\text{-error}}\\)\n\\(\\color{green}{\\text{Correct decision}}\\)\n\n\n\nThere is a trade-off between Type I and Type II errors (Agresti and Kateri 2022: 182-186):\n\nIf we try to decrease \\(P\\)(Type I) by selecting a lower \\(\\alpha\\), \\(P\\)(Type II) will inevitably increase.\nIf we increase the sample size to lower \\(P\\) (Type II), \\(P\\)(Type I) will increase again.\n\nIt is common practice to computer the power 1 - \\(P\\)(Type II) of test, in order to estimate the probability of correctly rejecting \\(H_0\\).\n\nlibrary(pwr)\n\n# w = effect size (between 0 and 1)\n# df = degrees of freedom\n# sig.level = significance level alpha\n# power = 1 - beta\npwr.chisq.test(w = 0.5, df = 1, sig.level = 0.05, power = 0.95)\n\n\n     Chi squared power calculation \n\n              w = 0.5\n              N = 51.97884\n             df = 1\n      sig.level = 0.05\n          power = 0.95\n\nNOTE: N is the number of observations\n\n# We'd need a sample size of N = 52 to detect an effect with effect size 0.5, alpha = 0.05, beta = 0.05, and 1 degree of freedom (low data complexity, e.g., simple 2x2 contingency table).\n\n\n\nComputing the \\(p\\)-value\nLet’s say that the statistical analysis of two discrete variables \\(X\\) and \\(Y\\) has returned a test statistic of \\(\\chi^2 = 6.5\\) for 2 \\(df\\). In order to compute the corresponding \\(p\\)-value we need to consult the sampling distribution of this test statistic.\n\n\n\n\n\n\n\\(\\chi^2\\) distribution\n\n\n\n\n\n\n\nShow the code\n# Load ggplot2\nlibrary(ggplot2)\n\n# Define the degrees of freedom\ndf &lt;- 2\n\n# Create a sequence of x values\nx &lt;- seq(0, 30, length.out = 1000)\n\n# Compute the chi-squared density\ny &lt;- dchisq(x, df = df)\n\n# Create a data frame\nchi_squared_data &lt;- data.frame(x = x, y = y)\n\n# Generate the plot\nggplot(chi_squared_data, aes(x = x, y = y)) +\n  geom_line(color = \"steelblue\", size = 1) +\n  labs(\n    title = \"Chi-Squared Distribution\",\n    subtitle = \"Probability density function with 2 degrees of freedom\",\n    x = \"Chi-squared value\",\n    y = \"Probability density\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(ylim = c(0, 0.05), xlim = c(0, 30)) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.title = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nBecause continuous functions have an infinite number of \\(x\\)-values, the probability of any single value must be 0.1 Therefore, if we are interested in obtaining actual probabilities from the PDF, we can only do so for intervals of values. The probability that a value \\(X\\) falls into the interval \\(a &lt; X &lt; b\\) is in fact equivalent to the area under the curve between \\(a\\) and \\(b\\) (cf. Equation 4).\n1 The proof for the underlying theorem is given in Heumann et al. (2022: 544).\\[\nP(a &lt; X &lt; b) = \\int_a^b f(x)dx.\n\\tag{4}\\]\nRecall the PDF \\(f(x)\\) of the \\(\\chi^2\\)-distribution with 2 degrees of freedom. The \\(p\\)-value corresponds to the green area under the curve ranging from \\(x = 6.5\\) up to \\(\\infty\\), which can be restated formally in Equation 5. This brings us back to the definition of the \\(p\\)-value: It is the probability that the \\(\\chi^2\\) score is equal to 6.5 or higher, i.e., \\(P(\\chi^2 \\geq 6.5)\\).\n\\[\nP(6.5 &lt; X &lt; \\infty) = \\int_{6.5}^\\infty f(x)dx.\n\\tag{5}\\]",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.4 Hypothesis testing"
    ]
  },
  {
    "objectID": "chapters/05-hypothesis_testing.html#practical-considerations",
    "href": "chapters/05-hypothesis_testing.html#practical-considerations",
    "title": "4.4 Hypothesis testing",
    "section": "Practical considerations",
    "text": "Practical considerations\n\n\n\n\n\n\nCommon pitfalls (cf. Agresti and Kateri 2022: 189-190)\n\n\n\n\nStatistical significance is NOT an indication of a causal relationship between the variables of interest (correlation \\(\\neq\\) causation).\n\\(p\\)-values do NOT signify the strength of an effect (\\(\\neq\\) effect size). It only helps identify whether there is an effect to begin with.\n\\(p\\)-values are NOT the probability of the null hypothesis being true.\nStatistical significance is only a starting point for further scientific inquiry, and by no means the end of it.",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.4 Hypothesis testing"
    ]
  },
  {
    "objectID": "chapters/05-hypothesis_testing.html#exercises",
    "href": "chapters/05-hypothesis_testing.html#exercises",
    "title": "4.4 Hypothesis testing",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1 Schröter & Kortmann (2016) investigate the relationship between subject realisation (overt vs. null) and the grammatical category Person (1.p. vs. 2.p. vs. 3.p.) in three varieties of English (Great Britain vs. Hong Kong vs. Singapore). They report the following test results (2016: 235):\n\nChi-square test scores: \\[\n\\begin{align}\n\\text{Singapore: \\quad} & \\chi^2 = 3.3245, df = 2, p = 0.1897 \\\\\n\\text{Hong Kong: \\quad} & \\chi^2 = 40.799, df = 2, p &lt; 0.01 \\\\\n\\text{Great Britain: \\quad} & \\chi^2 = 3.6183, df = 2, p = 0.1638 \\\\\n\\end{align}\n\\]\n\n\nWhat hypotheses are the authors testing?\nAssuming a significance level \\(\\alpha = 0.05\\), what statistical conclusions can be drawn from the test results?\nWhat could be the theoretical implications of these results?\n\n\n\nExercise 2 Try to develop statistical hypotheses for a research project you are currently working on!",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.4 Hypothesis testing"
    ]
  },
  {
    "objectID": "chapters/06-trees_forests.html",
    "href": "chapters/06-trees_forests.html",
    "title": "7.1 Tree-based methods",
    "section": "",
    "text": "For linguists:\n\nLevshina (2015): Chapter 14\nLevshina (2020)\nGries (2021): Chapter 7\n\nGeneral:\n\nJames et al. (2021): Chapter 8\nHastie, Tibshirani, and Friedman (2017): Chapters 9.2 & 15",
    "crumbs": [
      "7. Machine Learning",
      "7.1 Tree-based methods"
    ]
  },
  {
    "objectID": "chapters/06-trees_forests.html#recommended-reading",
    "href": "chapters/06-trees_forests.html#recommended-reading",
    "title": "7.1 Tree-based methods",
    "section": "",
    "text": "For linguists:\n\nLevshina (2015): Chapter 14\nLevshina (2020)\nGries (2021): Chapter 7\n\nGeneral:\n\nJames et al. (2021): Chapter 8\nHastie, Tibshirani, and Friedman (2017): Chapters 9.2 & 15",
    "crumbs": [
      "7. Machine Learning",
      "7.1 Tree-based methods"
    ]
  },
  {
    "objectID": "chapters/06-trees_forests.html#introduction",
    "href": "chapters/06-trees_forests.html#introduction",
    "title": "7.1 Tree-based methods",
    "section": "Introduction",
    "text": "Introduction\nDecision trees and random forests are very popular non-parametric methods. As such, “they do not make explicit assumptions about the functional form of \\(f\\)’’ (James et al. 2021: 23).\nIn this unit, we will cover the conceptual basics of these methods as well as their implementation in R using the tv data from Levshina (2020) in addition to the ELP data from the unit on Linear Regression. The libraries we will need are listed below:\nIn the tv data frame, our target variable will be T/V Form with the two outcomes ty (Russian 2.p.sg., informal) and vy (Russian 2.p.pl., polite).\n\nstr(tv)\n\nhead(tv)",
    "crumbs": [
      "7. Machine Learning",
      "7.1 Tree-based methods"
    ]
  },
  {
    "objectID": "chapters/06-trees_forests.html#decision-trees",
    "href": "chapters/06-trees_forests.html#decision-trees",
    "title": "7.1 Tree-based methods",
    "section": "Decision trees",
    "text": "Decision trees\nCore concepts:\n\nSegmenting the feature space: “[T]he feature space (i.e., the space spanned by all predictor variables) is recursively partitioned into a set of rectangular areas” (Strobl, Malley, and Tutz 2009: 325).\nImpurity reduction: These simplified prediction areas should consist of mostly homogeneous (i.e., ‘pure’ rather than ‘mixed’) observations.\nTree construction: The ‘decisions’ made when partitioning the training data can be visualised using tree structures. The nodes of a tree represent variables, the branches represent decision rules, and leaf nodes indicate the final outcome (e.g., a prediction).\nCART: The original computational implementation of decision trees is known as the CART (Classification and Regression Trees) algorithm developed by Breiman (1984).\n\n\nClassification trees\nIf we are dealing with a categorical response variable, the tree() function can be used to fit a classification tree in accordance with Breiman’s CART algorithm. For illustration, consider the tv data frame. We will model the choice of the pronoun Form based on the speaker’s and hearer’s social circle (Rel_Circle) and their difference in social class (Rel_Class).\n\n# Set random number generator for reproducibility\nset.seed(123)\n\n# Supply model formula\ntree.tv &lt;- tree(Form ~ Rel_Circle + Rel_Class, data = tv)\n\n# View tree statistics\nsummary(tree.tv)\n\n\nClassification tree:\ntree(formula = Form ~ Rel_Circle + Rel_Class, data = tv)\nNumber of terminal nodes:  9 \nResidual mean deviance:  0.974 = 213.3 / 219 \nMisclassification error rate: 0.2412 = 55 / 228 \n\n# Visualisation\nplot(tree.tv)\n\ntext(tree.tv, pretty = 3)\n\n\n\n\n\n\n\n\nAn important problem that arises during tree construction is that of split selection. When should the tree split a node into two further nodes and when not? Furthermore, when should the tree stop the splitting process entirely? In this respect, CART relies on the principle of impurity reduction: “The fundamental idea is to select each split of a subset so that the data in each of the descendent subsets are ‘purer’ than the data in the parent subset” (Breiman 1984: 23). A measure for node purity is the Gini index \\(G\\), which is defined as\n\\[\nG = \\sum_{k=1}^{K}{\\hat{p}_{mk}(1-\\hat{p}_{mk}),}\n\\]\nwhere \\(\\hat{p}_{mk}\\) measures the proportion of observations of a response level \\(k\\) in the \\(m\\)th prediction area of the training data set. Values close to 0 are indicative of high node purity, meaning that most observations belong to the same class (e.g., Form = ty). If splitting a node no longer leads to a substantial increase in purity, it becomes the terminal node, i.e., it is not split further. This terminal node returns the tree’s class prediction.\nIt is worth noting that modern CART implementations rely on different splitting criteria. For instance, Conditional Inference Trees use the \\(p\\) values of internal association tests to identify which variables warrant further subdivision of the training data. The presence or absence of correlation thus also determines whether or not a given node will be terminal (for more details, see B. Greenwell 2022: 122).\n\n# Fitting a conditional inference tree\nctree.tv &lt;- ctree(Form ~ ., data = tv) # dot . means 'include all predictors'\n\nplot(ctree.tv)\n\n\n\n\n\n\n\n\n\n\nRegression trees\nRegression trees are used for continuous response variables. Instead of providing class predictions, they return the mean value of observations in a given prediction area. The algorithm now strives to minimize the residual sum of squares (\\(RSS\\)). Consider the regression tree for reaction times depending on word length, frequency and part of speech:\n\n# CART tree\ntree.rt &lt;- tree(RT ~ Length + Freq + POS, data = ELP)\n\nsummary(tree.rt)\n\n\nRegression tree:\ntree(formula = RT ~ Length + Freq + POS, data = ELP)\nVariables actually used in tree construction:\n[1] \"Freq\"   \"Length\"\nNumber of terminal nodes:  10 \nResidual mean deviance:  8629 = 7507000 / 870 \nDistribution of residuals:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-220.10  -59.09  -11.17    0.00   49.66  397.40 \n\nplot(tree.rt)\n\ntext(tree.rt, pretty = 0)\n\n\n\n\n\n\n\n# Conditional inference tree\nctree.rt &lt;- ctree(RT ~ Length + Freq + POS, data = ELP) \n\nplot(ctree.rt)",
    "crumbs": [
      "7. Machine Learning",
      "7.1 Tree-based methods"
    ]
  },
  {
    "objectID": "chapters/06-trees_forests.html#random-forests",
    "href": "chapters/06-trees_forests.html#random-forests",
    "title": "7.1 Tree-based methods",
    "section": "Random forests",
    "text": "Random forests\nRandom forests (Breiman 2001) belong to the class of ensemble methods because they combine simpler models (e.g., individual decision trees) into a more complex and possibly more accurate model. As part of the RF algorithm, a great number of decision trees is trained on bootstrapped samples of the training data.\nSo far, random forests are essentially identical with Bagging (= bootstrap aggregation); however, an important additional characteristic of the RF algorithm is that only a random subset of the predictors is taken into consideration at each split. According to Strobl et al. (2009: 332), the resulting variability in tree structure is advantageous: “By combining the prediction of such a diverse set of trees, ensemble methods utilize the fact that classification trees are unstable, but, on average, produce the right prediction”.\n\nRegression forest\nFor regression tasks, random forests return the average prediction of all trees in the ensemble.\n\n# For regression\nrt.rf.reg &lt;- randomForest(RT ~ Length + Freq + POS, data = ELP,\n                                mtry = 1, # = sqrt(number of variables)\n                                ntree = 500) # number of trees\n\nrt.rf.reg\n\n\nCall:\n randomForest(formula = RT ~ Length + Freq + POS, data = ELP,      mtry = 1, ntree = 500) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 8972.927\n                    % Var explained: 43.64\n\n# Conditional random forest\nrt.crf.reg &lt;- cforest(RT ~ Length + Freq + POS, data = ELP, \n                    controls = cforest_unbiased(ntree = 500, mtry = 1))\n\n\n\nClassification forest\nFor classification, all trees cast a vote for one of the response classes. The OOB error estimate refers to the accuracy of out-of-bag (OOB) predictions. After the initial bootstrapping procedure, roughly a third of the training data remains unused. These observations, which were not used for fitting trees, can be used as a test data set. Predictions based on this internal test data set are called OOB predictions.\n\n# For classification\ntv.rf.class &lt;- randomForest(Form ~ ., data = tv,\n                            mtry = 4,\n                            ntree = 500)\n\ntv.rf.class\n\n\nCall:\n randomForest(formula = Form ~ ., data = tv, mtry = 4, ntree = 500) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n        OOB estimate of  error rate: 18.86%\nConfusion matrix:\n   ty vy class.error\nty 86 22   0.2037037\nvy 21 99   0.1750000\n\n# Conditional random forest\ntv.crf.class &lt;- cforest(Form ~ ., data = tv,\n                    controls = cforest_unbiased(ntree = 500, mtry = 4))\n\ntv.crf.class\n\n\n     Random Forest using Conditional Inference Trees\n\nNumber of trees:  500 \n\nResponse:  Form \nInputs:  Film, Rel_Age, Rel_Sex, Rel_Power, Rel_Circle, S_Class, H_Class, S_Age, H_Age, Rel_Class, Before68, Others, Office, S_Sex, H_Sex, Place \nNumber of observations:  228 \n\n\n\n\nVariable importance\nRandom forests allow users to assess whether or not certain predictors are useful for the model. The Gini index can be re-used to identify those variables that have led to the greatest reduction in impurity. However, this measure is biased towards predictors with many values (cf. Strobl et al. 2007).\n\n# Gini importance (Reaction times)\nvarImpPlot(rt.rf.reg)\n\n\n\n\n\n\n\n# Gini importance (Form of 2.p.)\nvarImpPlot(tv.rf.class)\n\n\n\n\n\n\n\n\nA more robust measure is (Conditional) Permutation Accuracy Importance which compares the predictive accuracy of the random forest model before and after randomly permuting the values of the predictors (cf. Strobl et al. 2008; Debeer and Strobl 2020).\n\n# Conditional permutation accuracy importance\nlibrary(\"permimp\")\n\n# Refit RF model with additional parameters\ntv.rf.class &lt;- randomForest(Form ~ .,\n                            data = tv,\n                            mtry = 4,\n                            ntree = 500,\n                            keep.inbag = TRUE,\n                            keep.forest = TRUE)\n\n# Compute CPI scores\ntv.rf.permimp &lt;- permimp(tv.rf.class, conditional = TRUE, progressBar = FALSE, threshold = .95) # Choose \"Yes\" in the console\n\n# Plot CPI scores\nplot(tv.rf.permimp, horizontal = TRUE, type = \"dot\", sort = TRUE)\n\n\n\nVisualising random forest models\nPartial dependence plots provide averaged predictions \\(\\hat{y}\\) for a given constellation of predictors. These averages are produced by the partial dependence function \\(f_k\\). If a categorical response variable has \\(K\\) possible values, it has the form \\[\nf_k(X) = \\log [p_k (x)] - \\frac{1}{N}\\sum_{k=1}^{K} \\log [p_k (x)],\n\\] with \\(p_k(x)\\) corresponding to the fitted probability of the 𝑘th level of the response variable for \\(k \\in \\{1, 2, ..., K\\}\\) (B. M. Greenwell 2017: 430; see also Hastie, Tibshirani, and Friedman 2017).\n\n\nShow the code\n# Form ~ Rel_Circle\nRel_Circle.partial &lt;- pdp::partial(tv.rf.class, pred.var = \"Rel_Circle\", which.class = \"ty\")\n\nRel_Circle.partial %&gt;% \n  ggplot(aes(x = Rel_Circle, y = yhat, group = 1)) +\n  geom_point(col = \"steelblue\") +\n  geom_line(col = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    title = \"Probability of 'ty' (2.p.sg.) depending on social circle\",\n    y = \"Log odds of 'ty'\"\n  )\n\n\n\n\n\n\n\n\n\nShow the code\n# RT ~ POS\npos.partial &lt;- pdp::partial(rt.rf.reg, pred.var = \"POS\")\n\npos.partial %&gt;% \n  ggplot(aes(x = POS, y = yhat, group = 1)) +\n  geom_point(col = \"steelblue\") +\n  geom_line(col = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    title = \"Predicted reaction times by POS\",\n    y = \"Predicted reaction time\"\n  )\n\n\n\n\n\n\n\n\n\nShow the code\n# RT ~ Length\nlength.partial &lt;- pdp::partial(rt.rf.reg, pred.var = \"Length\")\n\nlength.partial %&gt;% \n  ggplot(aes(x = Length, y = yhat)) +\n  geom_line(col = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    title = \"Predicted reaction times by word length\",\n    y = \"Predicted reaction time\"\n  )",
    "crumbs": [
      "7. Machine Learning",
      "7.1 Tree-based methods"
    ]
  },
  {
    "objectID": "chapters/02-data_frames.html",
    "href": "chapters/02-data_frames.html",
    "title": "2.4 Data frames",
    "section": "",
    "text": "Script\n\n\n\nYou can find the full R script associated with this unit here.",
    "crumbs": [
      "2. Introduction to R",
      "2.4 Data frames"
    ]
  },
  {
    "objectID": "chapters/02-data_frames.html#preparation",
    "href": "chapters/02-data_frames.html#preparation",
    "title": "2.4 Data frames",
    "section": "",
    "text": "Script\n\n\n\nYou can find the full R script associated with this unit here.",
    "crumbs": [
      "2. Introduction to R",
      "2.4 Data frames"
    ]
  },
  {
    "objectID": "chapters/02-data_frames.html#recommended-reading",
    "href": "chapters/02-data_frames.html#recommended-reading",
    "title": "2.4 Data frames",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nWinter (2020): Chapter 1.10-1.16\n\nSuggested video tutorial:\n\nUsing the Data Frame in R (DataCamp, 5min)\nLearn How to Subset, Extend & Sort Data Frames in R (DataCamp, 7min)",
    "crumbs": [
      "2. Introduction to R",
      "2.4 Data frames"
    ]
  },
  {
    "objectID": "chapters/02-data_frames.html#word-frequencies-ii",
    "href": "chapters/02-data_frames.html#word-frequencies-ii",
    "title": "2.4 Data frames",
    "section": "Word frequencies II",
    "text": "Word frequencies II\nRecall our simple linguistic dataset from the previous unit:\n\n\n\n\nTable 1: Verb lemma frequencies\n\n\n\n\n\n\nVerb\nFrequency\n\n\n\n\nstart\n418\n\n\nenjoy\n139\n\n\nbegin\n337\n\n\nhelp\n281\n\n\n\n\n\n\n\n\n\n\nWe thought of the columns as one-dimensional, indexed lists of elements:\n\nlemma &lt;- c(\"start\", \"enjoy\", \"begin\", \"help\")\n\nfrequency &lt;- c(418, 139, 337, 281)\n\nIn fact, R allows us to combine these two vectors into an actual spreadsheet. To this end, we need to apply the data.frame() function to two vectors of our choice. Note that they need to have the same length:\n\ndata &lt;- data.frame(lemma, frequency)\n\nprint(data)\n\n  lemma frequency\n1 start       418\n2 enjoy       139\n3 begin       337\n4  help       281\n\n\n\nEssential R concepts\nThe variable data is no longer a vector, but a data frame (often abbreviated as ‘df’). Once again, each element carries its own label and can be, therefore, accessed or manipulated.\nSince data frames are two-dimensional objects, the subsetting notation in square brackets [ ] needs to reflect that. This is the general pattern:\n\\[ \\text{df[row, column]}\n\\tag{1}\\]\nSay, we’re looking for the element at the intersection of the first row and first column. Applying the pattern above, we can access it like so:\n\ndata[1,1]\n\n[1] \"start\"\n\n\nBut what if we needed the entire first row? We’d simply omit the column part. Note, however, that the comma , needs to remain:\n\ndata[1,]\n\n  lemma frequency\n1 start       418\n\n\nSubsetting by columns is interesting. We can either use the square bracket notation [ ] or the column operator $:\n\ndata[,1]\n\n[1] \"start\" \"enjoy\" \"begin\" \"help\" \n\ndata$lemma\n\n[1] \"start\" \"enjoy\" \"begin\" \"help\" \n\n\n\n\nFiltering\nNot all the observations contained in a data frame are necessarily relevant for our research. In such cases, it may be important to subset the rows and columns according to certain criteria.\nAssume we only need those observations where the lemma frequencies are greater than 300. We can filter the dataset accordingly by specifying\n\nthe data frame,\nthe column of interest, and\nthe condition to apply to the rows.\n\nYou can read the code below as\n\n‘Take the data frame data and subset it according to the column data$frequency. Show me those rows where the values of data$frequency are greater than 300.’\n\n\ndata[data$frequency &gt; 300, ]\n\n  lemma frequency\n1 start       418\n3 begin       337\n\n\nWhat if we wanted to filter by lemma instead? To make it more concrete, assume we’re looking for frequency data on the verbs start and help ( but not on begin and help).\nWe can start by accessing the rows with data on start first:\n\ndata[data$lemma == \"start\", ]\n\n  lemma frequency\n1 start       418\n\n\nNext, we add a second, analogous condition. Combining multiple statements requires a logical operator. In this code chunk, we’re using | , which corresponds to a logical ‘or’ (also known as a “disjunction”).\n\ndata[data$lemma == \"start\" | data$lemma == \"help\", ]\n\n  lemma frequency\n1 start       418\n4  help       281\n\n\n\n\n\n\n\n\nWhy do we need to use “or” (|) and not “and” (&)?\n\n\n\nThe idea of combining statements somewhat naturally suggests a conjunction, which could be achieved via &. How come R doesn’t return anything if we do it that way?\n\ndata[data$lemma == \"start\" & data$lemma == \"help\", ]\n\n[1] lemma     frequency\n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\n\n\nThis looks unintuitive – is there another way to filter in R?\nYes, absolutely. The callouts below demonstrate a few popular alternatives. In the end, the exact way you filter doesn’t really matter, so long as you (as well as the people who have to work with your script) can understand what you’re trying to achieve with your code. Always make sure to add comments to your filtering operations!\n\n\n\n\n\n\nsubset()\n\n\n\n\n\nAlmost every subsetting operation we perform with square brackets can also be performed using the subset() function. Here are some expressions that are synonymous to the ones above:\n\nsubset(data, frequency &gt; 300)\n\n  lemma frequency\n1 start       418\n3 begin       337\n\nsubset(data, lemma == \"start\" | lemma == \"help\")\n\n  lemma frequency\n1 start       418\n4  help       281\n\n\n\n\n\n\n\n\n\n\n\ntidyverse\n\n\n\n\n\nThe tidyverse-ecosystem is a collection of packages specifically designed for handling typical data science tasks as comfortably and elegantly as possible, supplying countless helper functions for data manipulation, transformation and visualisation. Installation instructions are provided in 2.3 Libraries.\nA extensive guide to the main tidyverse functions is provided in Chapter 3 of the free eBook R For Data Science (2nd edition). Due to its clarity, most of the more advanced code in this reader will draw on tidyverse syntax.\nLet’s generate a tidyverse-style data frame, the tibble:\n\nlibrary(tidyverse)\n\ndata2 &lt;- tibble(\n  lemma = c(\"start\", \"enjoy\", \"begin\", \"help\"),\n  frequency = c(418, 139, 337, 281)\n)\n\nprint(data2)\n\n# A tibble: 4 × 2\n  lemma frequency\n  &lt;chr&gt;     &lt;dbl&gt;\n1 start       418\n2 enjoy       139\n3 begin       337\n4 help        281\n\n\nWe can single out certain columns with select():\n\nselect(data2, lemma)\n\n# A tibble: 4 × 1\n  lemma\n  &lt;chr&gt;\n1 start\n2 enjoy\n3 begin\n4 help \n\n\nIt is very easy to filter the data frame according to certain criteria:\n\nfilter(data2, frequency &gt; 300)\n\n# A tibble: 2 × 2\n  lemma frequency\n  &lt;chr&gt;     &lt;dbl&gt;\n1 start       418\n2 begin       337\n\nfilter(data2, lemma == \"start\" | lemma == \"help\")\n\n# A tibble: 2 × 2\n  lemma frequency\n  &lt;chr&gt;     &lt;dbl&gt;\n1 start       418\n2 help        281\n\n\nThe tidyverse features a special pipe operator %&gt;% that can be used to pass the output of one function on to the next one. It is conceptually similar to the coordinating conjunction and. The code can be rewritten in pipe notation as follows:\n\n# Read as: \"Take data2 and select the column with the name 'lemma'.\"\ndata2 %&gt;% \n  select(lemma)\n\n# Read as: \"Take data2 and show me those rows where frequency is greater than 300.\"\ndata2 %&gt;% \n  filter(frequency &gt; 300)\n\n# Read as: \"Take data2 and show me those rows that correspond to the lemma  'start' or 'help' or both.\"\ndata %&gt;% \n  filter(lemma == \"start\" | lemma == \"help\")",
    "crumbs": [
      "2. Introduction to R",
      "2.4 Data frames"
    ]
  },
  {
    "objectID": "chapters/02-data_frames.html#tier-1",
    "href": "chapters/02-data_frames.html#tier-1",
    "title": "2.4 Data frames",
    "section": "Tier 1",
    "text": "Tier 1\n\nExercise 1 Recreate the barplot from the previous unit by subsetting the data variable accordingly.\n\n\nExercise 2 Print the following elements by subsetting the data frame data accordingly.\n\n337\nbegin\nenjoy\nenjoy 139\nthe entire frequency column",
    "crumbs": [
      "2. Introduction to R",
      "2.4 Data frames"
    ]
  },
  {
    "objectID": "chapters/02-data_frames.html#tier-2",
    "href": "chapters/02-data_frames.html#tier-2",
    "title": "2.4 Data frames",
    "section": "Tier 2",
    "text": "Tier 2\n\nExercise 3 (Extension of Ex. 3 from Vectors) Verify that the following verbs are represented in the lemma column: enjoy, hit, find, begin. If they are in the data frame, print their frequency information.\n\n\nExercise 4 (Extension of Ex. 4 from Vectors) Use which() to find the rows where the frequency is greater than 200, and then print the lemma and frequency of those rows only.",
    "crumbs": [
      "2. Introduction to R",
      "2.4 Data frames"
    ]
  },
  {
    "objectID": "chapters/02-data_frames.html#tier-3",
    "href": "chapters/02-data_frames.html#tier-3",
    "title": "2.4 Data frames",
    "section": "Tier 3",
    "text": "Tier 3\n\nExercise 5 Diachronic corpora comprise data on language use across different time periods. This data frame indicates the frequencies of certain modal verbs across three time periods:\n\nmodals_df &lt;- data.frame(\n  modal = c(\"can\", \"could\", \"may\", \"might\", \"must\", \"shall\", \"should\", \"will\", \"would\"),\n  period1 = c(128, 68, 55, 21, 44, 19, 35, 85, 97),\n  period2 = c(142, 83, 41, 30, 39, 12, 52, 94, 119)\n)\n\n\nFind the most and least frequent modal verb in each time period.\nCalculate the percentage change in frequency for each modal verb between period1 and period2.\nCreate a new column trend with the values \"increasing\" and \"decreasing\" based on whether the frequency increased or decreased across periods.\n\n\n\nExercise 6 (Extension of Ex. 8 in Vectors.) Write a function that performs part-of-speech (POS) annotation on the sentence The quick brown fox jumps over the lazy dog. Here are a few code snippets to help you get started:\n\nYou can split up sentences into tokens using tokenize_words() from the tokenizers library.\n\n\nlibrary(tokenizers)\nlibrary(tidyverse)\n\ntext &lt;- \"Colorless green ideas sleep furiously.\"\ntext_tokenized &lt;- tokenize_words(text)\n\n# To lowercase the tokens\ntokens_lower &lt;- tolower(text_tokenized[[1]])\n\n\nVectors can have name attributes:\n\n\nword &lt;- \"read\"\n\n# Give it a name\nnames(word) &lt;- \"verb\"\n\n# Get rid of its name\nword &lt;- unname(word)\n\n\nThere are several ways to apply conditional logic:\n\n\nthings &lt;- c(\"apple\", \"cherry\", \"pear\", \"cucumber\", \"coconut\")\nfruits &lt;- c(\"apple\", \"cherry\", \"pear\")\nvegetables &lt;- c(\"cabbage\", \"carrot\", \"cucumber\")\n\n# Base R\nfood_analysis &lt;- ifelse(things %in% fruits, \"fruit\", \"not_fruit\")\n\n# Tidyverse\nfood_analysis2 &lt;- case_when(\n  things %in% fruits ~ \"yes\", # if elements from \"things\" are in \"fruits\", print \"yes\", else\n  TRUE ~ \"no\"                 # print \"no\" (default)\n)\n\n\nIf multiple conditions should be checked, the statements/cases have to be nested appropriately:\n\n\n# Base R\ncomplex_food_analysis &lt;- ifelse(things %in% fruits, \"fruit\",\n       ifelse(things %in% vegetables, \"vegatable\",\n              \"unknown\"))\n\n# Tidyverse\ncomplex_food_analysis2 &lt;- case_when(\n  things %in% fruits ~ \"fruit\", # if elements from \"things\" are in \"fruits\", print \"yes\", else\n  things %in% vegetables ~ \"vegetable\", # if they're in \"vegetables\", print \"yes\", else\n  TRUE ~ \"unknown\"     # print \"unknown\"           \n)",
    "crumbs": [
      "2. Introduction to R",
      "2.4 Data frames"
    ]
  },
  {
    "objectID": "chapters/01-rqs.html",
    "href": "chapters/01-rqs.html",
    "title": "1.2 Research questions",
    "section": "",
    "text": "Example: I am studying the pronunciation of [r] in New York because I want to find out whether different social groups differ with respect to their pronunciation, in order to help my readers understand how social factors influence our linguistic behavior.\n\nTopic: I am studying...\nIndirect question: Because I want to find out what, why, how...\n(General) linguistic significance: ... in order to help the readers understand how, why, or whether...\n\n\nCf. Booth, Colomb, and Williams (2008): 45–48\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nA research question must be simple, specific, and doable. It is better to make a small contribution to a particular problem than to aim at covering a whole subfield of sociolinguistics.\nKeep a research question stable in your paper, even if you come across interesting new ideas as you work on it.\nThe research question must be an explicit part of your paper. Mention it in the introduction and keep referring back to it from time to time.\nKeep in mind that the research question will determine both methodology and data.\n\n\nCf. Hazen (2015): 9–10\n\n\n\n\n\n\nResearch questions must be complemented by a set of falsifiable hypotheses. These will be covered in-depth in the chapter on hypothesis testing. In short,\n\nHypothesis \\(H_1\\) predicts a specific relationship between a dependent variable and an independent variable. If \\(H_1\\) holds, studies often speak of a correlation (or association) between variables.\nits opposite, Hypothesis \\(H_0\\), describes the state of affairs where the predicted relationship between the variables does not hold.\n\nThe predictions made by the hypotheses are based on previous research. Here is an example:\nResearch question: I am studying the pronunciation of [r] in New York because I want to find out whether different social groups differ with respect to their pronunciation, in order to help my readers understand how social factors influence our linguistic behavior.\n\nHypothesis \\(H_1\\): There is a difference in the use of postvocalic /r/ between upper-class and lower-class speakers.\nHypothesis \\(H_0\\): There is no difference in the use of postvocalic /r/ between upper-class and lower-class speakers.\n\n\n\n\n\nExercise 1 Assess the following research questions!\n\nI will try to find the specific linguistic features which a Shetlander uses when speaking Scottish standard English, i.e., Shetland accent in Scottish Standard English.\nIn this paper, I am going to analyze the use of American and Indian dialect features among Indian immigrants to the US because I want to find out whether they correlate with their willingness to integrate into American society; this will show the degree to which linguistic adaptation depends on cultural orientation.\nIn this paper, I am going to address the question of how gender influences language.\nThe study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all.\nIn this paper, adverts by Coca-Cola and Pepsi will be analyzed for the use of verb forms.\n\n\n\nExercise 2 Develop valid research questions on the basis of the following linguistic variables:\n\nSpeakers in bilingual conversations are especially likely to code-switch when there is a significant change of topic.\nWomen use more tentative language in conversations than men.",
    "crumbs": [
      "1. Fundamentals",
      "1.2 Research Questions"
    ]
  },
  {
    "objectID": "chapters/01-rqs.html#what-makes-a-good-research-question",
    "href": "chapters/01-rqs.html#what-makes-a-good-research-question",
    "title": "1.2 Research questions",
    "section": "",
    "text": "Example: I am studying the pronunciation of [r] in New York because I want to find out whether different social groups differ with respect to their pronunciation, in order to help my readers understand how social factors influence our linguistic behavior.\n\nTopic: I am studying...\nIndirect question: Because I want to find out what, why, how...\n(General) linguistic significance: ... in order to help the readers understand how, why, or whether...\n\n\nCf. Booth, Colomb, and Williams (2008): 45–48\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nA research question must be simple, specific, and doable. It is better to make a small contribution to a particular problem than to aim at covering a whole subfield of sociolinguistics.\nKeep a research question stable in your paper, even if you come across interesting new ideas as you work on it.\nThe research question must be an explicit part of your paper. Mention it in the introduction and keep referring back to it from time to time.\nKeep in mind that the research question will determine both methodology and data.\n\n\nCf. Hazen (2015): 9–10\n\n\n\n\n\n\nResearch questions must be complemented by a set of falsifiable hypotheses. These will be covered in-depth in the chapter on hypothesis testing. In short,\n\nHypothesis \\(H_1\\) predicts a specific relationship between a dependent variable and an independent variable. If \\(H_1\\) holds, studies often speak of a correlation (or association) between variables.\nits opposite, Hypothesis \\(H_0\\), describes the state of affairs where the predicted relationship between the variables does not hold.\n\nThe predictions made by the hypotheses are based on previous research. Here is an example:\nResearch question: I am studying the pronunciation of [r] in New York because I want to find out whether different social groups differ with respect to their pronunciation, in order to help my readers understand how social factors influence our linguistic behavior.\n\nHypothesis \\(H_1\\): There is a difference in the use of postvocalic /r/ between upper-class and lower-class speakers.\nHypothesis \\(H_0\\): There is no difference in the use of postvocalic /r/ between upper-class and lower-class speakers.\n\n\n\n\n\nExercise 1 Assess the following research questions!\n\nI will try to find the specific linguistic features which a Shetlander uses when speaking Scottish standard English, i.e., Shetland accent in Scottish Standard English.\nIn this paper, I am going to analyze the use of American and Indian dialect features among Indian immigrants to the US because I want to find out whether they correlate with their willingness to integrate into American society; this will show the degree to which linguistic adaptation depends on cultural orientation.\nIn this paper, I am going to address the question of how gender influences language.\nThe study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all.\nIn this paper, adverts by Coca-Cola and Pepsi will be analyzed for the use of verb forms.\n\n\n\nExercise 2 Develop valid research questions on the basis of the following linguistic variables:\n\nSpeakers in bilingual conversations are especially likely to code-switch when there is a significant change of topic.\nWomen use more tentative language in conversations than men.",
    "crumbs": [
      "1. Fundamentals",
      "1.2 Research Questions"
    ]
  },
  {
    "objectID": "chapters/03-data-annotation.html",
    "href": "chapters/03-data-annotation.html",
    "title": "3.3 Data annotation",
    "section": "",
    "text": "S. T. Gries (2013): Chapter 1.3.3",
    "crumbs": [
      "3. NLP with R",
      "3.3 Data Annotation"
    ]
  },
  {
    "objectID": "chapters/03-data-annotation.html#recommended-reading",
    "href": "chapters/03-data-annotation.html#recommended-reading",
    "title": "3.3 Data annotation",
    "section": "",
    "text": "S. T. Gries (2013): Chapter 1.3.3",
    "crumbs": [
      "3. NLP with R",
      "3.3 Data Annotation"
    ]
  },
  {
    "objectID": "chapters/03-data-annotation.html#sample-study",
    "href": "chapters/03-data-annotation.html#sample-study",
    "title": "3.3 Data annotation",
    "section": "Sample study",
    "text": "Sample study\n\nTheoretical background\nLet’s assume we are interested in the object realisation patterns of the verb eat in the British ICE component. A quick review of the literature tells us that …\n\n… argument realisation may be related to the aspectual structure of a verbal action (cf. Goldberg 2001), but …\n… there is a stronger focus on situation aspect (telicity/atelicity; i.e., logical endpoints of actions) than on grammatical aspect (i.e., perfective/progressive).\n\nSince grammatical aspect is also concerned with the temporal construal of actions, it raises the question of whether or not it can also influence object realisation. To investigate the relationship between aspect and object realisation, we will perform an exemplary analysis on the verb lemma EAT.\n\n\nObtaining data\nWe load all necessary libraries to query the ICE-GB corpus and run a KWIC-search using the regular expression \\\\b(eat(s|ing|en)?|ate)\\\\b, which finds all inflection forms of EAT. We then store the results in a spreadsheet file kwic_eat.xlsx.\n\n# Load library and corpus\nlibrary(quanteda)\nlibrary(writexl)\n\nICE_GB &lt;- readRDS(\"../datasets/ICE_GB.RDS\")\n\n# Perform query\nkwic_eat &lt;- kwic(ICE_GB,\n          phrase(\"\\\\b(eat(s|ing|en)?|ate)\\\\b\"),\n          valuetype = \"regex\",\n          window = 15)\n\n# Store results\nwrite_xlsx(kwic_eat, \"../datasets/kwic_eat.xlsx\")\n\nWhen you open kwic_eat.xlsx in a spreadsheet software, the file will contain 7 columns by default (docname, from, to, keyword, post, pattern). Each row corresponds to a match of your search expression in the corpus, which is equal to 113 here. This is your raw output.",
    "crumbs": [
      "3. NLP with R",
      "3.3 Data Annotation"
    ]
  },
  {
    "objectID": "chapters/03-data-annotation.html#data-annotation",
    "href": "chapters/03-data-annotation.html#data-annotation",
    "title": "3.3 Data annotation",
    "section": "Data annotation",
    "text": "Data annotation\nWhenever you decide to work on your corpus results, it is good practice to duplicate your file and append the current date to the filename. Re-save it as, for instance, kwic_eat_09_09_2024.xlsx and open it again. This way you’re performing basic version control, which will allow you to return to previous stages of your analysis with ease.\nIn your spreadsheet software, you can now assign your variables of interest to the empty columns next to your output data. For our specific example, we will need one that captures object realisation and one the type of verb aspect. Let’s simply call them object_realisation and verb_aspect.\n\n\n\n\n\n\nNaming variables\n\n\n\nOf course, you could also opt for a different column name, as long it has no spaces or special characters (e.g., !?%#). You could also name it object_realisation or, even more plainly, object, but not direct object or object realisation with spaces. Otherwise you are bound to encounter a surge of cryptic error messages in your R console.\n\n\nNow, you are ready to annotate your data! An easy coding scheme would involve classifying rows where eat occurs with an object as yes. Conversely, rows where the direct object is not realised syntactically are assigned the column value no. In the aspect column, verbal aspect will be coded as either perfective, progressive or neutral, following S. Gries and Deshors (2014): 118.\n\n\n\n\n\n\nDealing with problematic cases\nHowever, things are not always that clear-cut. What if you encounter a false positive, i.e., an erroneous hit in your dataset? Further down in the spreadsheet the keyword ate is actually part of the preceding word, inappropriate.\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\n113\nICE_GB/W2F-019.txt\n696\n696\n: 1 &gt; Too much colour on her face would be inappropri &lt; l &gt;\nate\n, she feels , but she wears a light foundation . &lt; ICE-GB:W2F-019 #28 :\n\\b(eat(s|ing|en)?|ate)\\b\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do I do with false hits?\n\n\n\nShort answer: Do not delete irrelevant rows or columns. Essentially, from the moment you’ve obtained your corpus output, you should withstand the temptation to delete anything from it. Instead, adopt the practice of indicating missing values or irrelevant rows by an NA in a separate column. In later analyses, these can be easily filtered out!\nThis also minimises the risk of accidentally getting rid of data that could have proven important at a later point in time.\n\n\n\n\nGetting the data back into R\nImport the Excel file via\n\n# Load library\nlibrary(readxl)\n\n# Read file contents into the variable \"kwic_data\"\nkwic_data &lt;- read_xlsx(\"../datasets/kwic_eat_09_09_2024.xlsx\")\n\n# Print the first six lines of \"kwic_data\"\nprint(head(kwic_data))\n\n# A tibble: 6 × 9\n  docname  from    to pre   keyword post  pattern object_realisation aspect_verb\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;              &lt;chr&gt;      \n1 ICE_GB…   458   458 had … eaten   anyw… \"\\\\b(e… no                 perfective \n2 ICE_GB…   478   478 : 1 … eating  will… \"\\\\b(e… no                 progressive\n3 ICE_GB…   785   785 &gt; Ye… eat     befo… \"\\\\b(e… no                 neutral    \n4 ICE_GB…  1198  1198 the … eat     them… \"\\\\b(e… yes                neutral    \n5 ICE_GB…  4529  4529 &gt; Ye… ate     in t… \"\\\\b(e… no                 neutral    \n6 ICE_GB…   958   958 know… eat     it f… \"\\\\b(e… yes                neutral    \n\n\n\n\nAdding a case list\nS. T. Gries (2013) recommends setting up the first column of the data frame such that it “numbers all n cases from \\(1\\) to \\(n\\) so that every row can be uniquely identified and so that you always restore one particular ordering (e.g., the original one)” (S. T. Gries 2013: 26). This is very easy to do: We specify a numeric vector ranging from 1 to the total number of rows in the data frame.\n\n# Create a new Case column (which, by default, is moved to the very end of the data frame)\nkwic_data$Case &lt;- 1:nrow(kwic_data)\n\n# Move the Case column to the front of the data frame\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nkwic_data &lt;- relocate(kwic_data, Case)\n\n# Print reordered data frame\nprint(head(kwic_data))\n\n# A tibble: 6 × 10\n   Case docname        from    to pre   keyword post  pattern object_realisation\n  &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;             \n1     1 ICE_GB/S1A-0…   458   458 had … eaten   anyw… \"\\\\b(e… no                \n2     2 ICE_GB/S1A-0…   478   478 : 1 … eating  will… \"\\\\b(e… no                \n3     3 ICE_GB/S1A-0…   785   785 &gt; Ye… eat     befo… \"\\\\b(e… no                \n4     4 ICE_GB/S1A-0…  1198  1198 the … eat     them… \"\\\\b(e… yes               \n5     5 ICE_GB/S1A-0…  4529  4529 &gt; Ye… ate     in t… \"\\\\b(e… no                \n6     6 ICE_GB/S1A-0…   958   958 know… eat     it f… \"\\\\b(e… yes               \n# ℹ 1 more variable: aspect_verb &lt;chr&gt;",
    "crumbs": [
      "3. NLP with R",
      "3.3 Data Annotation"
    ]
  },
  {
    "objectID": "chapters/03-data-annotation.html#where-do-i-go-from-here",
    "href": "chapters/03-data-annotation.html#where-do-i-go-from-here",
    "title": "3.3 Data annotation",
    "section": "Where do I go from here?",
    "text": "Where do I go from here?\nAs soon as you’ve fully annotated your dataset and reviewed it for potential coding errors, the next step involves analysing your data statistically to uncover potential patterns that are not visible to the naked eye. These patterns can include (subtle to major) differences in frequency of occurrence, relationships with other linguistically relevant features (e.g., register/genre) or probabilities (e.g., the probability that an object is omitted), among many others. The following units offer an introduction to statistics, where “statistics” is best understood as “a collection of methods which help us to describe, summarize, interpret, and analyse data” (Heumann, Schomaker, and Shalabh 2022).",
    "crumbs": [
      "3. NLP with R",
      "3.3 Data Annotation"
    ]
  },
  {
    "objectID": "chapters/06-mixed_effects_regression.html",
    "href": "chapters/06-mixed_effects_regression.html",
    "title": "6.3 Mixed-effects regression",
    "section": "",
    "text": "For linguists:\n\nSchäfer (2020)\n\nGeneral:\n\nGelman and Hill (2007)",
    "crumbs": [
      "6. Statistical Modelling",
      "6.3 Mixed-effects regression"
    ]
  },
  {
    "objectID": "chapters/06-mixed_effects_regression.html#recommended-reading",
    "href": "chapters/06-mixed_effects_regression.html#recommended-reading",
    "title": "6.3 Mixed-effects regression",
    "section": "",
    "text": "For linguists:\n\nSchäfer (2020)\n\nGeneral:\n\nGelman and Hill (2007)",
    "crumbs": [
      "6. Statistical Modelling",
      "6.3 Mixed-effects regression"
    ]
  },
  {
    "objectID": "chapters/06-mixed_effects_regression.html#preparation",
    "href": "chapters/06-mixed_effects_regression.html#preparation",
    "title": "6.3 Mixed-effects regression",
    "section": "Preparation",
    "text": "Preparation\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(tidyr)\nlibrary(lme4) # for linear mixed-effects models\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nlibrary(sjPlot)\nlibrary(ggeffects)\nlibrary(ggpubr)\n\n# Load data\nvarmorph &lt;- read.csv(\"../datasets/varmorph_data.csv\", header = TRUE)\n\n# Reduce data\nvarmorph %&gt;%\n  select(rt, target, prime_type, subj_id) %&gt;%\n  filter(prime_type != \"filler\") %&gt;% \n  drop_na() -&gt; varmorph2\n\n# Overview\nglimpse(varmorph2)\n\nRows: 7,038\nColumns: 4\n$ rt         &lt;dbl&gt; 599.63, 885.39, 1124.94, 568.68, 726.24, 1095.37, 492.96, 6…\n$ target     &lt;chr&gt; \"print\", \"defend\", \"tempt\", \"hunt\", \"staple\", \"pose\", \"kick…\n$ prime_type &lt;chr&gt; \"derived\", \"derived\", \"derived\", \"derived\", \"derived\", \"der…\n$ subj_id    &lt;chr&gt; \"3202\", \"3202\", \"3202\", \"3202\", \"3202\", \"3202\", \"3202\", \"32…",
    "crumbs": [
      "6. Statistical Modelling",
      "6.3 Mixed-effects regression"
    ]
  },
  {
    "objectID": "chapters/06-mixed_effects_regression.html#sec-mer",
    "href": "chapters/06-mixed_effects_regression.html#sec-mer",
    "title": "6.3 Mixed-effects regression",
    "section": "Multilevel models",
    "text": "Multilevel models\nAt their core, mixed-effects models “are extensions of regression in which data are structured in groups and coefficients can vary by group” (Gelman and Hill 2007: 237). Typical grouping structures found in linguistic data include speakers, regions, or lexical stimuli for which multiple observations are attested. Normally, such structures would violate the assumption of independence, but can be controlled for by capturing group-wise tendencies.\nFor illustration, a simple example of a hierarchical dataset is presented in Figure 1. If one were to, for instance, measure test scores for every student, it may be of interest how their performance varies not only from student to student but also from school to school. After all, the students are nested within their schools.\n\n\n\n\n\n\n\ngraph LR\n    A1[School 1] --&gt; B11(Student 1)\n    A1[School 1] --&gt; B12(Student 2)\n    A1[School 1] --&gt; B13(Student 3)\n    \n    A2[School 2] --&gt; B21(Student 4)\n    A2[School 2] --&gt; B22(Student 5)\n    A2[School 2] --&gt; B23(Student 6)\n    \n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nRead the following (partial) description of the experiments conducted by Ciaccio & Veríssimo on the morphological processing of complex lexical items (2022):\n\nSixty-nine intermediate to advanced non-native speakers of English (54 women; 15 men) took part in the experiment in exchange for payment or course credits. […] The experiment included 102 English monomorphemic verbs used as targets (e.g., print). These were preceded by their -ed past-tense form (e.g., printed) as the inflected prime, their -er nominalization (e.g., printer) as the derived prime, or by an unrelated prime. Unrelated primes were dissimilar in form and meaning from their corresponding targets; half of them were -ed inflected forms and half of them were -er derived words. (Ciaccio and Veríssimo 2022: 2267)\n\nInspect varmorph2 and characterise its multilevel structure.\n\n\n\nTypes of mixed-effects models\nVariance across groups can be captured by varying-intercept and/or varying-slope models. These varying coefficients also known as random effects (cf. Gelman and Hill (2007): 245). In the model equation, the intercept \\(\\alpha\\) and/or the slope \\(\\beta\\) is additionally indexed for the grouping factor. Let \\(J\\) denote the number of groups for \\(j = 1, ..., J\\).\n\n\n\n\n\n\nVarying-intercept model\n\n\n\nWe allow group-wise variation in the intercept by replacing \\(\\alpha\\) with \\(\\alpha_{j}\\) to indicate the intercept for the \\(j\\)-th group. It is defined as a random variable and follows the normal distribution. For instance, each participant in the aforementioned psycholinguistic would receive its own intercept rather than a global one for all participants.\n\\[\nY = \\alpha_{j} + \\beta_1X_{1} + \\beta_2X_{2} + ... + \\epsilon \\qquad \\alpha_{j} \\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha}^2)\n\\tag{1}\\]\n\n\n\n\n\n\n\n\nVarying-slope model\n\n\n\nWe will allow group-wise variation in the slope coefficients by replacing them with \\(\\beta_{ij}\\) to indicate the slope for the \\(j\\)-th group. The slope now functions as a random variable and is normally distributed. In the psycholinguistic study, each participant would be assigned its own slope coefficient.\n\\[\nY = \\alpha + \\beta_{1j}X_{1} + \\beta_{2j}X_{2} + ... + \\epsilon \\qquad \\beta_{j} \\sim N(\\mu_{\\beta}, \\sigma_{\\beta}^2)\n\\tag{2}\\]\n\n\n\n\nExample\nAssume we are predicting test performance by School. Using simulated data, the following series of plots plots illustrate …\n\n… School as a fixed effect,\n… random intercepts for each School,\n… random slopes for each School, and\n… random intercepts and slopes for each School.\n\n\n\n\n\n\n\nPlot: (1) Fixed-effects model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot: (2) Varying-intercept model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot: (3) Varying-slope model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot: (4) Varying-intercept and varying-slope model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear mixed-effects models\n\n\nApplication in R\n\nVarying-intercept model\n\n# Varying intercept model\n\n# Define reference level for \"prime_type\"\nvarmorph2$prime_type &lt;- factor(varmorph2$prime_type, levels = c(\"unrelated\", \"derived\", \"inflected\"))\n\n# Fit mixed-effects models\nvarmorph.me &lt;- lmer(rt ~ prime_type + # fixed effect\n                      (1 | subj_id) + # let intercept vary by subject\n                      (1 | target), # # let intercept vary by target word\n                      data = varmorph2)\n\n# Summarise results\nsummary(varmorph.me)\n\n\ntab_model(varmorph.me, show.se = TRUE, show.aic = TRUE, show.dev = TRUE)\n\n\n\n \nrt\n\n\nPredictors\nEstimates\nstd. Error\nCI\np\n\n\n(Intercept)\n715.70\n12.66\n690.89 – 740.52\n&lt;0.001\n\n\nprime type [derived]\n-31.31\n5.01\n-41.14 – -21.48\n&lt;0.001\n\n\nprime type [inflected]\n-33.96\n5.01\n-43.78 – -24.13\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n29483.89\n\n\n\nτ00 target\n3690.56\n\n\nτ00 subj_id\n7693.61\n\n\nICC\n0.28\n\n\nN subj_id\n69\n\n\nN target\n102\n\nObservations\n7038\n\n\nMarginal R2 / Conditional R2\n0.006 / 0.283\n\n\nDeviance\n92860.321\n\n\nAIC\n92855.624\n\n\n\n\n\n\n\n\n\n\n\n\nICC\n\n\n\nThe intraclass correlation coefficient (ICC) “ranges from \\(0\\) if the grouping conveys no information to \\(1\\) if all members of a group are identical” (Gelman and Hill 2007: 258). In other words, it indicates how much of the variance in the outcome can be explained by the grouping factor (e.g. school or participant).\n\n\n\n\nShow the code\n# Extract random effects and their standard errors\nranef_obj &lt;- ranef(varmorph.me)  # Extract random effects with conditional variance\nse_ranef &lt;- arm::se.ranef(varmorph.me)           # Extract standard errors for random effects\n\n# Prepare a data frame for 'subj_id' random intercepts with confidence intervals\nsubj_ranef &lt;- ranef_obj$subj_id  # Random effects for subjects\nsubj_se &lt;- se_ranef$subj_id      # Standard errors for subjects\n\n# Combine random effects and standard errors into a data frame\nsubj_df &lt;- data.frame(\n  subj_id = rownames(subj_ranef),\n  intercept = subj_ranef[, \"(Intercept)\"],\n  se = subj_se[, \"(Intercept)\"],\n  conf.low = subj_ranef[, \"(Intercept)\"] - 1.96 * subj_se[, \"(Intercept)\"],\n  conf.high = subj_ranef[, \"(Intercept)\"] + 1.96 * subj_se[, \"(Intercept)\"]\n)\n\n# Create the waterfall plot\nggplot(subj_df, aes(x = reorder(subj_id, intercept), y = intercept)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", col = \"grey10\") +\n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\n  coord_flip() +\n  geom_vline(xintercept = 0, linetype = \"dashed\", col = \"grey10\") +\n  labs(title = \"Random Intercepts by Subject\",\n       x = \"Subject ID\",\n       y = \"Random Intercept\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nVarying-slope model\n\n# Varying-slope model; replace 0 with 1 if you want the intercept to vary too\nvarmorph.me2 &lt;- lmer(rt ~ prime_type +\n                      (0 + prime_type | subj_id),\n                      data = varmorph2)\n\nsummary(varmorph.me2)\n\n\ntab_model(varmorph.me2, show.se = TRUE, show.aic = TRUE, show.dev = TRUE)\n\n\n\n \nrt\n\n\nPredictors\nEstimates\nstd. Error\nCI\np\n\n\n(Intercept)\n715.70\n11.11\n693.92 – 737.49\n&lt;0.001\n\n\nprime type [derived]\n-31.31\n5.57\n-42.23 – -20.39\n&lt;0.001\n\n\nprime type [inflected]\n-33.96\n5.32\n-44.38 – -23.53\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n33126.00\n\n\n\nτ00\n \n\n\nτ00\n \n\n\nτ11 subj_id.prime_typeunrelated\n7545.01\n\n\nτ11 subj_id.prime_typederived\n8193.86\n\n\nτ11 subj_id.prime_typeinflected\n7385.08\n\n\nρ01\n \n\n\nρ01\n \n\n\nICC\n0.14\n\n\nN subj_id\n69\n\nObservations\n7038\n\n\nMarginal R2 / Conditional R2\n0.006 / 0.141\n\n\nDeviance\n93452.160\n\n\nAIC\n93455.369\n\n\n\n\n\n\n\n\nShow the code\n# Extract random slopes\nranef_data1 &lt;- ranef(varmorph.me2)$subj_id\n\n# Extract standard errors\nranef_data1_se &lt;- arm::se.ranef(varmorph.me2)$subj_id\n\n# Convert data into long format\nrandom_effects_df &lt;- ranef_data1 %&gt;%\n  as.data.frame() %&gt;% \n  rownames_to_column(var = \"subj_id\") %&gt;%\n  pivot_longer(cols = -subj_id, \n               names_to = \"prime_type\", \n               values_to = \"random_effect\")\n\n\n# Create a data frame for standard errors\nse_df &lt;- as.data.frame(ranef_data1_se) %&gt;%\n  rownames_to_column(var = \"subj_id\") %&gt;%\n  pivot_longer(cols = -subj_id, \n               names_to = \"prime_type\", \n               values_to = \"se\")\n\n\n# Combine random effects with standard errors\ncombined_df &lt;- random_effects_df %&gt;%\n  left_join(se_df, by = c(\"subj_id\", \"prime_type\"))\n\n# Calculate confidence intervals\ncombined_df &lt;- combined_df %&gt;%\n  mutate(lower_ci = random_effect - 1.96 * se,\n         upper_ci = random_effect + 1.96 * se)\n\n\n# Dotplots with confidence intervals\nggplot(combined_df, aes(x = subj_id, y = random_effect, col = prime_type)) +\n  coord_flip() +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", col = \"grey10\") +\n  facet_wrap(~ prime_type) +\n  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), width = 0.2) +\n  labs(title = \"Random Effect of Prime Type by Subject ID\",\n       x = \"Random Slope\",\n       y = \"Subject ID\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))",
    "crumbs": [
      "6. Statistical Modelling",
      "6.3 Mixed-effects regression"
    ]
  },
  {
    "objectID": "chapters/04-continuous_data.html",
    "href": "chapters/04-continuous_data.html",
    "title": "4.4 Continuous data",
    "section": "",
    "text": "Heumann et al. (2022: Chapter 3)",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.4 Continuous data"
    ]
  },
  {
    "objectID": "chapters/04-continuous_data.html#suggested-reading",
    "href": "chapters/04-continuous_data.html#suggested-reading",
    "title": "4.4 Continuous data",
    "section": "",
    "text": "Heumann et al. (2022: Chapter 3)",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.4 Continuous data"
    ]
  },
  {
    "objectID": "chapters/04-continuous_data.html#preparation",
    "href": "chapters/04-continuous_data.html#preparation",
    "title": "4.4 Continuous data",
    "section": "Preparation",
    "text": "Preparation\nWe will use the dataset from the previous unit:\n\n# Libraries\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\n\n# Load data from working directory\ncl.order &lt;- read_xlsx(\"../datasets/Paquot_Larsson_2020_data.xlsx\")",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.4 Continuous data"
    ]
  },
  {
    "objectID": "chapters/04-continuous_data.html#measures-of-central-tendency",
    "href": "chapters/04-continuous_data.html#measures-of-central-tendency",
    "title": "4.4 Continuous data",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\nFrom here on out, we assume \\(X\\) is a continuous random variable with observations \\(\\{x_1, x_2, ..., x_n\\}\\) and sample size \\(n\\). Measures of central tendency offer convenient one-value-summaries of the distribution of \\(X\\).\n\nThe sample mean\nThe sample mean \\(\\bar{x}\\) is defined as\n\\[\n\\bar{x} = \\frac{x_1 + x_2 + ... + x_n}{n} \\\\ = \\frac{1}{n}\\sum_{i=1}^n{x_i}.\n\\tag{1}\\]\nIn R, we can obtain the average value of a numeric vector with the mean() function. Let’s do that for the length of main clauses found in the cl.order data:\n\nmean(cl.order$LEN_MC)\n\n[1] 9.265509\n\n\nThe output returned by this function provides a one-value summary of all observations contained in LEN_MC. Because the the mean \\(\\bar{x}\\) takes into account all data points, it is prone to the influence of outliers, i.e., extreme values.\nThe distribution of continuous variables is best visualised in terms of histograms or density plots, which are illustrated for LEN_MC. The blue line indicates the sample mean.\n\n\n\n\n\n\nVisualisation in R\n\n\n\n\n\n\nHistogram (ggplot2)Density plot (ggplot2)Histogram (Base R)Density plot (Base R)\n\n\n\n# Plot distribution of LEN_MC\ncl.length.hist &lt;- ggplot(cl.order, aes(x = LEN_MC)) +\n                  geom_histogram(binwidth = 2)\n\ncl.length.hist +\n  # Add mean\n  geom_vline(aes(xintercept = mean(LEN_MC)),\n             color = \"steelblue\",\n             linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n# Plot distribution of LEN_MC\ncl.length.dens &lt;- ggplot(cl.order, aes(x = LEN_MC)) +\n                  geom_density()\n\ncl.length.dens +\n  # Add mean\n  geom_vline(aes(xintercept = mean(LEN_MC)),\n             color = \"steelblue\",\n             linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nhist(cl.order$LEN_MC)\n  abline(v=mean(cl.order$LEN_MC),lwd=3, col = \"steelblue\")\n\n\n\n\n\n\n\n\n\n\n\nplot(density(cl.order$LEN_MC))\n  abline(v=mean(cl.order$LEN_MC),lwd=3, col = \"steelblue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe median\nThe median() function computes the “the halfway point of the data (50% of the data are above the median; 50% of the data are below” (Winter 2020: 58). As such, it is the measure of choice for data with many outliers as well as for ordinal data (e.g. Likert-scale ratings).\n\\[\n\\tilde{x}_{0.5} =\n\\begin{cases}\nx_{((n+1)/2)} & \\text{if } n \\text{ is odd.} \\\\\n\\frac{1}{2}(x_{n/2}+x_{(n/2+1)}) & \\text{if } n \\text{ is even.}\n\\end{cases}\n\\tag{2}\\]\n\nmedian(cl.order$LEN_MC)\n\n[1] 8\n\n\n\n\n\n\n\n\nVisualisation in R\n\n\n\n\n\nThe median of LEN_MC is represented by the red vertical line.\n\nHistogram (ggplot2)Density plot (ggplot2)Histogram (Base R)Density plot (Base R)\n\n\n\ncl.length.hist +\n  # Add mean\n  geom_vline(aes(xintercept = mean(LEN_MC)), color = \"steelblue\", linewidth = 1) +\n  # Add median\n  geom_vline(aes(xintercept = median(LEN_MC)), color = \"red\", linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\ncl.length.dens +\n  # Add mean\n  geom_vline(aes(xintercept = mean(LEN_MC)), color = \"steelblue\", linewidth = 1) +\n  # Add median\n  geom_vline(aes(xintercept = median(LEN_MC)), color = \"red\", linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nhist(cl.order$LEN_MC)\n  abline(v=mean(cl.order$LEN_MC),lwd=3, col = \"steelblue\")\n  abline(v=median(cl.order$LEN_MC),lwd=3, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nplot(density(cl.order$LEN_MC))\n  abline(v=mean(cl.order$LEN_MC),lwd=3, col = \"steelblue\")\n  abline(v=mean(cl.order$LEN_MC),lwd=3, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample variance and standard deviation\nIn order to assess how well the mean represents the data, it is instructive to compute the variance var() and the standard deviation sd() for a sample.\nThe unbiased sample variance \\(s^2\\) is defined as\n\\[\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^n{(x_i - \\bar{x})^2}.\n\\tag{3}\\]\nIn other words, it represents for the average squared deviation of all observations from the sample mean.\n\nvar(cl.order$LEN_MC)\n\n[1] 25.12585\n\n\nCorrespondingly, the standard deviation of the mean is the square root of the variance:\n\\[\ns = \\sqrt{s^2}\n\\tag{4}\\]\n\nsd(cl.order$LEN_MC)\n\n[1] 5.012569\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Comparison of different parameter values\n\n\n\n\n\n\n\n\n\nVisualisation in R\n\n\n\n\n\n\nExample 1Example 2\n\n\n\ncl.length.hist +\n  # Add verticle line for the mean\n  geom_vline(aes(xintercept = mean(LEN_MC)), color = \"steelblue\", linewidth = 1) +\n  # Add -1sd\n  geom_vline(aes(xintercept = mean(LEN_MC) - sd(LEN_MC)), color = \"orange\", linewidth = 1) +\n  # Add +1sd\n  geom_vline(aes(xintercept = mean(LEN_MC) + sd(LEN_MC)), color = \"orange\", linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n# Create data frame with mean and sd for each clause ORDER\n\ncl.order %&gt;% \n  # Select variables of interest\n  select(ORDER, LEN_MC) %&gt;% \n  # Group results of following operations by ORDER\n  group_by(ORDER) %&gt;% \n    # Create grouped summary of mean and sd for each ORDER\n    summarise(mean = mean(LEN_MC),\n                sd = sd(LEN_MC)) -&gt; cl_mean_sd; cl_mean_sd\n\n# A tibble: 2 × 3\n  ORDER  mean    sd\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 mc-sc  9.04  4.91\n2 sc-mc  9.75  5.22\n\n# Plot results \n\nggplot(cl_mean_sd, aes(x = ORDER, y = mean)) +\n  # Barplot with a specific variable mapped onto y-axis\n  geom_col() +\n  # Add mean and standard deviation to the plot\n  geom_errorbar(aes(x = ORDER,\n                    ymin = mean-sd,\n                    ymax = mean+sd), width = .2) +\n  theme_classic() +\n  labs(y = \"Mean length of main clauses\", x = \"Clause order\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantiles\nWhile median() divides the data into two equal sets (i.e., two 50% quantiles), the quantile() function makes it possible to partition the data further.\n\nquantile(cl.order$LEN_MC)\n\n  0%  25%  50%  75% 100% \n   2    6    8   11   31 \n\n\nquantile(x, 0) and quantile(x, 1) thus show the minimum and maximum values, respectively.\n\nquantile(cl.order$LEN_MC, 0)\n\n0% \n 2 \n\nquantile(cl.order$LEN_MC, 1)\n\n100% \n  31 \n\n\n\n\nQuartiles and boxplots\nConsider the distribution of clause length by clause order:\n\nBoxplot (Base R)Boxplot (ggplot2)\n\n\n\nboxplot(LEN_MC ~ ORDER, cl.order)\n\n\n\n\n\n\n\n\n\n\n\nggplot(cl.order, aes(x = ORDER, y = LEN_MC)) +\n  geom_boxplot() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nCompare it to the corresponding rotated density plot:\n\nggplot(cl.order, aes(x = LEN_MC, fill = ORDER)) +\n  geom_density(alpha = 0.5) +\n  coord_flip() +\n  theme_classic()",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.4 Continuous data"
    ]
  },
  {
    "objectID": "chapters/04-continuous_data.html#visualising-mixed-data",
    "href": "chapters/04-continuous_data.html#visualising-mixed-data",
    "title": "4.4 Continuous data",
    "section": "Visualising mixed data",
    "text": "Visualising mixed data\n\nA numerical and categorical variable\n\nBoxplot with geom_boxplot()\n\n\nggplot(cl.order, aes(x = ORDER, y = LEN_MC)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nDensitiy plot using the optional arguments color and/or fill\n\n\nggplot(cl.order, aes(x = LEN_MC, fill = ORDER)) +\n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nA barplot with geom_col()\n\n\nggplot(cl.order, aes(x = ORDER, y = LEN_MC)) +\n  geom_col(aes(x = ORDER, y = LEN_MC))\n\n\n\n\n\n\n\n\n\n\nMultivariate plots\n\nAdvanced scatterplot with four variables: LEN_MC (x), LEN_SC (y), ORDER (colour) and SUBORDTYPE (shape)\n\n\n# 4 variables\nggplot(cl.order, aes(x = LEN_MC, y = LEN_SC)) +\n  geom_point(aes(color = ORDER, shape = SUBORDTYPE))\n\n\n\n\n\n\n\n\n\nFacets\n\n\n# 5 variables\nggplot(cl.order, aes(x = LEN_MC, y = LEN_SC)) +\n  geom_point(aes(color = ORDER, shape = SUBORDTYPE)) +\n  facet_wrap(~MORETHAN2CL)",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.4 Continuous data"
    ]
  },
  {
    "objectID": "chapters/06-ordinal_regression.html",
    "href": "chapters/06-ordinal_regression.html",
    "title": "6.5 Ordinal regression",
    "section": "",
    "text": "General:\n\nBaguley (2012): Chapter 17.4.5\nO’Connell (2006)\nPowers and Xie (2008): Chapter 7\nDocumentation of Cumulative Link Models",
    "crumbs": [
      "6. Statistical Modelling",
      "6.5 Ordinal regression"
    ]
  },
  {
    "objectID": "chapters/06-ordinal_regression.html#suggested-reading",
    "href": "chapters/06-ordinal_regression.html#suggested-reading",
    "title": "6.5 Ordinal regression",
    "section": "",
    "text": "General:\n\nBaguley (2012): Chapter 17.4.5\nO’Connell (2006)\nPowers and Xie (2008): Chapter 7\nDocumentation of Cumulative Link Models",
    "crumbs": [
      "6. Statistical Modelling",
      "6.5 Ordinal regression"
    ]
  },
  {
    "objectID": "chapters/06-ordinal_regression.html#introduction",
    "href": "chapters/06-ordinal_regression.html#introduction",
    "title": "6.5 Ordinal regression",
    "section": "Introduction",
    "text": "Introduction\nIn her recent contribution, Glass (2021) examines possible reasons why certain transitive verbs have a stronger affinity towards object omission compared to others, placing special emphasis on the routinisation of the actions denoted by the verbs. Specifically, she assesses how high/low-routine contexts affect the acceptability of object omission for transitive verbs from different frequency bins.\nWe will replicate her findings using her survey data Glass_2021_survey_processed.csv1:\n1 The original dataset can be retrieved from Lelia Glass’s OSF repository: https://osf.io/t6zw5 [Last accessed: 27th September, 2024].\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ordinal)\n\n\nAttaching package: 'ordinal'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(sjPlot)\n\nLearn more about sjPlot with 'browseVignettes(\"sjPlot\")'.\n\nlibrary(effects)\n\nLoading required package: carData\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nlibrary(ggeffects)\nlibrary(ggpubr)\n\n# For additional tests\nlibrary(DescTools)\nlibrary(generalhoslem)\n\nLoading required package: reshape\n\nAttaching package: 'reshape'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nThe following object is masked from 'package:dplyr':\n\n    rename\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, smiths\n\nlibrary(brant)\n\n# Load data\nsurvey &lt;- read.csv(\"../datasets/Glass_2021_survey_processed.csv\")\n\n# Inspect dataset\nstr(survey)\n\n'data.frame':   784 obs. of  5 variables:\n $ rating       : int  3 4 3 3 2 4 3 2 3 2 ...\n $ freq         : chr  \"hi\" \"lo\" \"lo\" \"hi\" ...\n $ verb         : chr  \"pick\" \"catch\" \"throw\" \"break\" ...\n $ ParticipantID: chr  \"Participant1\" \"Participant1\" \"Participant1\" \"Participant1\" ...\n $ routine      : chr  \"hi\" \"lo\" \"lo\" \"lo\" ...\n\nhead(survey)\n\n  rating freq   verb ParticipantID routine\n1      3   hi   pick  Participant1      hi\n2      4   lo  catch  Participant1      lo\n3      3   lo  throw  Participant1      lo\n4      3   hi  break  Participant1      lo\n5      2   hi  taste  Participant1      hi\n6      4   hi bottle  Participant1      hi\n\n\n\n\n\n\n\n\nShort breakdown of the variables\n\n\n\n\n\n\nroutine: In Glass’s study, transitive verbs were randomly assigned to one of the following conditions:\n\n\n\n(High routine condition:) I worked at my poultry farm. Just like I always do, I butchered some chickens. Then I gathered some eggs.\n(Low-routine condition:) I visited a friend’s job. Just because people wanted me to try it, I butchered some chickens. Then I went for a walk.\n\nCf. Glass (2021: 66)\n\n\nunique(survey$routine)\n\n[1] \"hi\" \"lo\"\n\n\n\nrating records the responses of participants to a follow-up question regarding the acceptability of object omission. The answers are recorded on a 1-5 Likert scale.\n\n\nThe next time Caroline talks about butchering chickens the day before, how likely do you think she is to say the following?\n‘I butchered yesterday’\nCf. Glass (2021: 66)\n\n\nunique(survey$rating)\n\n[1] 3 4 2 5 1\n\n\n\nverb contains the items to be rated for the conditions in routine\n\n\nunique(survey$verb)\n\n[1] \"pick\"   \"catch\"  \"throw\"  \"break\"  \"taste\"  \"bottle\" \"sell\"   \"chop\"  \n\n\n\nfrequency relates to the frequency bins of the verbs:\n\n\nunique(survey$freq)\n\n[1] \"hi\" \"lo\"\n\n\n\nParticipantID identifies each of the 98 subjects who provided ratings",
    "crumbs": [
      "6. Statistical Modelling",
      "6.5 Ordinal regression"
    ]
  },
  {
    "objectID": "chapters/06-ordinal_regression.html#descriptive-overview",
    "href": "chapters/06-ordinal_regression.html#descriptive-overview",
    "title": "6.5 Ordinal regression",
    "section": "Descriptive overview",
    "text": "Descriptive overview\n\n\nShow the code\nggplot(survey, aes(x = rating, fill = freq)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~freq) +\n  theme_minimal() +\n  labs(title = \"Density of Ratings by Frequency\",\n       x = \"Rating\", y = \"Density\", fill = \"Frequency\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(survey, aes(x = rating, fill = routine)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~routine) +\n  theme_minimal() +\n  labs(title = \"Density of Ratings by Routine\",\n       x = \"Rating\", y = \"Density\", fill = \"Routine\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggridges)\nggplot(survey, aes(x = rating, y = verb, fill = verb)) +\n  geom_density_ridges(scale = 5, rel_min_height = 0.01, alpha = 0.6) +\n  theme_ridges() +\n  #theme(legend.position = \"none\") +\n  labs(title = \"Distribution of Ratings by Verb\",\n       x = \"Rating\", y = \"Verb\")\n\n\nPicking joint bandwidth of 0.381",
    "crumbs": [
      "6. Statistical Modelling",
      "6.5 Ordinal regression"
    ]
  },
  {
    "objectID": "chapters/06-ordinal_regression.html#modelling-ordinal-data",
    "href": "chapters/06-ordinal_regression.html#modelling-ordinal-data",
    "title": "6.5 Ordinal regression",
    "section": "Modelling ordinal data",
    "text": "Modelling ordinal data\nOur task is clear: We need to measure how routine and freq affect the variability in the acceptability ratings, while controlling for repeated measurements for verb and ParticipantID, which impose a hierarchical structure on the dataset.\nFormally speaking, we have \\(p\\) explanatory variables \\(X_1, X_2, ..., X_p\\) for \\(1, ..., p\\). The target variable, i.e. our \\(Y\\), is rating with the ordered, discrete outcomes \\(y \\in \\{1, 2, 3, 4, 5\\}\\).\nThe goal is to find a model \\(f\\) that describes the relationship between \\(Y\\) and \\(X_p\\) as accurately as possible and minimises the error term \\(\\epsilon\\):\n\\[\nY = f(X_1, X_2, ..., X_p) + \\epsilon\n\\tag{1}\\]\n\nOrdered logistic regression\nOne family of models that respects the ordered, yet categorical nature of \\(Y\\) is ordered (or ordinal) logistic regression. Other terms include proportional odds models and cumulative logit/link models.\n\n\n\n\n\n\nRecap: Logistic regression\n\n\n\n\n\nLogistic regression is used to model categorical response variables with two or more levels. For instance, let’s assume our \\(Y\\) is dichotomous with the following two outcomes:\n\\[\nY =\n\\begin{cases}\n\\text{yes} \\\\\n\\text{no}\n\\end{cases}\n\\tag{2}\\]\nUsing the logistic function, we can estimate the probability of one outcome versus the other given the predictors \\(X_p\\). Their log-transformed odds ratio (log odds) is equivalent of the all-too-familiar linear model:\n\\[\n\\log\\left(\\frac{P(Y = yes \\mid X_1, X_2, ..., X_p)}{1 - P(Y = yes \\mid X_1, X_2, ..., X_p)}\\right) = \\beta_0 + \\sum_{i=1}^p \\beta_iX_i\n\\tag{3}\\]\n\n\n\nCore to this approach is the notion of cumulative probabilities. Let \\(J\\) denote the number of ordered categories in \\(Y\\). In Glass’s case study, the estimated cumulative probabilities for each ordered outcome (= acceptability rating) would have the forms in Equation 4.\n\\[\n\\begin{array}{rcl}\nP(Y \\leq 1) & = & P(Y = 1) \\\\\nP(Y \\leq 2) & = & P(Y = 1) + P(Y = 2) \\\\\nP(Y \\leq 3) & = & P(Y = 1) + P(Y = 2) + P(Y = 3) \\\\\n& \\vdots & \\\\\nP(Y \\leq j) & = & P_1 + ... + P_j\n\\end{array}\n\\tag{4}\\]\nWe can now update our logistic regression model to take into account cumulative probabilities for \\(j = 1, ..., J-1\\).\n\\[\n\\log\\left(\\frac{P(Y \\leq j \\mid X_1, X_2, ..., X_p)}{1 - P(Y \\leq j \\mid X_1, X_2, ..., X_p)}\\right) = \\beta_0 + \\sum_{i=1}^p \\beta_iX_i\n\\tag{5}\\]\nThe intercepts \\(\\beta_{0_j}\\) serve as cutpoints between the adjacent ordinal categories. For \\(J = 5\\) categories, there are \\(J - 1 = 4\\) cutpoints, i.e.,\n\n1|2 for \\(P(Y \\leq 1)\\)\n2|3 for \\(P(Y \\leq 2)\\)\n3|4 for \\(P(Y \\leq 3)\\)\n4|5 for \\(P(Y \\leq 4)\\).\n\nGiven a change in predictor values, the slope coefficients \\(\\beta_pX_p\\) indicate how the probability of being in a higher rating category changes (Baguley 2012: 691–2).\nWe can obtain “regular” probabilities from the cumulative ones by drawing on the equivalence in Equation 6.\n\\[\nP(Y = j) = P(Y \\leq j) - P(Y \\leq j - 1)\n\\tag{6}\\]\nFor instance, the probability \\(P(Y = 3)\\) is equivalent to\n\\[\nP(Y = 3) = P(Y \\leq 3) - P(Y \\leq 2).\n\\tag{7}\\]\n\n\n\n\n\n\nAssumptions of proportional odds models\n\n\n\n\n\nThe proportional odds assumption stipulates a stable effect of the predictors on the (log) odds of the ordinal outcomes across all possible cutpoints (O’Connell 2006: 29). In case of violation, it is better to rely on partial proportional odds models or multinomial logistic regression instead.",
    "crumbs": [
      "6. Statistical Modelling",
      "6.5 Ordinal regression"
    ]
  },
  {
    "objectID": "chapters/06-ordinal_regression.html#application-in-r",
    "href": "chapters/06-ordinal_regression.html#application-in-r",
    "title": "6.5 Ordinal regression",
    "section": "Application in R",
    "text": "Application in R\nThere are several R packages that support ordinal logistic regression models. This section provides an overview of some of the more common (as well as well-documented) implementations.\n\nUsing polr() from the MASS library\n\n# Convert to factor and determine ordering\nsurvey$rating &lt;- factor(survey$rating, ordered = TRUE, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n# Fit polr model\nsurvey.polr &lt;- polr(rating ~ \n                      freq +\n                      routine,\n                      data = survey)\n\n# Model summary\nsummary(survey.polr)\n\n\nRe-fitting to get Hessian\n\n\nCall:\npolr(formula = rating ~ freq + routine, data = survey)\n\nCoefficients:\n             Value Std. Error  t value\nfreqlo    -0.01095     0.1291 -0.08483\nroutinelo -0.55521     0.1302 -4.26449\n\nIntercepts:\n    Value   Std. Error t value\n1|2 -0.8704  0.1228    -7.0859\n2|3  0.1342  0.1188     1.1290\n3|4  1.2528  0.1293     9.6856\n4|5  2.8915  0.2003    14.4345\n\nResidual Deviance: 2246.662 \nAIC: 2258.662 \n\n# R-squared and AIC\nPseudoR2(survey.polr, c(\"Nagelkerke\", \"AIC\"))\n\n  Nagelkerke          AIC \n   0.0244793 2258.6617419 \n\n\n\ntab_model(survey.polr, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, transform = NULL)\n\n\nRe-fitting to get Hessian\n\n\n\n\n\n \nrating\n\n\nPredictors\nLog-Odds\nstd. Error\nCI\np\n\n\n1|2\n-0.87\n0.12\n-1.11 – -0.63\n&lt;0.001\n\n\n2|3\n0.13\n0.12\n-0.10 – 0.37\n0.259\n\n\n3|4\n1.25\n0.13\n1.00 – 1.51\n&lt;0.001\n\n\n4|5\n2.89\n0.20\n2.50 – 3.28\n&lt;0.001\n\n\nfreq [lo]\n-0.01\n0.13\n-0.26 – 0.24\n0.932\n\n\nroutine [lo]\n-0.56\n0.13\n-0.81 – -0.30\n&lt;0.001\n\n\nObservations\n784\n\n\nR2 Nagelkerke\n0.024\n\n\nDeviance\n2246.662\n\n\nAIC\n2258.662\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the model parameters\n\n\n\n\n\n\nCoefficients: The conditions freqlo (low frequency) and routinelo (low-routine context) both have negative values, which means that both of them decrease the probability of obtaining a higher acceptability rating (compared to freqhi and routinehi).\nIntercepts: These represent the cutpoints between the ordinal categories, which are necessary for calculating the probabilities of each ordinal category.\n\n\n\n\n\n\nTesting assumptions and goodness of fit\n\nTest proportional odds assumption:\n\n\nbrant(survey.polr) # p &lt; 0.05 is a violation of the assumption\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     14.45   6   0.02\nfreqlo      6.5 3   0.09\nroutinelo   8.14    3   0.04\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\n\nHosmer-Lemeshow test, which is essentially a \\(\\chi^2\\)-test:\n\n\nlogitgof(survey$rating, # observed\n         fitted(survey.polr), # expected\n         ord = TRUE) # respect ordering\n\n\n    Hosmer and Lemeshow test (ordinal model)\n\ndata:  survey$rating, fitted(survey.polr)\nX-squared = 51.173, df = 35, p-value = 0.03808\n\n\n\nThe Lipsitz test is an extension of the Hosmer-Lemeshow test. Note that it requires the response to be a factor.\n\n\nlipsitz.test(survey.polr)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  rating ~ freq + routine\nLR statistic = 20.261, df = 9, p-value = 0.01637\n\n\n\nPart of the same family of tests is the Pulkstenis-Robinson test, which also relies on the \\(\\chi^2\\)-distribution:\n\n\npulkrob.chisq(survey.polr, catvars = c(\"freq\", \"routine\"))\n\n\n    Pulkstenis-Robinson chi-squared test\n\ndata:  formula:  rating ~ freq + routine\nX-squared = 21.476, df = 9, p-value = 0.0107\n\n\n\n\nVisualisation\n\nWith effects\n\n# Routine effect plot\nplot(Effect(focal.predictors = c(\"routine\"), mod = survey.polr), rug = FALSE, style=\"stacked\")\n\n\n\nWith ggeffects and ggplot2\n\n\nShow the code\n# Get the ggeffects data\neff &lt;- ggeffects::ggeffect(survey.polr, \"freq\")\n\n# Convert to a data frame\nplot_data &lt;- as.data.frame(eff)\n\n# Ensure the response.level has the desired levels\nplot_data$response.level &lt;- factor(plot_data$response.level, \n                                   levels = c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"),\n                                   labels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n# Create the plot with confidence intervals\np1 &lt;- ggplot(plot_data, aes(x = x, y = predicted, color = response.level, group = response.level)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1, linewidth = 0.5) +\n  scale_color_viridis_d() +\n  labs(\n    x = \"Frequency\",\n    y = \"Predicted Probability\",\n    color = \"Rating\"\n  ) +\n  facet_grid(~ response.level) +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    panel.grid.minor = element_blank(),\n    axis.title = element_text(face = \"bold\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(margin = margin(b = 10))\n  ) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1))\n\np1\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Get the ggeffects data for \"routine\"\neff_routine &lt;- ggeffects::ggeffect(survey.polr, \"routine\")\n\n# Convert to a data frame\nplot_data_routine &lt;- as.data.frame(eff_routine)\n\n# Ensure the response.level has the desired levels for \"routine\"\nplot_data_routine$response.level &lt;- factor(plot_data_routine$response.level, \n                                           levels = c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"),\n                                           labels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n# Create the second plot for \"routine\"\np2 &lt;- ggplot(plot_data_routine, aes(x = x, y = predicted, color = response.level, group = response.level)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1, linewidth = 0.5) +\n  scale_color_viridis_d() +\n  labs(\n    x = \"Routine\",\n    y = \"Predicted Probability\",\n    color = \"Rating\"\n  ) +\n  facet_grid(~ response.level) +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    panel.grid.minor = element_blank(),\n    axis.title = element_text(face = \"bold\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(margin = margin(b = 10))\n  ) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1))\n\np2\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Generate the interaction effects for \"freq\" and \"routine\"\neff_interaction &lt;- ggeffect(survey.polr, terms = c(\"freq\", \"routine\"))\n\n# Convert to a data frame\nplot_data_interaction &lt;- as.data.frame(eff_interaction)\n\n# Ensure the response.level has the desired levels\nplot_data_interaction$response.level &lt;- factor(plot_data_interaction$response.level, \n                                               levels = c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"),\n                                               labels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n\n# Create the interaction plot with facet by 'x' and color by 'response.level'\np_interaction &lt;- ggplot(plot_data_interaction, aes(x = group, y = predicted, color = response.level, group = response.level)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1, linewidth = 0.5) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Predicted Probabilities for Interaction of Frequency and Routine\",\n    x = \"Routine\",\n    y = \"Predicted Probability\",\n    color = \"Rating\"\n  ) +\n  facet_wrap(~ x, labeller = labeller(x = c(\"hi\" = \"High Frequency\", \"lo\" = \"Low Frequency\"))) +  # Facet by \"freq\"\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    panel.grid.minor = element_blank(),\n    axis.title = element_text(face = \"bold\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(margin = margin(b = 10))\n  ) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1))\n\n# Display the interaction plot\np_interaction\n\n\n\n\n\n\n\n\n\n\n\n\nUsing clm() from the ordinal library\n\n# Convert to factor and determine ordering\nsurvey$rating &lt;- factor(survey$rating, ordered = TRUE, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n# Fit cumulative link model\nclm.1 &lt;- ordinal::clm(rating ~ \n                    freq +\n                    routine,\n                    data = survey, Hess=TRUE)\n\n# Model summary\nsummary(clm.1)\n\n\ntab_model(clm.1, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, transform = NULL)\n\n\n\n\n \nrating\n\n\nPredictors\nLog-Odds\nstd. Error\nCI\np\n\n\n1|2\n-0.87\n0.12\n-1.11 – -0.63\n&lt;0.001\n\n\n2|3\n0.13\n0.12\n-0.10 – 0.37\n0.259\n\n\n3|4\n1.25\n0.13\n1.00 – 1.51\n&lt;0.001\n\n\n4|5\n2.89\n0.20\n2.50 – 3.28\n&lt;0.001\n\n\nfreq [lo]\n-0.01\n0.13\n-0.26 – 0.24\n0.932\n\n\nroutine [lo]\n-0.56\n0.13\n-0.81 – -0.30\n&lt;0.001\n\n\nObservations\n784\n\n\nR2 Nagelkerke\n0.024\n\n\nAIC\n2258.662\n\n\n\n\n\n\n\n\n\nMixed-effects ordinal regression\n\n\n\n\n\n\nRecap: Mixed-effects models\n\n\n\n\n\nIf the data is nested according to some grouping factor with \\(1, ..., k\\) groups, we can let the intercept and/or slopes vary by group. For instance, recall the varying-intercept model:\n\\[\nY = \\alpha_{k} + \\beta_1X_{1} + \\beta_2X_{2} + ... + \\epsilon \\qquad \\alpha_{k} \\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha}^2).\n\\] In this case we also speak of random effects.\n\n\n\n\n# Convert to factor and determine ordering\nsurvey$rating &lt;- factor(survey$rating, ordered = TRUE, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n# Fit mixed model with random intercepts for \"verb\" and \"ParticipantID\"\nclm.2 &lt;- ordinal::clmm(rating ~ \n                    freq * routine +\n                    (1 | verb) +\n                    (1 | ParticipantID),\n                    data = survey, Hess=TRUE)\n\n# Model summary\nsummary(clm.2)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: rating ~ freq * routine + (1 | verb) + (1 | ParticipantID)\ndata:    survey\n\n link  threshold nobs logLik  AIC     niter     max.grad cond.H \n logit flexible  784  -991.65 2001.30 598(2386) 3.81e-04 1.5e+02\n\nRandom effects:\n Groups        Name        Variance Std.Dev.\n ParticipantID (Intercept) 2.0628   1.4363  \n verb          (Intercept) 0.9059   0.9518  \nNumber of groups:  ParticipantID 98,  verb 8 \n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \nfreqlo            -0.1567     0.2138  -0.733 0.463437    \nroutinelo         -0.7473     0.2103  -3.553 0.000381 ***\nfreqlo:routinelo  -0.1406     0.2981  -0.472 0.637046    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -1.3562     0.4020  -3.374\n2|3   0.2062     0.3988   0.517\n3|4   1.8329     0.4053   4.523\n4|5   3.8639     0.4401   8.780\n\n\n\ntab_model(clm.2, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, transform = NULL)\n\n\n\n \nrating\n\n\nPredictors\nLog-Odds\nstd. Error\nCI\np\n\n\n1|2\n-1.36\n0.40\n-2.14 – -0.57\n0.001\n\n\n2|3\n0.21\n0.40\n-0.58 – 0.99\n0.605\n\n\n3|4\n1.83\n0.41\n1.04 – 2.63\n&lt;0.001\n\n\n4|5\n3.86\n0.44\n3.00 – 4.73\n&lt;0.001\n\n\nfreq [lo]\n-0.16\n0.21\n-0.58 – 0.26\n0.463\n\n\nroutine [lo]\n-0.75\n0.21\n-1.16 – -0.34\n&lt;0.001\n\n\nfreq [lo] × routine [lo]\n-0.14\n0.30\n-0.72 – 0.44\n0.637\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 ParticipantID\n2.06\n\n\nτ00 verb\n0.91\n\n\nICC\n0.47\n\n\nN verb\n8\n\n\nN ParticipantID\n98\n\nObservations\n784\n\n\nMarginal R2 / Conditional R2\n0.028 / 0.489\n\n\nAIC\n2001.305\n\n\n\n\n\n\n\n\nShow the code\n# Extract random effects\nre_verb &lt;- ranef(clm.2)$verb\nre_participant &lt;- ranef(clm.2)$ParticipantID\n\n# Create dataframes for random effects\ndf_verb &lt;- data.frame(verb = rownames(re_verb), re = re_verb[,1])\ndf_participant &lt;- data.frame(ParticipantID = rownames(re_participant), re = re_participant[,1])\n\n# Get predictions for an average case\npred_avg &lt;- ggpredict(clm.2, terms = c(\"freq\", \"routine\"))\n\n# Add random effects to predictions\npred_verb &lt;- crossing(pred_avg, df_verb) %&gt;%\n  mutate(predicted = predicted + re)\n\npred_participant &lt;- crossing(pred_avg, df_participant) %&gt;%\n  mutate(predicted = predicted + re)\n\n# Create a horizontal dot plot for random effects of participants\np_caterpillar &lt;- ggplot(df_participant, aes(x = re, y = reorder(ParticipantID, re))) +\n  geom_point(size = 3, color = \"steelblue3\") +  # Dots representing the random effects\n  labs(title = \"Random Effects for Participants\", \n       x = \"Random Effect Estimate (log odds)\", \n       y = \"Participant ID\") +\n   geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey20\") +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank())  # Removes the gridlines for y-axis\n\np_caterpillar2 &lt;- ggplot(df_verb, aes(x = re, y = reorder(verb, re))) +\n  geom_point(size = 3, color = \"steelblue3\") +  # Dots representing the random effects\n  labs(title = \"Random Effects for Verbs\", \n       x = \"Random Effect Estimate (log odds)\", \n       y = \"Verb\") +\n   geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey20\") +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank())  # Removes the gridlines for y-axis\n\nggarrange(p_caterpillar, p_caterpillar2, ncol = 2, common.legend = TRUE, legend = \"right\")",
    "crumbs": [
      "6. Statistical Modelling",
      "6.5 Ordinal regression"
    ]
  },
  {
    "objectID": "chapters/06-ordinal_regression.html#generalised-additive-mixed-effects-models-gamms",
    "href": "chapters/06-ordinal_regression.html#generalised-additive-mixed-effects-models-gamms",
    "title": "6.5 Ordinal regression",
    "section": "Generalised Additive Mixed-effects Models (GAMMs)",
    "text": "Generalised Additive Mixed-effects Models (GAMMs)\n\nSuggested reading\nFor linguists:\n\nBaayen & Linke (2020)\n\nGeneral:\n\nHastie & Tibshirani (1991)\nWood (2006)\n\n\n\nRationale\nA core assumption of Generalised Linear Models (GLMs) is a linear relationship between predictor(s) and response. If, however, one is interested in exploring potential non-linear trends without the risk of extreme overfitting, GAMs offer an elegant solution: Instead of relying on the linear sum of model coefficients, GAMs estimate more flexible smooth terms \\(f_k\\) for \\(k = 1, ..., p\\). For illustration, Equation 8 shows a linear additive model for a continuous target variable with \\(p\\) predictors.\n\\[\nY = \\beta_0 + \\sum\\limits_{k = 1}^p f_k(X_k)\n\\tag{8}\\]\n\n\nApplication in R\n\n# Load libraries\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n\nlibrary(itsadug)\n\nLoading required package: plotfunctions\n\n\n\nAttaching package: 'plotfunctions'\n\n\nThe following object is masked from 'package:ggpubr':\n\n    get_palette\n\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\n\nLoaded package itsadug 2.4 (see 'help(\"itsadug\")' ).\n\nlibrary(gratia)\n\n# Convert predictors to factors\nsurvey$ParticipantID &lt;- as.factor(survey$ParticipantID)\nsurvey$verb &lt;- as.factor(survey$verb)\n\n# Fit GAMM\ngam1 &lt;- bam(as.numeric(rating) ~ # treated as numeric term\n              freq + # linear term\n              routine + # linear term\n              s(ParticipantID, bs = \"re\") + # smooth term\n              s(verb, bs = \"re\"), # smooth term\n              data = survey, \n              family = ocat(R = 5) # number of ordinal categories\n            )\n\n# Model summary\nsummary(gam1)\n\n\nFamily: Ordered Categorical(-1,0.51,2.09,4.06) \nLink function: identity \n\nFormula:\nas.numeric(rating) ~ freq + routine + s(ParticipantID, bs = \"re\") + \n    s(verb, bs = \"re\")\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.3613     0.3880   0.931    0.352    \nfreqlo       -0.2195     0.1431  -1.534    0.125    \nroutinelo    -0.7752     0.1428  -5.430 7.79e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                    edf Ref.df      F p-value    \ns(ParticipantID) 77.654     97  4.677  &lt;2e-16 ***\ns(verb)           6.728      7 25.860  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDeviance explained = 27.3%\nfREML =   1631  Scale est. = 1         n = 784\n\n# Extract the intercepts for plotting\nthresh &lt;- gratia::theta(gam1) %&gt;% \n  tibble::as_tibble() %&gt;% \n  setNames(c(\"threshold\"))\n\n# Extract predictions for \"routine\"\nroutine_pred &lt;- ggpredict(gam1, terms = \"routine\")\n\n# Plot predictions\nroutine_pred %&gt;% \n  ggplot(aes(x = x, y = predicted)) +\n  geom_point(col = \"steelblue\") +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = 0), col = \"steelblue\") +\n  geom_hline(data = thresh, aes(yintercept = threshold), linetype = \"dashed\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Extract random effects for \"verb\"\nverb_pred &lt;- ggpredict(gam1, terms = \"verb\")\n\n# Plot random effect\nverb_pred %&gt;% \n  ggplot(aes(x = x, y = predicted)) +\n  geom_point(col = \"steelblue\") +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = 0), col = \"steelblue\") +\n  geom_hline(data = thresh, aes(yintercept = threshold), linetype = \"dashed\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Extract random effects for \"ParticipantID\"\nsubj_pred &lt;- ggpredict(gam1, terms = \"ParticipantID\")\n\n# Plot random effect\nsubj_pred |&gt;\n  ggplot(aes(x = x, y = predicted)) +\n  geom_point(col = \"steelblue\") +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = 0), col = \"steelblue\") +\n  #geom_line() +\n  geom_hline(data = thresh, aes(yintercept = threshold), linetype = \"dashed\") +\n  theme_minimal()",
    "crumbs": [
      "6. Statistical Modelling",
      "6.5 Ordinal regression"
    ]
  },
  {
    "objectID": "chapters/05-t_test.html",
    "href": "chapters/05-t_test.html",
    "title": "4.7 t-test",
    "section": "",
    "text": "Load packages and data:\n\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\n\ndata_vowels &lt;- read.csv(\"../datasets/Vowels_Apache.csv\", sep = \"\\t\")",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.7 t-test"
    ]
  },
  {
    "objectID": "chapters/05-t_test.html#preparation",
    "href": "chapters/05-t_test.html#preparation",
    "title": "4.7 t-test",
    "section": "",
    "text": "Load packages and data:\n\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\n\ndata_vowels &lt;- read.csv(\"../datasets/Vowels_Apache.csv\", sep = \"\\t\")",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.7 t-test"
    ]
  },
  {
    "objectID": "chapters/05-t_test.html#the-t-test",
    "href": "chapters/05-t_test.html#the-t-test",
    "title": "4.7 t-test",
    "section": "The \\(t\\)-test",
    "text": "The \\(t\\)-test\nThe way \\(t\\) is calculated depends on the sources of \\(X\\) and \\(Y\\): Do they originate from the same sample or from two (in-)dependent ones?\nFirst, we consider two independent samples from a population:\n\nSample \\(X\\) with the observations \\(\\{x_1, x_2, ..., {x_n}_1\\}\\), sample size \\(n_x\\), sample mean \\(\\hat{\\mu}_{x}\\) and sample variance \\(\\hat{\\sigma}^2_x\\).\nSample \\(Y\\) with the observations \\(\\{y_1, y_2, ..., {y_n}_2\\}\\), sample size \\(n_y\\), sample mean \\(\\hat{\\mu}_{y}\\) and sample variance \\(\\hat{\\sigma}^2_y\\).",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.7 t-test"
    ]
  },
  {
    "objectID": "chapters/05-t_test.html#workflow-in-r",
    "href": "chapters/05-t_test.html#workflow-in-r",
    "title": "4.7 t-test",
    "section": "Workflow in R",
    "text": "Workflow in R\n\nDefine hypotheses\n\n\\(H_0:\\) mean F1 frequency of men \\(=\\) mean F1 frequency of women.\n\\(H_1:\\) mean F1 frequency of men \\(\\ne\\) mean F1 frequency of women.\n\n\n\nDescriptive overview\nWe select the variables of interest and proceed calculate the mean F1 frequencies for each level of SEX, requiring a grouped data frame.\n\n\nCode\n# Filter data so as to show only those observations that are relevant\ndata_vowels %&gt;% \n  # Filter columns\n  select(HZ_F1, SEX) %&gt;%\n    # Define grouping variable\n    group_by(SEX) %&gt;% \n      # Compute mean and standard deviation for each sex\n      summarise(mean = mean(HZ_F1),\n                sd = sd(HZ_F1)) -&gt; data_vowels_stats\n\nknitr::kable(data_vowels_stats)\n\n\n\n\n\nSEX\nmean\nsd\n\n\n\n\nF\n528.8548\n110.80099\n\n\nM\n484.2740\n87.90112\n\n\n\n\n\n\n\nCode\n# Plot distributions\ndata_vowels_stats %&gt;% \n  ggplot(aes(x = SEX, y = mean)) +\n    geom_col() +\n    geom_errorbar(aes(x = SEX,\n                    ymin = mean-sd,\n                    ymax = mean+sd), width = .2) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\nCode\n# Plot quartiles\ndata_vowels %&gt;% \n  ggplot(aes(x = SEX, y = HZ_F1)) +\n    geom_boxplot() +\n    theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nCheck \\(t\\)-test assumptions\n\n# Normality\nshapiro.test(data_vowels$HZ_F1) # H0: data points follow the normal distribution; however, this test is pretty unreliable!\n\n\n    Shapiro-Wilk normality test\n\ndata:  data_vowels$HZ_F1\nW = 0.98996, p-value = 0.5311\n\n# Check histogram\nggplot(data_vowels, aes(x = HZ_F1)) +\n  geom_histogram(bins = 30) +\n  theme_classic()\n\n\n\n\n\n\n\n# Variance homogeneity\nvar.test(data_vowels$HZ_F1 ~ data_vowels$SEX) # H0: variances are not too different from each other\n\n\n    F test to compare two variances\n\ndata:  data_vowels$HZ_F1 by data_vowels$SEX\nF = 1.5889, num df = 59, denom df = 59, p-value = 0.07789\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.949093 2.660040\nsample estimates:\nratio of variances \n          1.588907 \n\n\n\n\nRunning the test\n\n# t-test for two independent samples \nt.test(data_vowels$HZ_F1 ~ data_vowels$SEX, paired = FALSE) # there is a significant difference between sample means!\n\n\n\nEffect size\nCohen’s d is a possible effect size measure for continuous data and is obtained by dividing the difference of both sample means by the pooled standard deviation:\n\\[\\frac{\\bar{x} - \\bar{y}}{\\sqrt{\\frac{{(n_1 - 1)s_x^2 + (n_2 - 1)s_y^2}}{{n_1 + n_2 - 2}}}}.\\]\n\ncohen.d(data_vowels$HZ_F1, data_vowels$SEX) # see also ?cohen.d for more details\n\n\nCohen's d\n\nd estimate: 0.4457697 (small)\n95 percent confidence interval:\n     lower      upper \n0.07976048 0.81177897 \n\n\n\n\nReporting the results\nAccording to a two-sample \\(t\\)-test, there is a significant difference between the mean F1 frequencies of male and female speakers of Apache (\\(t = 2.44\\), \\(df = 112.19\\), \\(p &lt; 0.05\\)). Therefore, \\(H_0\\) will be rejected.",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.7 t-test"
    ]
  },
  {
    "objectID": "chapters/05-binomial_test.html",
    "href": "chapters/05-binomial_test.html",
    "title": "4.5 Binomial test",
    "section": "",
    "text": "For linguists:\n\nGries (2021: 39–45)\n\nGeneral:\n\nBaguley (2012: Chapters 2.3.1, 4.5.1, 4.8.4)"
  },
  {
    "objectID": "chapters/05-binomial_test.html#recommended-reading",
    "href": "chapters/05-binomial_test.html#recommended-reading",
    "title": "4.5 Binomial test",
    "section": "",
    "text": "For linguists:\n\nGries (2021: 39–45)\n\nGeneral:\n\nBaguley (2012: Chapters 2.3.1, 4.5.1, 4.8.4)"
  },
  {
    "objectID": "chapters/05-binomial_test.html#linguistic-example-of-a-binomial-distribution",
    "href": "chapters/05-binomial_test.html#linguistic-example-of-a-binomial-distribution",
    "title": "4.5 Binomial test",
    "section": "2 Linguistic Example of a Binomial Distribution",
    "text": "2 Linguistic Example of a Binomial Distribution\nTo illustrate this, consider a binomial distribution in linguistics, such as gender bias in pronoun use. The research question is as follows:\n\nDo people tend to exhibit gender bias in pronoun use (e.g. he or she) when referring to professionals in leadership roles?\n\nAs a result, one can either expect to see pronouns used in a way that aligns with traditional gender stereotypes, or notice efforts to counteract such biases by adopting more inclusive and neutral language. Nevertheless, the pronouns that are being analysed can only be female or male and thus, they represent the required structure for a binomial distribution (he or she), which is binary data.\nA test that is built on the assumptions of the binomial distribution is called the Binomial Test. The Binomial Test calculates the likelihood of observing a particular pronoun (e.g., he) being used to refer to professionals in leadership roles, under the assumption of a given expected proportion, and assesses whether the observed frequency differs significantly from the expected one.\n\n\n\n\n\n\nIs this approach suitable for my research?\n\n\n\nIf you are working with corpus-based frequency data that involves binary categories (yes or no, male or female, VO or OV, British or US-American,…) and if you have a specific probability of success which is specified in your hypotheses and does not change from one trial to the next, then this approach is working for you and will lead to successful results."
  },
  {
    "objectID": "chapters/05-binomial_test.html#understanding-the-binomial-distribution-cumulative-mass-function-cmf-version",
    "href": "chapters/05-binomial_test.html#understanding-the-binomial-distribution-cumulative-mass-function-cmf-version",
    "title": "4.5 Binomial test",
    "section": "3 Understanding the Binomial Distribution (Cumulative Mass Function (CMF) Version)",
    "text": "3 Understanding the Binomial Distribution (Cumulative Mass Function (CMF) Version)\nThe binomial distribution is used to calculate the probability of observing up to and including \\(x\\) successes in \\(n\\) trials, given a fixed probability of success \\(P\\). This calculation is conducted using the Cumulative Mass Function (CMF), which is a running total of probabilities. The CMF is a valuable tool for calculating probabilities for ranges, such as “what is the probability of 0 to 3 successes?” as opposed to the probability of a specific number of successes (see Equation 1). 1\n1 For more information on the binomial test using a specific number of successes, please refer to The Probability Mass Function PMF.\\[ P(x;n,P) = \\sum_{i=1}^{|x|}\\bigg[\\binom{n}{i} p^i (1-P)^{n-i}\\bigg]\n\\tag{1}\\]\n\nKey elements:\n\n\\(x\\): The maximum number of successes that is of interest (e.g., up to 3 successes).\n\\(n\\): The total number of trials (e.g., total observations or experiments).\n\\(P\\): The probability of success in each trial (e.g., the chance of using he when describing a professional).\n\n\n\n\n\n\n\n\nLinguistic example\n\n\n\nImagine studying gender bias in pronoun use. If \\(P = 0.5\\) (i.e., the probability of obtaining the masculine pronoun), and you observe up to 3 uses of he out of 10 trials, the CMF calculates the probability of getting 0, 1, 2, or 3 successes (usage of he) combined."
  },
  {
    "objectID": "chapters/05-binomial_test.html#using-the-binomial-test-cmf-version-in-linguistic-research",
    "href": "chapters/05-binomial_test.html#using-the-binomial-test-cmf-version-in-linguistic-research",
    "title": "4.5 Binomial test",
    "section": "4 Using the Binomial Test (CMF-Version) in Linguistic Research",
    "text": "4 Using the Binomial Test (CMF-Version) in Linguistic Research\nThe CMF is especially useful when testing hypotheses about ranges of outcomes.\nSteps for a Binomial Test Using the CMF:\n\nHypotheses\n\n\nNull hypothesis (\\(H_0\\)): The observed data matches the expected cumulative probability. Therefore: The observed frequency of the pronouns he and she referring to professionals in leadership roles corresponds with the expected proportion of 50% each, indicating that both pronouns are being used equally.\nAlternative hypothesis (\\(H_1\\)): The observed data deviates from the expected cumulative probability. Therefore: The observed frequency of the pronouns he and she referring to professionals in leadership roles deviates from the expected proportion of 50% each, indicating a gender bias.\n\n\nPerforming the test in R by using the cumulative probabilities\n\n\nExample: To calculate the probability of observing up to 3 successes in 10 trials with P=0.5 each, the following code should be used:\n\npbinom(3,10,0.5)\n3. Interpreting the results:\n\nA p-value smaller than 0.05 indicates a significant deviation from the expected probability\nThe test helps to determine whether the observed data can be used to reject the H0.\nChecking the confidence interval (CI) provides the range of likely values for the true success probability\n\n\n\n\n\n\n\nThe Probability Mass Function (PMF)\n\n\n\nThis function is used to calculate the probability of observing exactly x successes in n trials, given a fixed probability of success P.\n\\[\n\\begin{align}\nf(x; n, P) & = \\binom{n}{x} P^x (1-P)^{n-x} \\\\\n\\end{align}\n\\]\nTo calculate the PMF binomial distributions in R, the following code should be used:\n\ndbinom(x = 3, size = 10, prob = 0.5)\n\n[1] 0.1171875"
  },
  {
    "objectID": "chapters/05-binomial_test.html#one-sided-vs.-two-sided-tests",
    "href": "chapters/05-binomial_test.html#one-sided-vs.-two-sided-tests",
    "title": "4.5 Binomial test",
    "section": "5 One-Sided vs. Two-Sided Tests",
    "text": "5 One-Sided vs. Two-Sided Tests\nThe choice between one-sided and two-sided tests depends on the research question.\n\nOne-Sided Test: Used only for analysing deviations in one direction.\n\nExample: Is a participant’s performance better than chance?\n\nTwo-Sided Test: Used for analysing deviations in both directions.\n\nExample: Is the frequency of male pronouns different from the frequency of female pronouns? (The result can either be that the frequency of male pronouns is higher than the frequency of female pronouns or that they are less frequent than female pronouns.)\n\n\nTwo-Sided Binomial Test\n\nbinom.test(x = 15, n = 20, p = 0.5, alternative = \"two.sided\")\n\n\n    Exact binomial test\n\ndata:  15 and 20\nnumber of successes = 15, number of trials = 20, p-value = 0.04139\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5089541 0.9134285\nsample estimates:\nprobability of success \n                  0.75 \n\n\nOne-Sided Binomial Test\n\nbinom.test(x = 15, n = 20, p = 0.5, alternative = \"greater\")\n\n\n    Exact binomial test\n\ndata:  15 and 20\nnumber of successes = 15, number of trials = 20, p-value = 0.02069\nalternative hypothesis: true probability of success is greater than 0.5\n95 percent confidence interval:\n 0.5444176 1.0000000\nsample estimates:\nprobability of success \n                  0.75 \n\n\n\n\n\n\n\n\nPlease note\n\n\n\nThe function binom.test() in R only uses the CMF function."
  },
  {
    "objectID": "chapters/05-binomial_test.html#comparing-two-groups-proportion-tests",
    "href": "chapters/05-binomial_test.html#comparing-two-groups-proportion-tests",
    "title": "4.5 Binomial test",
    "section": "6 Comparing Two Groups: Proportion Tests",
    "text": "6 Comparing Two Groups: Proportion Tests\nFor the comparison of proportions between two groups, the prop.test() function can be used.\nExample: An experiment is being conducted, in which participants have to determine whether a word they are being shown is an actual lemma of the English language or not.\n\nGroup A achieved 24 correct answers out of 25 trials\nGroup B obtained 19 correct answers out of 25 trials.\nTo see, whether the proportions are significantly different, the following code can be used.\n\n\nprop.test(c(24,19), c(25,24))\n\nWarning in prop.test(c(24, 19), c(25, 24)): Chi-squared approximation may be\nincorrect\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(24, 19) out of c(25, 24)\nX-squared = 1.8525, df = 1, p-value = 0.1735\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.05222032  0.38888699\nsample estimates:\n   prop 1    prop 2 \n0.9600000 0.7916667"
  },
  {
    "objectID": "chapters/05-binomial_test.html#testing-event-counts-over-time-the-poisson-test",
    "href": "chapters/05-binomial_test.html#testing-event-counts-over-time-the-poisson-test",
    "title": "4.5 Binomial test",
    "section": "7 Testing Event Counts Over Time: The Poisson Test",
    "text": "7 Testing Event Counts Over Time: The Poisson Test\nFor analysing event counts over time, such as pauses in speech, the Poisson distribution with the poisson.test() function can be useful.\nExample: An experiment is conducted in which speakers are asked to talk for a predetermined time. and it is being researched how often pauses occur in speech.\n\nParticipant A pauses 18 times in 5 minutes.\nThe researcher expects 2 pauses per minute.\nTo see whether the observed pause rate differs from the expected pause rate, the following test is highly convenient.\n\n\npoisson.test(18, T=5, r=2)\n\n\n    Exact Poisson test\n\ndata:  18 time base: 5\nnumber of events = 18, time base = 5, p-value = 0.01705\nalternative hypothesis: true event rate is not equal to 2\n95 percent confidence interval:\n 2.133588 5.689552\nsample estimates:\nevent rate \n       3.6"
  },
  {
    "objectID": "chapters/01-index.html",
    "href": "chapters/01-index.html",
    "title": "Preface",
    "section": "",
    "text": "This collection of handouts provides a hands-on introduction to data analysis and statistical methods in quantitative corpus linguistics with R. It is designed with accessibility in mind, assuming no prior knowledge of programming or statistics. All you need to get started is a laptop; everything else will be explained within these pages.\nPrimarily, this reader is geared towards students attending the classes Language Variation (BA) and Statistics for Linguistics (MA) at the Catholic University of Eichstätt-Ingolstadt (Germany). However, it is also meant to equip students currently working on their BA/MA/PhD theses with the tools they need to conduct empirical studies on a wide array of linguistic phenomena. The methods presented here reflect the state-of-the-art in corpus-linguistic research, providing readers with current and relevant analytical techniques."
  },
  {
    "objectID": "chapters/01-index.html#overview",
    "href": "chapters/01-index.html#overview",
    "title": "Preface",
    "section": "Overview",
    "text": "Overview\nWe begin by establishing the general structure of corpus-linguistic studies, accompanied by key theoretical and practical considerations. The second section introduces R as an analytical tool, covering its core functionality. With these fundamentals in place, we proceed to query corpora directly within R. Chapters four and five focus on describing discrete and continuous outputs, as well as identifying meaningful associations and differences in the data. Finally, to gain more nuanced insights into potential patterns, we apply common machine learning algorithms to fit, evaluate, and interpret statistical models.\nThroughout this journey, readers will develop the skills to conduct robust corpus-linguistic analyses, from basic querying to advanced statistical modeling."
  },
  {
    "objectID": "chapters/01-index.html#collaborators",
    "href": "chapters/01-index.html#collaborators",
    "title": "Preface",
    "section": "Collaborators",
    "text": "Collaborators\nVladimir Buskin, PhD student at the Department of English Language and Linguistics, Catholic University of Eichstätt-Ingolstadt\nThomas Brunner, Assistant Professor at the Department of English Language and Linguistics, Catholic University of Eichstätt-Ingolstadt\nPhilippa Adolf, PhD student at the Department of Romance Studies, University of Vienna"
  },
  {
    "objectID": "chapters/06-logistic_regression.html",
    "href": "chapters/06-logistic_regression.html",
    "title": "6.2 Logistic Regression",
    "section": "",
    "text": "For linguists:\n\nLevshina (2015: Chapter 12)\nWinter (2020: Chapter 12)\n\nFull theoretical treatment:\n\nJames et al. (2021: Chapter 4)\nHosmer & Lemeshow (2008)",
    "crumbs": [
      "6. Statistical Modelling",
      "6.2 Logistic regression"
    ]
  },
  {
    "objectID": "chapters/06-logistic_regression.html#recommended-reading",
    "href": "chapters/06-logistic_regression.html#recommended-reading",
    "title": "6.2 Logistic Regression",
    "section": "",
    "text": "For linguists:\n\nLevshina (2015: Chapter 12)\nWinter (2020: Chapter 12)\n\nFull theoretical treatment:\n\nJames et al. (2021: Chapter 4)\nHosmer & Lemeshow (2008)",
    "crumbs": [
      "6. Statistical Modelling",
      "6.2 Logistic regression"
    ]
  },
  {
    "objectID": "chapters/06-logistic_regression.html#preparation",
    "href": "chapters/06-logistic_regression.html#preparation",
    "title": "6.2 Logistic Regression",
    "section": "Preparation",
    "text": "Preparation\nConsider the data from Buskin’s (n.d.)1 corpus-study on subject pronoun realisation:\n1 The input data can be downloaded from this OSF repository: https://osf.io/qgnms.\n# Load libraries\nlibrary(tidyverse)\nlibrary(rms)\nlibrary(broom)\nlibrary(sjPlot)\nlibrary(ggeffects)\nlibrary(ggpubr)\n\n# Load data\ndata_pro &lt;- read.csv(\"INPUT_pronouns.csv\", sep = \",\", header = TRUE)\n\n# Inspect data\nstr(data_pro)\nhead(data_pro)\n\n\nTarget variable:\n\nReference (‘overt’, ‘null’)\n\nExplanatory variables:\n\nPerson (‘1.p.’, ‘2.p’, ‘3.p’ as well as the dummy pronouns ‘it’ and ‘there’)\nRegister (the text category in the International Corpus of English; ‘S1A’ are informal conversations, whereas ‘S1B’ comprises formal class lessons)\nVariety (British English ‘GB’, Singapore English ‘SING’ and Hong Kong English ‘HK’), and\nReferentiality (‘referential’ with an identifiable referent or ‘non-referential’ with no/generic reference)\n\n\n\nhead(data_pro)\n\n  Reference Person Register Variety Referentiality\n1     overt      3      S1A      GB    referential\n2     overt      3      S1A      GB    referential\n3     overt      3      S1A      GB    referential\n4     overt      3      S1A      GB    referential\n5     overt      3      S1A      GB    referential\n6     overt      3      S1A      GB    referential\n\ntable(data_pro$Reference)\n\n\n null overt \n  174  4664 \n\n\n\nDescriptive overview",
    "crumbs": [
      "6. Statistical Modelling",
      "6.2 Logistic regression"
    ]
  },
  {
    "objectID": "chapters/06-logistic_regression.html#logistic-regression",
    "href": "chapters/06-logistic_regression.html#logistic-regression",
    "title": "6.2 Logistic Regression",
    "section": "Logistic regression",
    "text": "Logistic regression\nIn contrast to linear regression, logistic regression models a qualitative response variable \\(Y\\) with two outcomes2. In the present study, \\(Y\\) is pronominal Reference and has the outcomes Reference = null and Reference = overt, which represent null and overt subjects, respectively. Dichotomous variables of this kind are also often coded as yes/no or 1/0.\n2 Logistic regression can also be used for \\(\\geq 3\\) classes by breaking down the response variable into a series of dichotomous variables. This is also known as multinomial logistic regression or softmax regression.Another difference from linear regression is the output of the model:\n\nIn linear regression, we obtain a predicted value for the continuous response variable we’re interested in. For instance, if we’re modelling reaction times, the model will return an estimated mean reaction time (given the predictors).\nIn logistic regression, the model will return a probability. In variational linguistics, this may correspond to the probability that a speaker will use one syntactic variant versus the other.\n\nA core component of logistic regression is the logistic function. The rationale for using it is that the output of the function will always lie between \\(0\\) and \\(1\\), and it will always denote a probability.\n\n\n\n\n\n\n\n\n\n\nThe simple logistic model\nAssuming a binary response variable \\(Y\\) with the values 1 and 0 and a single predictor \\(X\\), the conditional probability \\(P(Y = 1 \\mid X)\\) is then equivalent to the inverse logit in Equation 1.\n\\[\nP(Y = 1 \\mid  X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}.\n\\tag{1}\\]\nWith some manipulation it can be shaped into a form that is definitely more familiar:\n\\[\n\\log\\left(\\frac{P(Y = 1 \\mid  X)}{1 - P(Y = 1 \\mid  X)}\\right) = \\beta_0 + \\beta_1X.\n\\tag{2}\\]\nThe logistic model has several characteristic components. The fraction \\(\\frac{P(Y = 1 \\mid X)}{1-P(Y = 1 \\mid X)}\\) represents the odds, which stand for to the probability of one outcome (e.g., Reference = null) compared to the other (e.g., Reference = overt). Their logarithmic transformation are the log odds (or logits) of one outcome versus the other.\n\n\n\n\n\n\nUnderstanding log odds\n\n\n\nWhen interpreting the output of a logistic model, note that\n\npositive log odds indicate an increase in \\(\\frac{P(Y = 1 \\mid X)}{1-P(Y = 1 \\mid X)}\\), whereas\nnegative log odds indicate a decrease in \\(\\frac{P(Y = 1 \\mid X)}{1-P(Y = 1 \\mid X)}\\).\n\n\n\nIn more concrete terms: If we are interested in the probability that the form of pronominal reference is null (our \\(Y\\)) while taking into account the extra-linguistic context (Register; our \\(X\\)), the model would then have the general form in Equation 3.\n\\[\n\\log\\left(\\frac{P(\\text{Reference} = \\text{null} \\mid \\text{Register})}{1- P(\\text{Reference} = \\text{null} \\mid \\text{Register})}\\right) = \\beta_0 + \\beta_1\\text{Register}\n\\tag{3}\\]\n\n\nMultiple logistic regression\nIf more than one predictor is included, the above equations can be expanded so as to take into account \\(p\\) slopes for \\(p\\) independent variables \\(X_1, X_2, ..., X_p\\).\n\\[ P(Y = 1 \\mid X_1, ..., X_p) = \\frac{e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}{1 + e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}.\n\\tag{4}\\]\nConsequently, the log odds correspond to the sum of linear predictors \\(\\beta_1X_1 + \\beta_2X_2 + ...+ \\beta_pX_p\\) (cf. Equation 5).\n\\[\n\\log\\left(\\frac{P(Y = 1 \\mid X_1, ..., X_p)}{1 - P(Y = 1 \\mid X_1, ...,  X_p)}\\right) = \\beta_0 + \\sum_{i=1}^{p} \\beta_i X_i\n\\tag{5}\\]\n\n\nOdds ratios\nTo assess the strength of an effect, it is instructive to examine the odds ratios that correspond to the model coefficients. Odds ratios (OR) are defined as\n\\[\nOR(X_1) = e^{\\beta_1}.\n\\]\n\n\n\n\n\n\nUnderstanding odds ratios\n\n\n\n\n\nEssentially, the OR describes the ratio between two odds with respect to another independent variable. This is illustrated for Reference given Register below:\n\\[\n\\text{OR}(\\text{Reference} \\mid \\text{Register}) = \\frac{\\frac{P(\\text{Reference} = \\text{null} \\mid \\text{Register} = \\text{S1A})}{P(\\text{Reference} = \\text{overt} \\mid \\text{Register} = \\text{S1A})}}{\\frac{P(\\text{Reference} = \\text{null} \\mid \\text{Register} = \\text{S1B})}{P(\\text{Reference} = \\text{overt} \\mid \\text{Register} = \\text{S1B})}}\n\\]\nRead as: ‘The ratio between the probability of a null vs. overt object in S1A and the probability of a null vs. overt object in S1B’.",
    "crumbs": [
      "6. Statistical Modelling",
      "6.2 Logistic regression"
    ]
  },
  {
    "objectID": "chapters/06-logistic_regression.html#finding-beta_0-and-beta_1-maximum-likelihood-estimation",
    "href": "chapters/06-logistic_regression.html#finding-beta_0-and-beta_1-maximum-likelihood-estimation",
    "title": "6.2 Logistic Regression",
    "section": "Finding \\(\\beta_0\\) and \\(\\beta_1\\): Maximum Likelihood Estimation",
    "text": "Finding \\(\\beta_0\\) and \\(\\beta_1\\): Maximum Likelihood Estimation\nIn contrast to continuous data, the estimation of parameters for discrete response variables is less straightforward in that there is no unique solution. Rather than finding a regression line that minimises the distance to all data points, the default approach of logistic models is to find the parameter values that are most likely, given the data. Hence this procedure is also known as Maximum Likelihood Estimation (MLE).\nThe model first makes an assumption about the probability distribution of the data. For categorical data, the binomial distribution is a common choice. Equation 6 indicates the corresponding probability mass function, which describes the probability \\(\\pi\\) of observing \\(y\\) successes in \\(k\\) independent Bernoulli trials. In other words, if we tossed a coin \\(n = 10\\) times and observed \\(y = 5\\) heads (i.e., 5 successes), what is the probability \\(\\pi\\) of a success?\n\\[\nf(y; k; \\pi) = \\binom{k}{y} \\pi^y (1 - \\pi)^{n-y}\n\\tag{6}\\]\nNow, let \\(\\beta\\) denote some parameter of interest (e.g., the slope coefficient of a logistic regression model). Given some observed data, the likelihood of this parameter can be described in terms of the likelihood function \\(L(\\beta)\\) in Equation 7. It assumes \\(n\\) binomial probability mass functions with trials \\(k = 1\\) and computes their product. Since the binomial coefficient \\(\\binom{n}{y}\\) is a constant term, it is typically dropped. In essence, we’re multiplying successes \\(\\pi^{y_i}_i\\) with failures \\((1 - \\pi_i)^{1-y_i}\\) for each data point.\n\\[\nL(\\beta) = \\prod_{i=1}^n \\pi^{y_i}_i (1 - \\pi_i)^{1-y_i}\n\\tag{7}\\]\nWithin the context of our pronoun data, this can be understood more intuitively as the product of the probability that a subject is a null pronoun with the probability that it is not:\n\\[\nL(\\beta) = \\prod_{i=1}^n \\pi^{\\text{null}}_i (1 - \\pi_i)^{1-\\text{null}}\n\\tag{8}\\]\nConventionally, this expression is log-transformed in order to convert the product into a sum because, for one, sums are easier to handle computationally. The log-likelihood function \\(\\ell(\\beta)\\) in Equation 9 forms the basis for a variety of goodness-of-fit measures used to evaluate logistic regression models.\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n y_i \\log(\\pi_i) + (1 - y_i) \\log(1 - \\pi_i)\n\\tag{9}\\]\nAnd in more concrete terms:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\text{null}_i \\log(\\pi_i) + (1 - \\text{null}_i) \\log(1 - \\pi_i)\n\\tag{10}\\]\nIn a next step, all \\(\\pi_i\\)s would be substituted by their respectively probabilities from the right half of Equation Equation 4.\nThe goal is to find the value that maximises \\(\\ell(\\beta)\\), i.e., the maximum likelihood estimator \\(\\hat{\\beta}\\). Approximate solutions can be attained via iterative optimisation techniques (e.g. Newton-Ralphson). Sometimes the algorithm may fail to find an optimal solution, which R may report as a model convergence error. For further technical details, see Wood (2006: 63-66) or Agresti & Kateri (2022: 291-294).",
    "crumbs": [
      "6. Statistical Modelling",
      "6.2 Logistic regression"
    ]
  },
  {
    "objectID": "chapters/06-logistic_regression.html#workflow-in-r",
    "href": "chapters/06-logistic_regression.html#workflow-in-r",
    "title": "6.2 Logistic Regression",
    "section": "Workflow in R",
    "text": "Workflow in R\n\nResearch question and hypotheses\nHow do the intra- and extra-linguistic variables suggested in the literature affect subject pronoun realisation (Definite Null Instantiation) in British English, Singapore English and Hong Kong English?\nGiven the significance level \\(\\alpha = 0.05\\), the hypotheses are: \\[\n\\begin{aligned}\nH_0: & \\quad \\text{None of the predictor coefficients deviate from 0}.\\\\\nH_1: & \\quad \\text{At least one predictor coefficient deviates from 0}.\n\\end{aligned}\n\\]\nThese can be restated mathematically as:\n\\[\n\\begin{aligned}\nH_0: & \\quad \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0 \\\\\nH_1: & \\quad \\text{At least one } \\beta_i \\neq 0 \\text{ for } i \\in \\{1, 2, \\ldots, p\\}\n\\end{aligned} \\]\n\n\nConvert to factors and specify reference levels\nThe next step involves specifying reference levels for all categorical variables. This step is very important because it will directly impact the parameter estimation procedure and, consequently, influence our interpretation of the model output.\n\nThe reference level of the response is usually chosen such that it corresponds to the unmarked or most frequent case. Since overt pronouns are much more common in the data, the reference level of the Reference variable will be set to Reference = overt. This way, the model coefficients will directly represent the probability of the null subject variant (i.e., the special case) given certain predictor configurations.\nThe predictor levels need to be specified as well. Among other things, we are interested in how the Asian Englishes pattern relative to British English. Therefore, we will define British English as the baseline for comparison.\n\nWe will use the following specifications:\n\n\n\n\n\n\n\n\nVariable\nFactor Levels\nPreferred Reference level\n\n\n\n\nRegister\nS1A, S1B\nS1A\n\n\nVariety\nGB, SING, HK\nGB\n\n\nPerson\n1, 2, 3, it, there\n3\n\n\nReferentiality\nreferential, non-referential\nreferential\n\n\n\n\n# Store \"Reference\" as factor\ndata_pro$Reference &lt;- as.factor(data_pro$Reference)\n\n## Specify reference level (the 'unmarked' case)\ndata_pro$Reference &lt;- relevel(data_pro$Reference, \"overt\")\n\n## Print levels\nlevels(data_pro$Reference)\n\n[1] \"overt\" \"null\" \n\n\nRepeat the procedure for the remaining categorical variables.\n\n\nCode\n# Store \"Register\" as factor\ndata_pro$Register &lt;- as.factor(data_pro$Register)\n\n## Specify reference level\ndata_pro$Register &lt;- relevel(data_pro$Register, \"S1A\")\n\n# Store \"Variety\" as factor\ndata_pro$Variety &lt;- as.factor(data_pro$Variety)\n\n## Specify reference level\ndata_pro$Variety &lt;- relevel(data_pro$Variety, \"GB\")\n\n# Store \"Person\" as factor\ndata_pro$Person &lt;- as.factor(data_pro$Person)\n\n## Specify reference level\ndata_pro$Person &lt;- relevel(data_pro$Person, \"3\")\n\n# Store \"Referentiality\" as factor\ndata_pro$Referentiality &lt;- as.factor(data_pro$Referentiality)\n\n## Specify reference level\ndata_pro$Referentiality &lt;- relevel(data_pro$Referentiality, \"referential\")\n\n\n\n\nModel fitting\nThere are two functions that can fit logistic models in R: lrm() and glm().\n\n\n\n\n\n\nNote\n\n\n\nThe model formula below does not include Referentiality because several intermediary steps revealed it to be almost completely irrelevant for predicting Reference. In addition, the existing (and significant) interaction Variety:Person has been excluded to improve the interpretability of the model.\n\n\n\n# With lrm(); requires library(\"rms\")\n\n# Fit interaction model\nReference.lrm &lt;- lrm(Reference ~ Register + Variety + Register:Variety + Person, data = data_pro)\n\n# View model statistics\nReference.lrm\n\nLogistic Regression Model\n\nlrm(formula = Reference ~ Register + Variety + Register:Variety + \n    Person, data = data_pro)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs          4838    LR chi2     120.43      R2       0.092    C       0.729    \n overt       4664    d.f.             9     R2(9,4838)0.023    Dxy     0.458    \n null         174    Pr(&gt; chi2) &lt;0.0001    R2(9,503.2)0.199    gamma   0.488    \nmax |deriv| 4e-10                            Brier    0.034    tau-a   0.032    \n\n                            Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept                   -3.4132 0.2746 -12.43 &lt;0.0001 \nRegister=S1B                 0.0269 0.3807   0.07 0.9437  \nVariety=HK                   0.6712 0.3174   2.11 0.0345  \nVariety=SING                 1.1193 0.2959   3.78 0.0002  \nPerson=1                    -0.8807 0.1811  -4.86 &lt;0.0001 \nPerson=2                    -1.6441 0.2695  -6.10 &lt;0.0001 \nPerson=it                    0.7897 0.2978   2.65 0.0080  \nPerson=there                -2.5641 1.0095  -2.54 0.0111  \nRegister=S1B * Variety=HK    0.6035 0.4521   1.34 0.1819  \nRegister=S1B * Variety=SING -0.4753 0.4688  -1.01 0.3107  \n\n\n\n# With (glm); available in base R\n# Note the additional \"family\" argument!\nReference.glm &lt;- glm(Reference ~ Register + Variety + Register:Variety + Person, data = data_pro, family = \"binomial\")\n\n# View model statistics\nsummary(Reference.glm)\n\n\n\n\n\n\n\nStepwise variable selection\n\n\n\nWith the function drop1(), it is possible to successively remove variables from the complex model to ascertain which ones improve the model significantly (i.e., decrease the deviance and AIC scores).\n\ndrop1(Reference.glm, test = \"Chisq\")\n\nSingle term deletions\n\nModel:\nReference ~ Register + Variety + Register:Variety + Person\n                 Df Deviance    AIC    LRT Pr(&gt;Chi)    \n&lt;none&gt;                1378.4 1398.4                    \nPerson            4   1460.2 1472.2 81.828  &lt; 2e-16 ***\nRegister:Variety  2   1387.5 1403.5  9.100  0.01057 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nConfidence intervals and odds ratios\n\n# Tidy the model output\ntidy_model &lt;- tidy(Reference.glm, conf.int = TRUE)\n\n# Remove intercept, compute odds ratios and their CIs\ntidy_model &lt;- tidy_model %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(\n    odds_ratio = exp(estimate),\n    odds.conf.low = exp(conf.low),\n    odds.conf.high = exp(conf.high)\n  )\n\n\n\nVisualisation\n\nPlot model coefficients:\n\n\n\nCode\n# Create the coefficient plot\nggplot(tidy_model, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient Estimate (log-odds)\",\n    y = \"Predictor\",\n    title = \"Coefficient Estimates with Confidence Intervals\",\n    caption = \"*Note that the CIs of singificant predictors do not include 0.\"\n  )\n\n\n\n\n\n\n\n\n\nCode\n# Plot odds ratios\nggplot(tidy_model, aes(x = exp(estimate), y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = odds.conf.low, xmax = odds.conf.high), height = 0.2) +\n  geom_vline(xintercept = 1, linetype = \"dashed\", color = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient Estimate (odds ratios)\",\n    y = \"Predictor\",\n    title = \"Odds ratios with Confidence Intervals\",\n    caption = \"*Note that the CIs of singificant predictors do not include 1.\"\n  )\n\n\n\n\n\n\n\n\n\n\nPlot predicted probabilities:\n\n\n\nCode\n# Use ggeffect() from the ggeffects package\nplot(ggeffect(Reference.glm, terms = c(\"Register\"))) + geom_line()\n\n\n\n\n\n\n\n\n\nCode\nplot(ggeffect(Reference.glm, terms = c(\"Variety\"))) + geom_line()\n\n\n\n\n\n\n\n\n\nCode\nplot(ggeffect(Reference.glm, terms = c(\"Person\"))) + geom_line()\n\n\n\n\n\n\n\n\n\n\n\nInterpret the model\nThe logistic regression model is statistically significant at \\(p &lt; 0.001\\) (\\(\\chi^2 = 120.43\\), \\(df = 9\\)) and has acceptable fit (Nagelkerke’s-\\(R^2\\) = \\(0.09\\), \\(C = 0.73\\)).\nThe model coefficients indicate that null subjects are significantly more likely in Singapore English compared to British English (Estimate = 1.12, 95% CI [0.56, 1.73], \\(p &lt; 0.001\\)). This effect is moderate with an \\(OR\\) of 3.06 (95% CI [1.75, 5.64]), suggesting that the probability of subject omission is elevated by a factor of approximately 3 in the Singaporean variety.\n…\n\n\nFurther model diagnostics\nPost-hoc evaluation of the logistic regression model is just as important as it is for linear regression. However, several assumptions can be relaxed. Although independence of observations, a linear relationship between predictors and response as well as uncorrelated predictors remain essential, the stipulations on the distribution and variance of the residuals may be disregarded.\nMulticollinearity can be inspected via the vif() function from the car package.\n\n\nCode\n# Variable inflation factors further reveal severe multicollinearity\nvif(Reference.lrm)\n\n\n               Register=S1B                  Variety=HK \n                   5.818111                    4.006084 \n               Variety=SING                    Person=1 \n                   3.421687                    1.140407 \n                   Person=2                   Person=it \n                   1.089502                    1.102908 \n               Person=there   Register=S1B * Variety=HK \n                   1.007148                    6.218803 \nRegister=S1B * Variety=SING \n                   3.685075 \n\n\nCross-validating the model is always recommended to assess the model’s ability to generalise beyond the training data and thus prevent overfitting.\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Refit the model with additional settings\nReference.val &lt;- lrm(Reference ~ Register + Variety + Register:Variety + Person, data = data_pro, x = T, y = T)\n\n# Perform 200-fold cross-validation\nmodel.validated &lt;- validate(Reference.val, B = 200)\n\n# Slope optimism should be as low possible!\nmodel.validated\n\n\n          index.orig training    test optimism index.corrected   n\nDxy           0.4592   0.4655  0.4456   0.0200          0.4393 200\nR2            0.0923   0.0981  0.0843   0.0138          0.0785 200\nIntercept     0.0000   0.0000 -0.2177   0.2177         -0.2177 200\nSlope         1.0000   1.0000  0.9262   0.0738          0.9262 200\nEmax          0.0000   0.0000  0.0622   0.0622          0.0622 200\nD             0.0247   0.0263  0.0225   0.0038          0.0208 200\nU            -0.0004  -0.0004  0.0002  -0.0006          0.0002 200\nQ             0.0251   0.0268  0.0223   0.0045          0.0206 200\nB             0.0336   0.0336  0.0337  -0.0001          0.0337 200\ng             1.0081   1.2487  1.1361   0.1127          0.8954 200\ngp            0.0319   0.0326  0.0303   0.0022          0.0297 200",
    "crumbs": [
      "6. Statistical Modelling",
      "6.2 Logistic regression"
    ]
  },
  {
    "objectID": "chapters/01-maths.html",
    "href": "chapters/01-maths.html",
    "title": "1.4 Set theory and mathematical notation",
    "section": "",
    "text": "Throughout this reader—particularly in later chapters—we will frequently rely on fundamental concepts from set theory and mathematics. These frameworks provide a precise and concise way to express complex ideas, making them essential tools for statistical science. The aim of this unit is to introduce key formalisms and build foundational literacy in their use. While a comprehensive treatment is beyond the scope of this reader, the following resources offer further guidance:\n\nPartee, Meulen, and Wall (1990)\nHalmos (1974)\nSpivak (1994)\nHammack (2018)",
    "crumbs": [
      "1. Fundamentals",
      "1.4 Formal aspects"
    ]
  },
  {
    "objectID": "chapters/01-maths.html#introduction",
    "href": "chapters/01-maths.html#introduction",
    "title": "1.4 Set theory and mathematical notation",
    "section": "",
    "text": "Throughout this reader—particularly in later chapters—we will frequently rely on fundamental concepts from set theory and mathematics. These frameworks provide a precise and concise way to express complex ideas, making them essential tools for statistical science. The aim of this unit is to introduce key formalisms and build foundational literacy in their use. While a comprehensive treatment is beyond the scope of this reader, the following resources offer further guidance:\n\nPartee, Meulen, and Wall (1990)\nHalmos (1974)\nSpivak (1994)\nHammack (2018)",
    "crumbs": [
      "1. Fundamentals",
      "1.4 Formal aspects"
    ]
  },
  {
    "objectID": "chapters/01-maths.html#set-theory",
    "href": "chapters/01-maths.html#set-theory",
    "title": "1.4 Set theory and mathematical notation",
    "section": "Set theory",
    "text": "Set theory\nSets are finite or infinite collections of elements. Typically, they are denoted by capital letters (\\(A\\), \\(B\\), etc.). Their elements can virtually anything – letters, numbers, functions, or even more sets. For example, define a set \\(L\\) which contains the words eat, drink, sleep, bathe:1\n1 Note that the order of the elements in a set is irrelevant; even if the elements of \\(L\\) were rearranged, the elements themselves would remain identical: \\(\\{eat, drink, sleep, bathe\\} = \\{drink, bathe, sleep, eat\\}\\).\\[\nL = \\{eat, drink, sleep, bathe\\}.\n\\]\n\n\nShow R code\n# Define L\nL &lt;- c(\"eat\", \"drink\", \"sleep\", \"bathe\") \n\n\nThe cardinality (or ‘size’) of \\(L\\) is 4 because it contains 4 unique elements. This is usually written as \\(\\mid L \\mid = 4\\).\n\n\nShow R code\n# Number of elements in L\nlength(L)\n\n\nWe can characterise the membership of individual elements with regard to a certain set. The element eat is in \\(L\\), i.e., \\(eat \\in L\\). By contrast, convince is not, which is written \\(convince \\notin L\\).\n\n\nShow R code\n# Check element membership\n\"eat\" %in% L\n\n\"convince\" %in% L\n\n\nA set that does not contain any elements is an empty set \\(\\emptyset = \\{\\}\\). As such, it as a subset of every set – even of itself!2\n2 Here is another strange observation: \\(\\emptyset\\) and \\(\\{\\emptyset\\}\\) don’t express the same thing. While \\(\\emptyset\\) contains nothing (\\(\\mid\\emptyset\\mid = 0\\)), \\(\\{ \\emptyset \\}\\) is a set that has one element (\\(\\mid\\{ \\emptyset \\}\\mid = 1\\)), and this one element is actually nothing.\nSubset, union, and intersection\nLet’s define two new sets\n\\[\nG = \\{eat, drink\\}\n\\]\nand\n\\[\nM = \\{eat, run\\}.\n\\]\n\n\nShow R code\n# Define new sets\nG &lt;- c(\"eat\", \"drink\")\n\nM &lt;- c(\"eat\", \"run\")\n\n\nWhen comparing \\(G\\) with \\(L\\), we can see that all of the elements in \\(G\\) are also represented in \\(L\\). This renders \\(G\\) a subset of \\(L\\), written \\(G \\subseteq L\\). However, not all elements of \\(M\\) are also in \\(G\\). Specifically, run is in \\(M\\), but not in \\(F\\); therefore, \\(M \\nsubseteq G\\).\n\n\nShow R code\n# Check if F is a subset of L\nall(F %in% L)\n\n\n# Check if M is a subset of F\nall(M %in% F)\n\n\nIf we are looking for an element that is in \\(M\\) or in \\(L\\) or in both, we are talking about the union \\(M \\cup L\\):\n\\[\nM \\cup L = \\{eat, drink, sleep, bathe, run\\}.\n\\]\n\n\nShow R code\n# Union of M and L\nunion(M, L)\n\n\nBy contrast, if we are interested in the elements that all sets \\(L\\), \\(G\\) and \\(M\\) have in common, we are dealing with their intersection \\(L \\cap G \\cap M\\). In fact, there is only one element that satisfies this condition:\n\\[\nL \\cap G \\cap M = \\{eat\\}.\n\\]\n\n\nShow R code\n# Intersection of two sets\nintersect(L, G)\n\n# Intersection of multiple sets\nReduce(intersect, list(L, G, M))",
    "crumbs": [
      "1. Fundamentals",
      "1.4 Formal aspects"
    ]
  },
  {
    "objectID": "chapters/01-maths.html#large-operators",
    "href": "chapters/01-maths.html#large-operators",
    "title": "1.4 Set theory and mathematical notation",
    "section": "Large operators",
    "text": "Large operators\n\nSums and products\nLong summation expressions tend to be rather unwieldy. For example, if we are taking the sum of all natural numbers from 1 up to 15, writing this out would take up a lot of space (and time):\n\\[\n1+2+3+4+5+6+7+8+9+10+11+12+13+14+15.\n\\] For this very reason, mathematicians have introduced a more compact notation that only takes a fraction of the time to write while communicating the exact same idea. Using the Greek letter \\(\\Sigma\\) (‘sigma’), we could rewrite it as\n\\[\n\\sum_{i=1}^{15} i.\n\\] This notation can be read as the sum from \\(i = 1\\) to \\(15\\). It is the sum of the numbers obtained by letting \\(i = 1, 2, \\dots, 15\\).\n\n\nShow R code\n# Sum of the number from 1 to 15\nsum(1:15)\n\n\nThe same principle applies if the terms of the sum are letters. If we have\n\\[\na_1 + a_2 + a_3 + a_4 + a_5,\n\\] then we can define a sum where the index \\(i\\) iterates through the elements in the set \\(\\{1, 2, 3, 4, 5\\}\\). The letter \\(a\\) remains constant.\n\\[\n\\sum_{i=1}^5 a_i\n\\] Alternatively, to make it even more explicit, we could write\n\\[\n\\sum_{i\\in \\{1, 2, 3, 4, 5\\}} a_i.\n\\]\nThe index can be denoted by any letter (\\(j\\), \\(k\\) etc.) but not \\(n\\), which is reserved for the upper limit:\n\\[\n\\sum_{i=1}^n a_i = a_1 + a_2 + a_3 + \\dots + a_n.\n\\]\nWe can apply the same reasoning to other operations such as products. The only difference is that we’d have to use \\(\\Pi\\) (‘pi’) instead of \\(\\Sigma\\). If we are taking the product of all natural numbers from 5 to 10, we’d write\n\\[\n\\prod_{i=5}^{10} i = 5 \\cdot 6 \\cdot 7 \\cdot 8 \\cdot 9 \\cdot 10.\n\\]\n\n\nShow R code\n# Product of the numbers from 5 to 10\nprod(5:10)\n\n\nThe set of all values that the index below the operator can take on is known as the index set. It can be defined as one sees fit. Say, we need the product of all numbers in \\(K = \\{1, 3, 5, 7\\}\\). Then the corresponding notation would be\n\\[\n\\prod_{i\\in K} i = 1 \\cdot 3 \\cdot 5 \\cdot 7.\n\\]\n\n\nUnion and intersection over indexed sets\nSo far, we’ve seen how the large-operator symbols \\(\\Sigma\\) and \\(\\Pi\\) can express sums and products over index ranges or sets. The same notation style can also be used for set operations, such as unions and intersections across a large number of sets.\nAssume we have indexed sets \\(A_1, A_2, \\dots, A_n\\). Instead of writing out the full union\n\\[\nA_1 \\cup A_2 \\cup A_3 \\cup  \\dots \\cup A_n,\n\\] we can use a large union symbol to express this more compactly:\n\\[\n\\bigcup_{i=1}^n A_i.\n\\]\n\n\nShow R code\n# Define multiple sets\nA1 &lt;- c(\"a\", \"b\", \"c\")\nA2 &lt;- c(\"a\", \"e\", \"f\")\nA3 &lt;- c(\"a\", \"h\", \"i\")\nA4 &lt;- c(\"a\", \"k\", \"l\")\n\n# Union of all sets\nReduce(union, list(A1, A2, A3, A4))\n\n\nSimilarly, if we’re interested in the elements common to all sets in the sequence—their intersection—we write:\n\\[\n\\bigcap_{i=1}^n A_i = A_1 \\cap A_2 \\cap A_3 \\cap \\dots \\cap A_n.\n\\]\n\n\nShow R code\n# Intersection of all sets\nReduce(intersect, list(A1, A2, A3, A4))",
    "crumbs": [
      "1. Fundamentals",
      "1.4 Formal aspects"
    ]
  },
  {
    "objectID": "chapters/05-chi_square_test.html",
    "href": "chapters/05-chi_square_test.html",
    "title": "4.6 Chi-squared test",
    "section": "",
    "text": "For linguists:\n\nGries (2021): Chapter 4.1.2.1\n\nGeneral:\n\nBaguley (2012): Chapter 4\nAgresti and Kateri (2022): Chapter 5",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.6 Chi-squared test"
    ]
  },
  {
    "objectID": "chapters/05-chi_square_test.html#suggested-reading",
    "href": "chapters/05-chi_square_test.html#suggested-reading",
    "title": "4.6 Chi-squared test",
    "section": "",
    "text": "For linguists:\n\nGries (2021): Chapter 4.1.2.1\n\nGeneral:\n\nBaguley (2012): Chapter 4\nAgresti and Kateri (2022): Chapter 5",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.6 Chi-squared test"
    ]
  },
  {
    "objectID": "chapters/05-chi_square_test.html#preparation",
    "href": "chapters/05-chi_square_test.html#preparation",
    "title": "4.6 Chi-squared test",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nScript\n\n\n\nYou can find the full R script associated with this unit here.\n\n\n\n#  Load libraries\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(confintr) # for effect size calculation\n\n# Load data\ngenitive &lt;- read.csv(\"Grafmiller_genitive_alternation.csv\", sep = \"\\t\")",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.6 Chi-squared test"
    ]
  },
  {
    "objectID": "chapters/05-chi_square_test.html#the-pearson-chi2-test-of-independence",
    "href": "chapters/05-chi_square_test.html#the-pearson-chi2-test-of-independence",
    "title": "4.6 Chi-squared test",
    "section": "The Pearson \\(\\chi^2\\)-test of independence",
    "text": "The Pearson \\(\\chi^2\\)-test of independence\nThe first step of any significance test involves setting up the null and alternative hypothesis. In this unit, we will perform a sample analysis on the relationship between genitive Type and text Possessor Animacy. Specifically, we will focus on the independence of these two discrete variables (i.e, the presence or absence of correlation).\n\n\\(H_0:\\) The variables Type and Possessor Animacy are independent.\n\\(H_1:\\) The variables Type and Possessor Animacy are not independent.\n\nNext, we compute a test statistic that indicates how strongly our data conforms to proportions expected under \\(H_0\\). To this end, we will need two types of values:\n\nthe observed frequencies \\(f_{ij}\\) present in our data set\nthe expected frequencies \\(e_{ij}\\), which we would expect to see if \\(H_0\\) were true,\n\nwhere \\(f, e \\in \\mathbb{N}\\). The indices \\(i\\) and \\(j\\) uniquely identify the cell counts in all column-row combinations of a contingency table.\n\nObserverd frequencies\nThe table below represents a generic contingency table where \\(Y\\) and \\(X\\) are categorical variables and have the values \\(Y = \\{y_1, y_2, \\dots, y_i \\}\\) and \\(X = \\{x_1, x_2, \\dots, x_j\\}\\). In the table, each cell indicates the count of observation \\(f_{ij}\\) corresponding to the \\(i\\)-th row and \\(j\\)-th column.\n\n\n\n\n\n\n\\(X\\)\n\n\n\n\n\n\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n…\n\\(x_j\\)\n\n\n\n\n\\(y_1\\)\n\\(f_{11}\\)\n\\(f_{12}\\)\n…\n\\(f_{1j}\\)\n\n\n\n\n\\(y_2\\)\n\\(f_{21}\\)\n\\(f_{22}\\)\n…\n\\(f_{2j}\\)\n\n\n\n\\(Y\\)\n…\n…\n…\n…\n…\n\n\n\n\n\\(y_i\\)\n\\(f_{i1}\\)\n\\(f_{i2}\\)\n…\n\\(f_{ij}\\)\n\n\n\n\n\nIn the genitive data, the observed frequencies correspond to how often each Type value (i.e., s and of) is attested for a given Possessor Animacy group (i.e., animate and inanimate). These can be computed in a very straightforward fashion by applying R’s table() function to the variables of interest.\n\nobserved_freqs &lt;- table(genitive$Type, genitive$Possessor_Animacy2)\n\nprint(observed_freqs)\n\n    \n     animate inanimate\n  of     592      2511\n  s     1451       544\n\n\n\n\nExpected frequencies\nThe expected frequencies require a few additional steps. Usually, these steps are performed automatically when conducting the chi-squared test in R, so you don’t have to worry about calculating them by hand. We will do it anyway to drive home the rationale of the test.\n\n\n\n\n\n\n\\(X\\)\n\n\n\n\n\n\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n…\n\\(x_j\\)\n\n\n\n\n\\(y_1\\)\n\\(e_{11}\\)\n\\(e_{12}\\)\n…\n\\(e_{1j}\\)\n\n\n\n\n\\(y_2\\)\n\\(e_{21}\\)\n\\(e_{22}\\)\n…\n\\(e_{2j}\\)\n\n\n\n\\(Y\\)\n…\n…\n…\n…\n…\n\n\n\n\n\\(y_i\\)\n\\(e_{i1}\\)\n\\(e_{i2}\\)\n…\n\\(e_{ij}\\)\n\n\n\n\n\nThe expected frequencies \\(e_{ij}\\) are given by the formula in Equation 1. In concrete terms, we go through each cell in the cross-table and multiply the corresponding row sums with the column sums, dividing the result by the total number of occurrences in the sample. For example, there are \\(592\\) occurrences of of genitives in the animate group. The row sum is \\(592+ 2511 = 3103\\) and the column sum is \\(592 + 1451 = 2043\\). Next, we take their product \\(3103 \\cdot 2043\\) and divide it by the total number of observations, which is \\(592 + 2511 + 1451 + 544 = 5098\\). Thus we obtain an expected frequency of \\(\\frac{3103 \\times 2043}{5098} \\approx 1243\\) under the null hypothesis.\n\\[\ne_{ij} = \\frac{i\\textrm{th row sum} \\times j \\textrm{th column sum}}{\\textrm{sample size}}\n\\tag{1}\\]\nThe expected frequencies for our combination of variables is shown below. In which cells can you see the greatest deviations between observed and expected frequencies?\n\n\nShow the code\n## Calculate row totals\nrow_totals &lt;- rowSums(observed_freqs)\n\n## Calculate column totals\ncol_totals &lt;- colSums(observed_freqs)\n\n## Total number of observations\ntotal_obs &lt;- sum(observed_freqs)\n\n## Calculate expected frequencies\nexpected_freqs &lt;- outer(row_totals, col_totals) / total_obs\n\nprint(expected_freqs)\n\n\n     animate inanimate\nof 1243.5129  1859.487\ns   799.4871  1195.513\n\n\n\n\nConducting the test\nThe \\(\\chi^2\\)-test now offers a convenient way of quantifying the differences between the two tables above. It measures how much the observed frequencies deviate from the expected frequencies for each cell in a contingency table (cf. Heumann, Schomaker, and Shalabh 2022: 249-251). The gist of this procedure is summarised in Equation 2.\n\\[\n\\text{Chi-squared } \\chi^2 =\\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\n\\tag{2}\\]\nLet \\(I\\) denote the total number of rows and \\(J\\) the total number of columns, with degress of freedom equal to \\((I-1) \\cdot (J-1)\\). We compute the joint squared deviations between \\(f_{ij}\\) and \\(e_{ij}\\) for every row-column combination:\n\\[\n\\chi^2 = \\sum_{i=1}^{I}\\sum_{j=1}^{J}{\\frac{(f_{ij} - e_{ij})^2}{e_{ij}}}\n\\tag{3}\\]\nA visual representation is given in the table below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\)\n\n\n\n\n\n\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n…\n\\(x_j\\)\n\n\n\n\n\\(y_1\\)\n\\(\\frac{(f_{11} - e_{11})^2}{e_{11}}\\)\n\\(\\frac{(f_{12} - e_{12})^2}{e_{12}}\\)\n…\n\\(\\frac{(f_{1j} - e_{1j})^2}{e_{1j}}\\)\n\n\n\n\n\\(y_2\\)\n\\(\\frac{(f_{21} - e_{21})^2}{e_{21}}\\)\n\\(\\frac{(f_{22} - e_{22})^2}{e_{22}}\\)\n…\n\\(\\frac{(f_{2j} - e_{2j})^2}{e_{2j}}\\)\n\n\n\n\\(Y\\)\n…\n…\n…\n…\n…\n\n\n\n\n\\(y_i\\)\n\\(\\frac{(f_{i1} - e_{i1})^2}{e_{i1}}\\)\n\\(\\frac{(f_{i2} - e_{i1})^2}{e_{i2}}\\)\n…\n\\(\\frac{(f_{ij} - e_{ij})^2}{e_{ij}}\\)\n\n\n\n\nThe implementation in R is a simple one-liner. Keep in mind that we have to supply absolute frequencies to chisq.test() rather than percentages.\n\nfreqs_test &lt;- chisq.test(observed_freqs)\n\nprint(freqs_test)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  observed_freqs\nX-squared = 1453.4, df = 1, p-value &lt; 2.2e-16\n\n\nQuite conveniently, the test object freqs_test stores the expected frequencies, which can be easily accessed via subsetting. Luckily, they are identical to what we calculated above!\n\nfreqs_test$expected\n\n    \n       animate inanimate\n  of 1243.5129  1859.487\n  s   799.4871  1195.513\n\n\n\n\nAssumptions of the chi-squared test\nThis \\(\\chi^2\\)-test comes with certain statistical assumptions. Violations of these assumptions decrease the validity of the result and could, therefore, lead to wrong conclusions about relationships in the data. In this case, other tests should be consulted.\n\n\n\n\n\n\nImportant\n\n\n\n\nAll observations are independent of each other.\n80% of the expected frequencies are \\(\\geq\\) 5.\nAll observed frequencies are \\(\\geq\\) 1.\n\n\n\nDependent observations (e.g., multiple measurements per participant) are a common problem of linguistic data and should always be controlled for. The gold standard are hierarchical (multilevel) models which respect grouping structures.\nIf the (expected) frequencies are low, it is recommended to use a more robust test such as the Fisher’s Exact Test or the log-likelihood ratio test (\\(G\\)-test).\n\n\n\n\n\n\nFisher’s Exact Test\n\n\n\n\n\nWhile the \\(\\chi^2\\)-test can only approximate the \\(p\\)-value, Fisher’s Exact Test can provide an exact solution. Note that for anything more complex than a \\(2 \\times 2\\) table, it becomes considerably more computationally expensive; if it takes too long, set simulate.p.value = TRUE.\nDrawing on the hypergeometric distribution (see ?dhyper()), it computes the probability of all frequency tables that are equal or more extreme than the one observed.\n\nfisher.test(observed_freqs)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  observed_freqs\np-value &lt; 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.07722662 0.10118258\nsample estimates:\nodds ratio \n0.08841846 \n\n\n\n\n\n\n\n\n\n\n\n\\(G\\)-test\n\n\n\n\n\nThe \\(G^2\\)-statistic is analogous to \\(\\chi^2\\), but it tends to be more robust for lower observed counts. It is defined as\n\\[\nG^2 = 2\\sum_{i=1}^{I}\\sum_{j=1}^{J} f_{ij} \\ln\\left({\\frac{f_{ij}}{e_{ij}}}\\right)\n\\tag{4}\\]\nand implemented in R via the DescTools package.\n\n# Load library (install if necessary)\nlibrary(DescTools)\n\n# Perform G-test (preferably for tables with more than 2 rows/columns)\nGTest(observed_freqs)\n\n\n    Log likelihood ratio (G-test) test of independence without correction\n\ndata:  observed_freqs\nG = 1502.8, X-squared df = 1, p-value &lt; 2.2e-16\n\n\n\n\n\n\n\nHow do I make sense of the test results?\nThe test output has three ‘ingredients’:\n\nthe chi-squared score (X-squared)\nthe degrees of freedom (df)\nthe p-value.\n\nIt is absolutely essential to report all three of those as they determine each other. Here a few possible wordings that could be used:\n\nAccording to a \\(\\chi^2\\)-test, there is a highly significant association between genitive type and possessor animacy at \\(p &lt; 0.001\\) (\\(\\chi^2 = 106.44, df = 1\\)), thus justifying the rejection of \\(H_0\\).\n\n\nA \\(\\chi^2\\)-test revealed a highly significant association between genitive type and possessor animacy (\\(\\chi^2(1) = 106.44\\), \\(p &lt; 0.001\\)), supporting the rejection of \\(H_0\\).\n\n\nThe \\(\\chi^2\\)-test results (\\(\\chi^2 = 106.44\\), \\(df = 1\\), \\(p &lt; 0.001\\)) provide strong evidence against the null hypothesis, demonstrating a significant association between genitive type and possessor animacy.\n\nOn the whole, the test results suggest that the dependent variable Type and the explanatory variable Possessor Animacy are not independent of each other. The probability of randomly observing such a distribution under the assumption of no effect is lower than 0.05 (actually, it is \\(&lt; 2.2 \\cdot 10^{-16}\\)), which is sufficient to reject the null hypothesis.\nWe can infer that a speaker’s choice of clause Type is very likely influenced by the animacy of the possessor; in other words, these two variables are correlated. However, there are still several things the test does not tell us:\n\nAre there certain variable combinations where the \\(\\chi^2\\)-scores are particularly high?\nHow strongly do Type and Possessor Animacy influence each other?",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.6 Chi-squared test"
    ]
  },
  {
    "objectID": "chapters/05-chi_square_test.html#pearson-residuals",
    "href": "chapters/05-chi_square_test.html#pearson-residuals",
    "title": "4.6 Chi-squared test",
    "section": "Pearson residuals",
    "text": "Pearson residuals\nIf we’re interested in what cells show the greatest difference between observed and expected frequencies, an option would be to inspect the Pearson residuals (cf. Equation 5).\n\\[ \\text{residuals} = \\frac{\\text{observed} - \\text{expected}}{\\sqrt{\\text{expected}}}\n\\tag{5}\\]\nThese can be accessed via the test results stored freqs_test.\n\nfreqs_test$residuals\n\n    \n       animate inanimate\n  of -18.47557  15.10868\n  s   23.04185 -18.84282\n\n\nThe function assocplot() can automatically compute the pearson residuals for any given contingency table and create a plot that highlights their contributions. If the bar is above the dashed line, it is black and indicates that a category is observed more frequently than expected (e.g., of genitives in the inanimate group). Conversely, bars are coloured grey if a category is considerably less frequent than expected, such as s-genitives with inanimate possessors.\n\nassocplot(t(observed_freqs), col = c(\"black\", \"lightgrey\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting the residuals: Configural Frequency Analysis\n\n\n\n\n\nThe chi-squared test only provides a \\(p\\)-value for the entire contingency table. But what if we wanted to test the residuals for their significance as well? Configural Frequency Analysis (Krauth and Lienert 1973) allows us to do exactly that: It performs a significance test for all combinations of variable values in a cross-table. Moreover, CFA is not limited to two variables only. Technically, users can test for associations between arbitrary numbers of variables, but should be aware of the increasing complexity of interpretation.\n\nlibrary(cfa) # install library beforehand\n\n# Get the observed counts and convert them to a data frame\nconfig_df &lt;- as.data.frame(observed_freqs)\n\n# Convert to matrix\nconfigs &lt;- as.matrix(config_df[, 1:2])  # first two columns contain the configurations (= combinations of variable values)\ncounts &lt;- config_df$Freq # Freq column contains the corresponding counts\n\n# Perform CFA on configuarations and counts; apply Bonferroni correction for multiple testing\ncfa_output &lt;- cfa(configs, counts, bonferroni = TRUE)\n\n# Print output\nprint(cfa_output)\n\n\n*** Analysis of configuration frequencies (CFA) ***\n\n         label    n  expected         Q    chisq p.chisq sig.chisq         z\n1    s animate 1451  799.4871 0.1515671 530.9268       0      TRUE  25.09332\n2  s inanimate  544 1195.5129 0.1669481 355.0519       0      TRUE -21.53650\n3   of animate  592 1243.5129 0.1690271 341.3468       0      TRUE -21.24783\n4 of inanimate 2511 1859.4871 0.2011766 228.2722       0      TRUE  18.95630\n  p.z sig.z\n1   0  TRUE\n2   1  TRUE\n3   1  TRUE\n4   0  TRUE\n\n\nSummary statistics:\n\nTotal Chi squared         =  1455.598 \nTotal degrees of freedom  =  1 \np                         =  0 \nSum of counts             =  5098 \n\nLevels:\n\nVar1 Var2 \n   2    2",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.6 Chi-squared test"
    ]
  },
  {
    "objectID": "chapters/05-chi_square_test.html#effect-size",
    "href": "chapters/05-chi_square_test.html#effect-size",
    "title": "4.6 Chi-squared test",
    "section": "Effect size",
    "text": "Effect size\nThe \\(p\\)-value only indicates the presence of correlation, but not its strength – regardless of how low it may be. It does not convey how much two variables determine each other. For this reason, it is highly recommended to report an effect size measure alongside the \\(p\\)-value. One such measure is Cramér’s \\(V\\), which takes values in the interval \\([0, 1]\\):\n\\[\nV = \\sqrt{\\frac{\\chi^2}{n \\times (min(nrow, ncol) - 1)}}.\n\\tag{6}\\]\nThe package confintr implements this in its cramersv() function:\n\ncramersv(freqs_test)\n\n[1] 0.5339337\n\n\nThe association between two categorical variables is stronger, the closer \\(V\\) approximates 1. Conversely, if \\(V = 0\\), then the variables are completely independent. There are various guidelines in the literature that provide thresholds for “small”, “moderate” and “large” effects, yet these are rarely justified on theoretical grounds and could be viewed as arbitrary.",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.6 Chi-squared test"
    ]
  },
  {
    "objectID": "chapters/05-chi_square_test.html#only-one-variable-the-pearson-chi2-goodness-of-fit-test",
    "href": "chapters/05-chi_square_test.html#only-one-variable-the-pearson-chi2-goodness-of-fit-test",
    "title": "4.6 Chi-squared test",
    "section": "Only one variable? The Pearson \\(\\chi^2\\) goodness-of-fit test",
    "text": "Only one variable? The Pearson \\(\\chi^2\\) goodness-of-fit test\nThe \\(\\chi^2\\)-statistic can also be utilised in simpler scenarios that involve only one single categorical variable.1 Consider the small eat_obj_aspect.xlsx dataset, where the object_realisation column indicates whether the object was realised or dropped in a given observation.\n1 For this reason, the distinction between dependent and independent variable is irrelevant here.\n# Load data\neat &lt;- read_xlsx(\"../datasets/eat_obj_aspect.xlsx\")\n\n# Show object realisation pattern\neat_observed &lt;- table(eat$object_realisation)\n\nprint(eat_observed)\n\n\n no yes \n 57  45 \n\n\nWe can use the \\(\\chi^2\\) goddness-of-fit test “to compare an observed frequency distribution against its expected probability of occurrence” (Baguley 2012: 132). In other words, we can check if the observed object data matches the frequencies we’d expect to see if both outcomes of object realisation were equally likely and hence randomly distributed.\nOur hypotheses are thus\n\n\\(H_0:\\) Observed frequencies of object realisation \\(=\\) expected frequencies of object realisation\n\\(H_1:\\) Observed frequencies of object realisation \\(\\neq\\) expected frequencies of object realisation\n\nThe expected frequencies are given by\n\\[\ne_i = \\frac{\\text{number of observations}}{\\text{number of cells}},\n\\tag{9}\\]\nand the test statistic simplifies to\n\\[\n\\chi^2 = \\sum_{i=1}^{I}{\\frac{(f_{i} - e_{i})^2}{e_{i}}}.\n\\tag{10}\\]\nIn R:\n\n# Perform the chi-squared goodness-of-fit test\neat_gof &lt;-  chisq.test(eat_observed)\n\nprint(eat_gof)\n\n\n    Chi-squared test for given probabilities\n\ndata:  eat_observed\nX-squared = 1.4118, df = 1, p-value = 0.2348\n\n# Check expected frequencies (should be greater or equal to 5)\nprint(eat_gof$expected)\n\n no yes \n 51  51 \n\n\n\nA chi-squared goodness-of-fit test indicates that the distribution of object realisation for eat does not significantly differ from an equal distribution (\\(\\chi^2\\) = 1.41, \\(df\\) = 1, \\(p\\) = 0.23). In the sample at hand, the verb eat does not seem to prefer one variant or the other.",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.6 Chi-squared test"
    ]
  },
  {
    "objectID": "chapters/05-chi_square_test.html#exercises",
    "href": "chapters/05-chi_square_test.html#exercises",
    "title": "4.6 Chi-squared test",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1 Schröter & Kortmann (2016) investigate the relationship between subject realisation (overt vs. null) and the grammatical category Person (1.p. vs. 2.p. vs. 3.p.) in three varieties of English (Great Britain vs. Hong Kong vs. Singapore). They report the following test results (2016: 235):\n\nChi-square test results: \\[\n\\begin{align}\n\\text{Singapore: \\quad} & \\chi^2 = 3.3245, df = 2, p = 0.1897 \\\\\n\\text{Hong Kong: \\quad} & \\chi^2 = 40.799, df = 2, p &lt; 0.01 \\\\\n\\text{Great Britain: \\quad} & \\chi^2 = 3.6183, df = 2, p = 0.1638 \\\\\n\\end{align}\n\\]\n\n\nWhat hypotheses are the authors testing?\nAssuming a significance level \\(\\alpha = 0.05\\), what statistical conclusions can be drawn from the test results?\nWhat could be the theoretical implications of these results?\n\n\n\nExercise 2 Conduct a small-scale analysis of genitive Type by Genre (~ 1 page). Structure your write-up as follows:\n\nResearch question and hypotheses\nStatistical methods\nResults (frequency tables, plots, test results, effect size)\nInterpretation of standardised residuals\n(Optional: Interpretation of configural frequency analysis)\nBrief theoretical assessment\nConclusion",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.6 Chi-squared test"
    ]
  },
  {
    "objectID": "chapters/07-EFA.html",
    "href": "chapters/07-EFA.html",
    "title": "7.4 Exploratory Factor Analysis",
    "section": "",
    "text": "For linguists:\n\nLevshina (2015: Chapter 18)\n\nGeneral:\n\nMair (2018: Chapter 2)",
    "crumbs": [
      "7. Machine Learning",
      "7.4 EFA"
    ]
  },
  {
    "objectID": "chapters/07-EFA.html#recommended-reading",
    "href": "chapters/07-EFA.html#recommended-reading",
    "title": "7.4 Exploratory Factor Analysis",
    "section": "",
    "text": "For linguists:\n\nLevshina (2015: Chapter 18)\n\nGeneral:\n\nMair (2018: Chapter 2)",
    "crumbs": [
      "7. Machine Learning",
      "7.4 EFA"
    ]
  },
  {
    "objectID": "chapters/07-EFA.html#preparation",
    "href": "chapters/07-EFA.html#preparation",
    "title": "7.4 Exploratory Factor Analysis",
    "section": "Preparation",
    "text": "Preparation\nThe implementation of Exploratory Factor Analysis in R is very similar to that of Principle Components Analysis. To highlight these similarities, we will use the same libraries (most importantly psych) and the same dataset scope_sem_sub as in the unit on PCA (see ?@sec-pca-prep for further details).\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(purrr)\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\nlibrary(GPArotation)\n\n\nAttaching package: 'GPArotation'\n\nThe following objects are masked from 'package:psych':\n\n    equamax, varimin\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n# Load data\nscope_sem_df &lt;- readRDS(\"../datasets/scope_sem.RDS\")\n\n# Select subset\nscope_sem_sub &lt;- scope_sem_df[,1:11]\n\n# Overview\nglimpse(scope_sem_sub)\n\nRows: 1,702\nColumns: 11\n$ Verb               &lt;chr&gt; \"abstain\", \"abstract\", \"abuse\", \"accelerate\", \"acce…\n$ Resnik_strength    &lt;dbl&gt; 0.40909889, 0.18206692, 0.12473608, -0.76972217, -1…\n$ Conc_Brys          &lt;dbl&gt; -0.94444378, -1.92983639, -0.59478833, 0.22107437, …\n$ Nsenses_WordNet    &lt;dbl&gt; -0.68843996, 0.27755219, 0.00155443, -0.68843996, 0…\n$ Nmeanings_Websters &lt;dbl&gt; -0.95559835, 0.73781281, 0.73781281, -0.27823388, 0…\n$ Visual_Lanc        &lt;dbl&gt; -2.2545455, 0.6103733, 1.3354358, -0.4342084, -0.34…\n$ Auditory_Lanc      &lt;dbl&gt; -0.84225787, -0.35605108, 1.54797548, 0.18795651, 1…\n$ Haptic_Lanc        &lt;dbl&gt; -0.75523987, -0.29089287, 1.25099360, -0.18911818, …\n$ Olfactory_Lanc     &lt;dbl&gt; -0.14444936, -0.37350419, -0.53335522, -0.37350419,…\n$ Gustatory_Lanc     &lt;dbl&gt; 0.27698988, -0.10105698, -0.36148925, -0.52110903, …\n$ Interoceptive_Lanc &lt;dbl&gt; 1.08153427, -0.06560311, 1.64313895, 1.45452985, 0.…",
    "crumbs": [
      "7. Machine Learning",
      "7.4 EFA"
    ]
  },
  {
    "objectID": "chapters/07-EFA.html#exploratory-factor-analysis-vs.-pca",
    "href": "chapters/07-EFA.html#exploratory-factor-analysis-vs.-pca",
    "title": "7.4 Exploratory Factor Analysis",
    "section": "Exploratory Factor Analysis vs. PCA",
    "text": "Exploratory Factor Analysis vs. PCA\nExploratory Factor Analysis (EFA) is quite similar to PCA in that it compresses the high-dimensional feature space, yet the core idea is not to capture as much variance as possible with as few variables as possible, but rather reveal latent (= invisible) variables, i.e., factors.\nThe computation bears some resemblance to that of PCA, with the main difference being that an observation \\(x_m\\) is assumed to be generated by combinations of factor loadings \\(\\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{mp}\\) with the underlying factors \\(\\xi_{1}, \\xi_{2}, \\dots, \\xi_{p}\\) (see Equation 1). Everything to the right of the equation can only be obtained by running estimation procedures such as Principle Axis Factoring or Maximum Likelihood Estimation.\n\\[\nx_1 = \\lambda_{11}\\xi_{11} + \\lambda_{12}\\xi_{12} + \\dots + \\lambda_{1p}\\xi_{p} + \\epsilon_1\n\\tag{1}\\]\nWhen retrieving PCA and EFA loadings, several interpretive differences must be kept in mind:\n\n\n\n\n\n\nKey differences between EFA and PCA\n\n\n\n\nPCA: PCA weights can be conceptualised as “directions in feature space along which the data vary the most” (James et al. 2021: 503) and are analogous to regression slopes. Features with similar loadings on a given PC will be very close to each other in a biplot and could be understood as correlated with each other.\nEFA: The factor loadings in an EFA, on the other hand, directly indicate how strong a factor is correlated with an existing independent variable in the dataset. As such, they help identify and interpret the underlying constructs that have given rise to the data. We can think of EFA loadings as regression coefficients and correlation coefficients at the same time.",
    "crumbs": [
      "7. Machine Learning",
      "7.4 EFA"
    ]
  },
  {
    "objectID": "chapters/07-EFA.html#application-in-r",
    "href": "chapters/07-EFA.html#application-in-r",
    "title": "7.4 Exploratory Factor Analysis",
    "section": "Application in R",
    "text": "Application in R\nWe use our insights from the PCA analysis, according to which three latent variables are enough to capture the bulk of variance in the dataset. When fitting an EFA model, principle axis factoring is the default solution, but could also be changed to fm = \"ml\" to perform Maximum Likelihood Estimation.\n\nefa1 &lt;- fa(scope_sem_sub[,-1], nfactors = 3, rotate = \"none\", fm = \"pa\")\n\nThe remaining printing and plotting methods are identical to PCA.\n\nPrint loadings:\n\n\nloadings(efa1)\n\n\nLoadings:\n                   PA1    PA2    PA3   \nResnik_strength            0.419 -0.269\nConc_Brys           0.800  0.228 -0.300\nNsenses_WordNet     0.560 -0.628  0.236\nNmeanings_Websters  0.495 -0.555  0.233\nVisual_Lanc         0.576  0.141 -0.255\nAuditory_Lanc      -0.270 -0.132  0.153\nHaptic_Lanc         0.608  0.123       \nOlfactory_Lanc      0.291  0.482  0.441\nGustatory_Lanc      0.263  0.513  0.657\nInteroceptive_Lanc -0.245         0.386\n\n                 PA1   PA2   PA3\nSS loadings    2.196 1.482 1.143\nProportion Var 0.220 0.148 0.114\nCumulative Var 0.220 0.368 0.482\n\n\n\nPlot loadings:\n\n\nplot(efa1, labels = colnames(scope_sem_sub[,-1]), main = NA)\n\n\n\n\n\n\n\n\n\nPlot PA scores and loadings:\n\n\nbiplot(efa1, choose = c(1, 2), main = NA,\n       pch = 20, col = c(\"darkgrey\", \"blue\"))\n\n\n\n\n\n\n\nbiplot(efa1, choose = c(2, 3), main = NA,\n       pch = 20, col = c(\"darkgrey\", \"blue\"))\n\n\n\n\n\n\n\n\n\nRotation\nFactors are typically rotated in order to aid in their interpretation, resulting in much clearer loading patterns. Varimax rotation is the default technique and does not affect the model fit (i.e., there is no loss in explained variance; for details see (Mair 2018: 26-29).1\n1 Varimax is a so-called orthogonal rotation technique and, therefore, does not introduce correlations between the factors. If correlated factors are explicitly desired, oblique rotations such as oblimin and promax provide apt alternatives (Mair 2018: 27).\nefa2 &lt;- fa(scope_sem_sub[,-1], nfactors = 3, rotate = \"Varimax\", fm = \"pa\")\n\nloadings(efa2)\n\n\nLoadings:\n                   PA1    PA2    PA3   \nResnik_strength     0.188 -0.470       \nConc_Brys           0.868  0.130  0.107\nNsenses_WordNet     0.145  0.861       \nNmeanings_Websters  0.115  0.771       \nVisual_Lanc         0.638              \nAuditory_Lanc      -0.336              \nHaptic_Lanc         0.572  0.192  0.165\nOlfactory_Lanc      0.164         0.694\nGustatory_Lanc                    0.873\nInteroceptive_Lanc -0.410         0.199\n\n                 PA1   PA2   PA3\nSS loadings    1.868 1.627 1.326\nProportion Var 0.187 0.163 0.133\nCumulative Var 0.187 0.350 0.482\n\n\nThe rotated EFA object paints a picture that is very similar to the PCA result from the previous unit.\n\ndiagram(efa2, main = NA)\n\n\n\n\n\n\n\n\n\nbiplot(efa2, choose = c(1, 2), main = NA,\n       pch = 20, col = c(\"darkgrey\", \"blue\"))\n\n\n\n\n\n\n\nbiplot(efa2, choose = c(2, 3), main = NA,\n       pch = 20, col = c(\"darkgrey\", \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the EFA output\n\n\n\n\nPerception: The first principle axis is once more loaded heavily (and positively) by increasing concreteness scores in addition to higher visual and haptic ratings. Moreover, they display strong linear relationships. The negative association with interoceptive ratings suggests that referents that tend be perceived directly with their senses (concreteness) do not tend to be perceived inside their body.\nSenses: In PA2 we find the inverse pattern of PC2 – very strong positive correlations with sense-related features and a weaker, yet notable negative correlation with selectional preference strength. If a verb has more senses, it tends to carry less information about its context.\nIngestion: Interoceptive ratings are no longer part of the picture, thus giving way to the gustatory and olfactory perception of referents.",
    "crumbs": [
      "7. Machine Learning",
      "7.4 EFA"
    ]
  },
  {
    "objectID": "sections/01-fundamentals.html",
    "href": "sections/01-fundamentals.html",
    "title": "1. Fundamentals",
    "section": "",
    "text": "This section covers the fundamental concepts needed to understand statistics for corpus linguistics.\n\n\n\n1.1 Basics\n1.2 Research Questions\n1.3 Linguistic Variables\n1.4 Formal aspects"
  },
  {
    "objectID": "sections/01-fundamentals.html#chapters",
    "href": "sections/01-fundamentals.html#chapters",
    "title": "1. Fundamentals",
    "section": "",
    "text": "1.1 Basics\n1.2 Research Questions\n1.3 Linguistic Variables\n1.4 Formal aspects"
  },
  {
    "objectID": "chapters/04-probability.html",
    "href": "chapters/04-probability.html",
    "title": "4.2 Probability theory",
    "section": "",
    "text": "Most accessible:\n\nBaguley (2012): Chapter 2\n\nTechnical:\n\nHeumann, Schomaker, and Shalabh (2022a): Chapter 7\nAgresti and Kateri (2022): Chapter 2\n\nProof-based:\n\nCasella and Berger (2002): Chapters 1 & 2",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.2 Probability theory"
    ]
  },
  {
    "objectID": "chapters/04-probability.html#recommended-reading",
    "href": "chapters/04-probability.html#recommended-reading",
    "title": "4.2 Probability theory",
    "section": "",
    "text": "Most accessible:\n\nBaguley (2012): Chapter 2\n\nTechnical:\n\nHeumann, Schomaker, and Shalabh (2022a): Chapter 7\nAgresti and Kateri (2022): Chapter 2\n\nProof-based:\n\nCasella and Berger (2002): Chapters 1 & 2",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.2 Probability theory"
    ]
  },
  {
    "objectID": "chapters/04-probability.html#defining-probability",
    "href": "chapters/04-probability.html#defining-probability",
    "title": "4.2 Probability theory",
    "section": "Defining probability",
    "text": "Defining probability\nThe OED provides several non-technical definitions of term probability, which include\n\n‘the extent to which something is likely to happen or be the case’1 and\na ‘thing judged likely to be true, to exist, or to happen’2,\n\n1 See https://doi.org/10.1093/OED/1639707847.2 See https://doi.org/10.1093/OED/3638534852.While these dictionary definitions capture the intuitive (and rather subjective) nature of probability in everyday language, they leave important questions unanswered. How can we quantify probability? How can we describe the full range of possible outcomes — and their respective probabilities — in a systematic way? To address these concerns, we need a more objective and mathematically grounded notion of probability.\n\nRelative frequency\nAgresti & Kateri (2022: 29) adduce a frequency-based interpretation of probability:\n\n“For an observation of a random phenomenon, the probability of a particular outcome is the proportion of times that outcome would occur in an indefinitely long sequence of like observations, under the same conditions.”\n\nIn other words, the probability of an event3 \\(A\\), denoted \\(P(A)\\), is equivalent to the long-term relative frequency of \\(A\\) as the number of observations \\(n\\) increases. The relative frequency \\(f(A)\\) is obtained by dividing the frequency of occurrence \\(n(A)\\) by the sample size \\(n\\), i.e.,\n3 An event is understood as any subset of the sample space \\(S\\), comprising one or more outcomes.\\[\nf(A) = \\frac{n(A)}{n}.\n\\tag{1}\\]\nHeumann et al. (2022b: 118) explains that \\(f(A)\\) converges to the probability \\(P(A)\\) as \\(n\\) approaches infinity:\n\\[\nP(A) = \\lim_{n\\to\\infty} \\frac{n(A)}{n}.\n\\tag{2}\\]\n\nExample 1 In the FrameNet data, we can use relative frequencies to estimate the probability that an element is a core element, i.e., unique and essential to a frame.\n\n# Probability that an element is a (non-)core element\nframenet %&gt;% \n  count(Coreness) %&gt;% \n  mutate(rel_freq = n/sum(n))\n\n# A tibble: 2 × 3\n  Coreness      n rel_freq\n  &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt;\n1 core     159371    0.824\n2 non-core  33978    0.176\n\n\n\n\n\n\n\n\n\nKolmogorov’s axions\n\n\n\n\n\nKolmogorov’s axioms\nOn the most abstract level, probabilities are defined as functions that associate elements from the sample space \\(S = \\{s_1, s_2, ..., s_n\\}\\) with values in the interval \\([0, 1]\\), subject to certain conditions.\nThe Russian Mathematician Andrei Kolmogorov (1903–1987) proposed three axioms that a probability function \\(P\\) must satisfy:\n\nEvery event \\(A\\) in the sample space \\(S\\) has a probability\n\n\\[\nP(A) \\geq 0.\n\\tag{3}\\]\n\nThe probability of the sample space \\(S\\)\n\n\\[\nP(S) = 1.\n\\tag{4}\\]\n\nAssuming two disjoint (i.e., mutually exclusive) events \\(A\\) and \\(B\\), then\n\nNote that this axiom can be generalised to infinite series of pairwise disjoint events \\(A_i\\):\n\\[\nP(A \\cup B) = P(A) + P(B).\n\\tag{5}\\]\nNote that this axiom can be generalised to infinite series of pairwise disjoint events \\(A_i\\):\n\\[\nP\\left(\\bigcup\\limits_{i=1}^{\\infty} A_i\\right) = \\sum\\limits_{i=1}^{\\infty} P(A_i).\n\\]\n\n\n\n\n\nConditional probability\nIn many linguistic contexts, we’re interested in the probability of one event occurring given that another event has already occurred. This concept is captured by conditional probability, which measures the probability of event \\(A\\) happening when we know that event \\(B\\) has taken place. It is a way of capturing prior knowledge.\nThe conditional probability of \\(A\\) given \\(B\\) is denoted \\(P(A|B)\\) and is defined as:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)},\n\\tag{6}\\]\nprovided that \\(P(B) &gt; 0\\). This formula tells us that the conditional probability is the ratio of the probability that both events occur to the probability that the conditioning event occurs.\nOne of the most important results in probability theory is Bayes’ theorem, which allows us to “reverse” conditional probabilities:\n\\[\nP(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}.\n\\tag{7}\\]\nThis theorem is fundamental to Bayesian statistics.\n\nExample 2 Using the framenet data, we will compute the probabilities:\n\n\\(P(\\text{Agent})\\): The probability that a frame element is an AGENT.\n\n\n# P(Agent)\nframenet %&gt;%\n  count(Frame.Element) %&gt;%\n  mutate(prob = n / sum(n)) %&gt;%\n  filter(Frame.Element == \"Agent\")\n\n# A tibble: 1 × 3\n  Frame.Element     n   prob\n  &lt;chr&gt;         &lt;int&gt;  &lt;dbl&gt;\n1 Agent         12421 0.0642\n\n\n\n\\(P(\\text{Agent} \\mid \\text{Abandonment})\\): The probability that a frame element is an AGENT, given the Abandonment frame.\n\n\n# P(Agent | Abandonment)  \nframenet %&gt;%\n  filter(Frame == \"Abandonment\") %&gt;%\n  count(Frame.Element) %&gt;%\n  mutate(prob = n / sum(n)) %&gt;%\n  filter(Frame.Element == \"Agent\")\n\n# A tibble: 1 × 3\n  Frame.Element     n  prob\n  &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt;\n1 Agent            34 0.298\n\n\n\n\\(P(\\text{Agent} \\mid \\text{Abandonment}, \\text{leave})\\): The probability that a frame element is an AGENT, given the Abandonment frame and the verb leave.\n\n\n# P(Agent | Abandonment, leave)  \nframenet %&gt;%\n  filter(Frame == \"Abandonment\", Verb == \"leave.v\") %&gt;%\n  count(Frame.Element) %&gt;%\n  mutate(prob = n / sum(n)) %&gt;%\n  filter(Frame.Element == \"Agent\")\n\n# A tibble: 1 × 3\n  Frame.Element     n  prob\n  &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt;\n1 Agent            10 0.233\n\n\nWhat is the probability that the frame is Abandonment, given that the frame element is an AGENT, i.e. \\(P(\\text{Abandonment} \\mid \\text{Agent})\\)? We compute\n\\[\nP(\\text{Abandonment} \\mid \\text{Agent}) = \\frac{P(\\text{Agent} \\mid \\text{Abandonment}) \\cdot P(\\text{Abandonment})}{P(\\text{Agent})}.\n\\]\n\n\nShow the code\n# Total number of observations\nN_total &lt;- nrow(framenet)\n\n# Count of Frame = \"Abandonment\"\nn_abandonment &lt;- framenet %&gt;%\n  filter(Frame == \"Abandonment\") %&gt;%\n  count() %&gt;%\n  pull(n)\n\n# Count of Frame.Element = \"Agent\"\nn_agent &lt;- framenet %&gt;%\n  filter(Frame.Element == \"Agent\") %&gt;%\n  count() %&gt;%\n  pull(n)\n\n# Count of Frame.Element = \"Agent\" and Frame = \"Abandonment\"\nn_agent_abandonment &lt;- framenet %&gt;%\n  filter(Frame == \"Abandonment\", Frame.Element == \"Agent\") %&gt;%\n  count() %&gt;%\n  pull(n)\n\n# Now compute:\n# P(Agent | Abandonment) = n_agent_abandonment / n_abandonment\n# P(Abandonment) = n_abandonment / N_total\n# P(Agent) = n_agent / N_total\n\np_abandonment_given_agent &lt;- (n_agent_abandonment / n_abandonment) * (n_abandonment / N_total) / (n_agent / N_total)\n\np_abandonment_given_agent\n\n\n[1] 0.0027373",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.2 Probability theory"
    ]
  },
  {
    "objectID": "chapters/04-probability.html#probability-distributions",
    "href": "chapters/04-probability.html#probability-distributions",
    "title": "4.2 Probability theory",
    "section": "Probability distributions",
    "text": "Probability distributions\nRecall the concept of random variables introduced in 4.1 Data types. They describe random processes, which means that the outcomes of the experiment are not pre-determined in any way; there is always some degree of uncertainty involved. Each outcome occurs with a certain probability.\nWhen we associate each outcome of a random variable with a probability, we obtain its probability distribution. A function that explicitly maps probabilities onto the discrete outcomes of a discrete random variable \\(X\\) is called probability mass function (\\(pmf\\)). In the continuous case, we speak of a probability density function (\\(pdf\\)).\n\nExample 3 We can establish the probability distribution of frame elements for the verb eat (INGESTION frame) as follows:\n\n# Obtain observations on the verb \"eat\"\neat &lt;- framenet %&gt;% \n  filter(Verb == \"eat.v\", Frame == \"Ingestion\")\n\n# Count tokens and compute relative frequencies\neat_data &lt;- eat %&gt;% \n  count(Frame.Element) %&gt;% \n  mutate(rel_freq = n/sum(n)) %&gt;% \n  arrange(desc(rel_freq))\n\n# Ensure that probabilities sum up to 1\nsum(eat_data$rel_freq)\n\n[1] 1\n\n# Plot the PMF\neat_data %&gt;% \n  ggplot(aes(x = Frame.Element, y = rel_freq)) +\n  geom_col() +\n  labs(title = \"PMF of frame elements for 'eat' (INGESTION frame) \")\n\n\n\n\n\n\n\n\n\n\nDiscrete distributions\n\nUniform distribution\nIf all discrete outcomes have the same probability, their distribution is called uniform. A well-known example is the toss of a coin: If the coin is fair, the outcomes \\(\\{\\text{Heads}\\}\\) and \\(\\{\\text{Tails}\\}\\) are equally likely. More generally, if a discrete random variable \\(X\\) has \\(k\\) different outcomes, then any outcome \\(x_i\\) has the probability\n\\[\nP(X = x_i) = \\frac{1}{k}.\n\\tag{8}\\]\nIf we toss a fair coin once, the sample space is \\(S = \\{\\text{Heads}, \\text{Tails}\\}\\), and we have \\(P(\\text{Heads}) = P(\\text{Tails}) = \\frac{1}{2}\\).\nNote that all probabilities have to add up to 1, i.e.,\n\\[\n\\sum_{x\\in X} P(x) = 1.\n\\tag{9}\\]\n\nExample 4 A single throw of a die has the sample space \\(S = \\{1, 2, 3, 4, 5, 6\\}\\). The long-term relative frequency of each outcome, and thus its probability, should be \\(1/6 \\approx 0.17\\).\n\n\nShow the code\n# Load libraries\nlibrary(tidyverse)\n\n# First, set a seed for reproducibility; this ensure that we always generate the 'same' random numbers\nset.seed(123)\n\n# Simulate 10,000 rolls of a fair six-sided die\ndie_rolls &lt;- sample(1:6, size = 10000, replace = TRUE)\n\n# Plot the probability mass function\nggplot(data.frame(die_rolls), aes(x = factor(die_rolls))) +\n  geom_bar(aes(y = ..count../sum(..count..)), fill = \"steelblue\") +\n  geom_hline(yintercept = 1/6, col = \"black\") +\n  labs(title = \"Rolling a die 10000 times (k = 6, p = 1/6)\",\n       y = \"Relative frequency\",\n       x = \"Outcome\") +\n  scale_y_continuous(limits = c(0, 0.25)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nBinomial distribution\nMany corpus-linguistic studies are concerned with discrete random variables \\(X\\) that have exactly two outcomes, such as the dative alternation (give somebody something vs. give something to somebody), the particle placement alternation (pick something up vs. pick up something), or subject and object realisation (I’ve eaten something vs. I’ve eaten Ø).\nAssume we make \\(n\\) independent binary observations (also known as Bernoulli trials) of \\(X\\). If one of the two outcomes of \\(X\\) has a fixed probability \\(\\pi\\) (often denoted ‘success’) and the other one the probability \\(1 - \\pi\\) (‘failure’), \\(X\\) follows a binomial distribution.4 We can also use the shorthand notation\n4 The natural extension of the binomial distribution to \\(k\\) different outcomes is called the multinomial distribution with index \\(n\\) and probabilities \\(\\pi_1, \\pi_2, \\dots, \\pi_k\\).\\[\nX \\sim Binom(n, \\pi).\n\\]\nThe elements inside the parentheses are the parameters of the distribution, determining the outcomes of \\(X\\): the number \\(n\\) of independent observations and the probability \\(\\pi\\) of ‘success’. As such, they affect the shape of the probability mass (or density) function.\n\nExample 5 Suppose we throw a fair coin \\(30\\) times and count how often we obtain ‘heads’. We’d expect to see this outcome \\(30 \\cdot 0.5 = 15\\) times.\n\n\nShow the code\n# Set seed for reproducibility\nset.seed(123)\n\n# Set parameters for binomial distribution\nn &lt;- 30    # Number of trials\npi &lt;- 0.5  # Probability of HEADS\n\n# Set up the pmf\nsuccesses &lt;- 0:n\nprob_mass &lt;- dbinom(successes, size = n, prob = pi)\n\n# Create a data frame for plotting\nbinom_data &lt;- data.frame(successes = successes, probability = prob_mass)\n\n# Basic plot approach\nggplot(binom_data, aes(x = successes, y = probability)) +\n  geom_segment(aes(xend = successes, yend = 0), linewidth = 1.2) +\n  labs(title = \"PMF of n = 30 coin tosses with P({Heads}) = 0.5\",\n       x = \"Occurrences of {Heads}\", y = \"Relative frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson distribution\nThe Poisson distribution is particularly suitable for frequency data, which is ubiquitous in corpus linguistics. A Poisson-distributed random variable is fully determined by its parameter \\(\\lambda\\), which determines how often events occur.\n\\[\nX \\sim Pois(\\lambda).\n\\]\nAssume a word occurs 3 times per 1,000 words. We would define the rate parameter as \\(\\lambda = 3\\), which is the expected and likeliest outcome.\n\n\nShow the code\n# Define the occurrence rate\nlambda &lt;- 3\n\n# Define the range of x values\nx_range &lt;- 0:10\n\n# Compute PMF values using dpois()\npoisson_pmf &lt;- data.frame(\n  x = x_range,\n  probability = dpois(x_range, lambda)\n)\n\n# Create the PMF plot using ggplot\nggplot(poisson_pmf, aes(x = x, y = probability)) +\n  geom_segment(aes(xend = x, yend = 0), linewidth = 1.2) + \n  labs(title = \"Poisson PMF with λ = 3\",\n       x = \"Number of events\",\n       y = \"Probability\") +\n  scale_x_continuous(breaks = x_range) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nContinuous distributions\n\nThe normal distribution\nA great number of numerical variables in the world follow the well-known normal (or Gaussian) distribution, which includes test scores, weight and height, among many others. The plot below illustrates its characteristic bell-shape: Most observations are in the middle, with considerably fewer near the fringes. For example, most people are rather “average” in height; there are only few people that are extremely short or extremely tall.\nThe normal distribution is typically described in terms of two parameters: The population mean \\(\\mu\\) and the population variance \\(\\sigma\\). If a random variable \\(X\\) is normally distributed, we typically use the notation in Equation 10.\n\\[ X \\sim \\mathcal{N}(\\mu, \\sigma^2).\n\\tag{10}\\]\nThe \\(\\mu\\) parameter corresponds to the expected value \\(E(X)\\), which is a typical (or average) value of a distribution.\nThe spread of data points around the expectation is the population variance and corresponds to \\(\\sigma^2\\):\n\\[\nVar(X) = E(X-E(X))^2.\n\\]\nThe population standard deviation \\(\\sigma\\) is the average distance from the expectation and is defined as \\(\\sqrt{Var(X)}\\).\n\nExample 6 The plot illustrates a standard normal distribution for \\(X \\sim \\mathcal{N}(0, 1)\\). The \\(y\\)-axis indicates the density of population values; note that since the Gaussian distribution is a continuous distribution with technically infinite \\(x\\)-values, the probability of any given value must be 0. We can only obtain probabilities for intervals of values, which are given by\n\\[\nP(a \\leq X \\leq b) = \\int_a^b f(x)dx.\n\\tag{11}\\]\n\n\nShow the code\n# Set parameters for normal distribution\nmu &lt;- 0     # mean (expectation)\nsigma &lt;- 1  # standard deviation (square root of variance)\nvariance &lt;- sigma^2\n\n# Generate a sequence of x-values in a range of +/- 4 standard deviations from the mean\nx_values &lt;- seq(mu - 4*sigma, mu + 4*sigma, length.out = 1000)\n\n# Collect everything in a data frame\nnorm_data &lt;- data.frame(\n  x = x_values,\n  density = dnorm(x_values, mean = mu, sd = sigma)\n)\n\n# Plot the simulated data\nggplot(norm_data, aes(x = x, y = density)) +\n  geom_line(linewidth = 1, color = \"#0D47A1\") +\n  labs(\n    title = \"PDF for N(0, 1)\",\n    x = \"x\",\n    y = \"Probability density\") +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick facts about the Gaussian bell curve\n\n\n\n\n\nQuite interestingly,\n\n68% all values fall within one standard deviation of the mean,\n95% within two, and\n99.7% within three.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe lognormal distribution\nWhile many variables follow a normal distribution, others exhibit a characteristic right-skewed pattern where most observations cluster near zero but some extend far into the positive tail. This is particularly common with reaction times or survival data.\nThe log-normal distribution describes variables whose natural logarithm follows a normal distribution. If \\(\\ln(X) \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then \\(X\\) follows a log-normal distribution, denoted as:\n\\[\nX \\sim LogN(\\mu, \\sigma^2).\n\\] The parameters \\(\\mu\\) and \\(\\sigma^2\\) refer to the mean and variance of the underlying normal distribution (i.e., of \\(\\ln(X)\\)), not of \\(X\\) itself.\n\nExample 7 Word frequencies in natural language corpora typically follow a log-normal distribution. Consider the frequency distribution of lemmas in a corpus, where most words occur rarely but a few occur very frequently (following Zipf’s law).\n\n\nShow the code\n# Set parameters for log-normal distribution\nmu_log &lt;- 2      # Mean of the underlying normal distribution (log scale)\nsigma_log &lt;- 1   # Standard deviation of the underlying normal distribution\n\n# Generate x values on the positive real line\nx_values &lt;- seq(0.1, 50, length.out = 1000)\n\n# Calculate the probability density\nlognorm_data &lt;- data.frame(\n  x = x_values,\n  density = dlnorm(x_values, meanlog = mu_log, sdlog = sigma_log)\n)\n\n# Plot the log-normal distribution\nggplot(lognorm_data, aes(x = x, y = density)) +\n  geom_line(linewidth = 1, color = \"purple\") +\n  labs(\n    title = \"PDF for Log-Normal Distribution (μ = 2, σ = 1)\",\n    x = \"Frequency (occurrences per million words)\",\n    y = \"Probability density\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\n# Show the relationship between normal and log-normal\n# Generate sample data\nset.seed(123)\nnormal_sample &lt;- rnorm(1000, mean = mu_log, sd = sigma_log)\nlognormal_sample &lt;- exp(normal_sample)\n\n# Create comparison plot\npar(mfrow = c(1, 2))\nhist(normal_sample, main = \"Normal Distribution\\n(log scale)\", \n     xlab = \"ln(X)\", col = \"#90CAF9\", breaks = 30)\nhist(lognormal_sample, main = \"Log-Normal Distribution\\n(original scale)\", \n     xlab = \"X\", col = \"#FFCDD2\", breaks = 30)",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.2 Probability theory"
    ]
  },
  {
    "objectID": "chapters/01-fundamentals.html",
    "href": "chapters/01-fundamentals.html",
    "title": "1. Fundamentals",
    "section": "",
    "text": "This section covers the fundamental concepts needed to understand statistics for corpus linguistics.\n\n\n\n1.1 Basics\n1.2 Research Questions\n1.3 Linguistic Variables\n1.4 Formal aspects"
  },
  {
    "objectID": "chapters/01-fundamentals.html#chapters",
    "href": "chapters/01-fundamentals.html#chapters",
    "title": "1. Fundamentals",
    "section": "",
    "text": "1.1 Basics\n1.2 Research Questions\n1.3 Linguistic Variables\n1.4 Formal aspects"
  },
  {
    "objectID": "chapters/01-linguistic_variables.html",
    "href": "chapters/01-linguistic_variables.html",
    "title": "1.3 Linguistic variables",
    "section": "",
    "text": "The classical view: Labov (1972) defines a linguistic variable as “two ways of saying the same thing.”\nA restriction: Meyerhoff (2009: 11) summarized, “In sum, a sociolinguistic variable can be defined as a linguistic variable that is constrained by social or non-linguistic factors…”\nA more open view: Kiesling (2011) argued, “Given the variability of what counts as a variable, we must define what counts as a variable more broadly than ‘two or more ways of saying the same thing’. We will simply say that a linguistic variable is a choice or option about speaking in a speech community… Note that this definition does not in any way require us to state that the meaning be the same, although there should be some kind of equivalence noted.”\n\n\n\n\n\n\n\nDiscussion\n\n\n\nWhich of the following variables are good sociolinguistic variables, and which of them are poor?\n\n/fɔːθ flɔː/ vs. /fɔːrθ flɔːr/\nThis enables him to preside over the process which I have described vs. This enables him to preside over the process that I have described vs. This enables him to preside over the process ∅ I have described.\nThe pair found the briefcase on a bus station bench at Bath central bus station. vs. The briefcase was found on a bus station bench at Bath central bus station by the pair.\nArt is after all the subject of attention for both critic and historian, even though the functions and methods of the two sorts of writer have drawn apart. vs. Art histories often make an attempt to keep to chronology, although the difficulties include the crucial fact that in art there is no clear sequence of events. vs. Many of his readers approved his sensitive and appreciative understanding of paintings, though without sharing his political views.\n/pleɪŋ/ vs. /pleɪn/\n[tʰ] in /tɔp/ vs. [t] in stop.",
    "crumbs": [
      "1. Fundamentals",
      "1.3 Linguistic Variables"
    ]
  },
  {
    "objectID": "chapters/01-linguistic_variables.html#what-is-a-linguistic-variable",
    "href": "chapters/01-linguistic_variables.html#what-is-a-linguistic-variable",
    "title": "1.3 Linguistic variables",
    "section": "",
    "text": "The classical view: Labov (1972) defines a linguistic variable as “two ways of saying the same thing.”\nA restriction: Meyerhoff (2009: 11) summarized, “In sum, a sociolinguistic variable can be defined as a linguistic variable that is constrained by social or non-linguistic factors…”\nA more open view: Kiesling (2011) argued, “Given the variability of what counts as a variable, we must define what counts as a variable more broadly than ‘two or more ways of saying the same thing’. We will simply say that a linguistic variable is a choice or option about speaking in a speech community… Note that this definition does not in any way require us to state that the meaning be the same, although there should be some kind of equivalence noted.”\n\n\n\n\n\n\n\nDiscussion\n\n\n\nWhich of the following variables are good sociolinguistic variables, and which of them are poor?\n\n/fɔːθ flɔː/ vs. /fɔːrθ flɔːr/\nThis enables him to preside over the process which I have described vs. This enables him to preside over the process that I have described vs. This enables him to preside over the process ∅ I have described.\nThe pair found the briefcase on a bus station bench at Bath central bus station. vs. The briefcase was found on a bus station bench at Bath central bus station by the pair.\nArt is after all the subject of attention for both critic and historian, even though the functions and methods of the two sorts of writer have drawn apart. vs. Art histories often make an attempt to keep to chronology, although the difficulties include the crucial fact that in art there is no clear sequence of events. vs. Many of his readers approved his sensitive and appreciative understanding of paintings, though without sharing his political views.\n/pleɪŋ/ vs. /pleɪn/\n[tʰ] in /tɔp/ vs. [t] in stop.",
    "crumbs": [
      "1. Fundamentals",
      "1.3 Linguistic Variables"
    ]
  },
  {
    "objectID": "chapters/01-linguistic_variables.html#the-principle-of-accountability",
    "href": "chapters/01-linguistic_variables.html#the-principle-of-accountability",
    "title": "1.3 Linguistic variables",
    "section": "The principle of accountability",
    "text": "The principle of accountability\n\n\n\n\n\n\nTask\n\n\n\nTwo linguists aim to study the preference for passives among men and women. They extract all the passives from 500,000 words of male speech and all passives from 500,000 words of female speech and report the results. What’s wrong?",
    "crumbs": [
      "1. Fundamentals",
      "1.3 Linguistic Variables"
    ]
  },
  {
    "objectID": "chapters/01-linguistic_variables.html#subtypes-of-variables",
    "href": "chapters/01-linguistic_variables.html#subtypes-of-variables",
    "title": "1.3 Linguistic variables",
    "section": "Subtypes of variables",
    "text": "Subtypes of variables\n\nLinguistic perspective\n\nphonetic/phonological\nmorphological\nsyntactic\npragmatic\n\n\n\nSociolinguistic perspective\nSociolinguistic variables also differ with regard to their salience in society.\n\nStereotypes are strongly socially marked and part of popular discourse about language.\n\nh-dropping in Cockney\nCanadian eh at the end of sentences\nAustralian dinkum: I was fair dinkum about my interest in their culture ‘authentic, genuine’\n\nMarkers show both social and style stratification; all members of a society react similarly in taking care to avoid the pattern in formal registers.\n\n(r)\n(th)\n\nIndicators differentiate social groups. However, people are not aware of them and therefore do not avoid them in formal registers.\n\nSame vowel in God and Guard in New York City\n\n\n\nCf. Mesthrie (2011).",
    "crumbs": [
      "1. Fundamentals",
      "1.3 Linguistic Variables"
    ]
  },
  {
    "objectID": "chapters/01-linguistic_variables.html#many-morphosyntactic-variables-in-english",
    "href": "chapters/01-linguistic_variables.html#many-morphosyntactic-variables-in-english",
    "title": "1.3 Linguistic variables",
    "section": "Many morphosyntactic variables in English",
    "text": "Many morphosyntactic variables in English\n\n\n\n\n\n\n\nVariable\nExample\n\n\n\n\nIndefinite Pronouns\neverybody vs. everyone\n\n\nCase and order of coordinated pronouns\nmy husband and I vs. my husband and me vs. me and my husband\n\n\nthat vs. zero complementation\nI don’t think that/Ø it’s a problem.\n\n\nthat vs. gerundial complementation\nremember that vs. remember V-ing; try to vs. try and vs. try V-ing\n\n\nParticle placement\nset the computer up vs. set up the computer\n\n\nThe dative alternation\ngive the book to John vs. give John the book\n\n\nThe genitive alternation\nJohn’s house vs. the house of John\n\n\nRelativization strategies\nwh-word vs. that vs. Ø\n\n\nAnalytic vs. synthetic comparatives\nwarmer vs. more scary\n\n\nPlural existentials\nthere are some places vs. there’s some places\n\n\nFuture temporal reference\nwill vs. going to vs. progressive etc.\n\n\nDeontic modality\nmust vs. have to vs. need to vs. got to etc.\n\n\nStative possession\nhave vs. have got vs. got\n\n\nQuotatives\nsay vs. be like vs. go etc.\n\n\nnot vs. no\nnot anybody vs. nobody; not anyone vs. no one; not anything vs. nothing\n\n\nNOT vs. AUX contraction\nthat’s not vs. that isn’t etc.\n\n\n\n\nCf. Gardner et al. (2021).",
    "crumbs": [
      "1. Fundamentals",
      "1.3 Linguistic Variables"
    ]
  },
  {
    "objectID": "chapters/02-importing_exporting.html",
    "href": "chapters/02-importing_exporting.html",
    "title": "2.6 Import/export data",
    "section": "",
    "text": "Script\n\n\n\nYou can find the full R script associated with this unit here.",
    "crumbs": [
      "2. Introduction to R",
      "2.6 Importing/Exporting"
    ]
  },
  {
    "objectID": "chapters/02-importing_exporting.html#recommended-reading",
    "href": "chapters/02-importing_exporting.html#recommended-reading",
    "title": "2.6 Import/export data",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nWinter (2020): Chapter 1.11",
    "crumbs": [
      "2. Introduction to R",
      "2.6 Importing/Exporting"
    ]
  },
  {
    "objectID": "chapters/02-importing_exporting.html#preparation",
    "href": "chapters/02-importing_exporting.html#preparation",
    "title": "2.6 Import/export data",
    "section": "Preparation",
    "text": "Preparation\nThe first section of an R script should always specify the libraries that are needed for executing the code to follow. In this unit, we will need readxl and writexl to aid us with importing MS Excel files.\n\nlibrary(readxl)\nlibrary(writexl)\n\nIf you haven’t installed them yet, the R console will throw an error message. For instructions on how to install an R package, consult the unit on Libraries.\n\nExporting data\nAssume we’d like to export our data frame with word frequencies to a local file on our system. Let’s briefly regenerate the data frame:\n\n# Generate data frame\ndata &lt;- data.frame(lemma = c(\"start\", \"enjoy\", \"begin\", \"help\"), \n                   frequency = c(418, 139, 337, 281))\n\n# Print contents\nprint(data)\n\n  lemma frequency\n1 start       418\n2 enjoy       139\n3 begin       337\n4  help       281\n\n\nThere are two common formats in which tabular data can be stored:\n\nas .csv-files (‘comma-separated values’; native format of LibreOffice Calc)\nas .xls/.xlsx-files (Microsoft Excel)\n\n\n\n\n\n\n\nExport to CSV\n\n\n\n\n\nTo save our data data frame in .csv-format, we can use the write_table() function:\n\nwrite.csv(data, \"frequency_data.csv\")\n\nThe file is now stored at the location of your current R script. You can open this file …\n\nin LibreOffice\nin Microsoft Excel via File &gt; Import &gt; CSV file &gt; Select the file &gt; Delimited and then Next &gt; Comma and Next &gt; General and Finish.\n\nClearly, opening CSV files in MS Excel is quite cumbersome, which is why it’s better to export it as an Excel file directly.\n\n\n\n\n\n\n\n\n\nExport to Excel\n\n\n\n\n\nWe use the write_xlsx() function provided by the package writexl.\n\nwrite_xlsx(data, \"frequency_data.xlsx\")\n\nThe file is now stored at the location of your currently active R script. You should now be able to open it in MS Excel without any issues.\n\n\n\n\n\nImporting data\nLet’s read the two files back into R.\n\n\n\n\n\n\nImport from CSV\n\n\n\n\n\nTo import the CSV file, we can use the read.csv() function:\n\nimported_csv &lt;- read.csv(\"frequency_data.csv\")\nprint(imported_csv)\n\n  X lemma frequency\n1 1 start       418\n2 2 enjoy       139\n3 3 begin       337\n4 4  help       281\n\n\nIt appears that read.csv() has also written the row numbers to the file. This is not the desired outcome and can be prevented by adding an additional argument:\n\nimported_csv &lt;- read.csv(\"frequency_data.csv\", row.names = 1)\nprint(imported_csv) # Problem solved!\n\n  lemma frequency\n1 start       418\n2 enjoy       139\n3 begin       337\n4  help       281\n\n\n\n\n\n\n\n\nA note on file encodings and separators\n\n\n\n\n\nWhen working with CSV files, you may encounter issues with character encodings and separators, especially when:\n\nworking with files from different operating systems,\ndealing with text containing special characters (é, ü, ñ, etc.), or\nimporting files created in different regions (e.g., European vs. US).\n\nThe most common encoding-related parameters for read.csv() are:\n\n# For files with special characters (recommended default)\ndata &lt;- read.csv(\"myfile.csv\", encoding = \"UTF-8\")\n\n# For files from Windows systems\ndata &lt;- read.csv(\"myfile.csv\", encoding = \"latin1\")\n\n# For files using semicolons and commas as decimal points\ndata &lt;- read.csv(\"myfile.csv\", sep = \";\", dec = \",\")\n\n\nIf you see garbled text like Ã© instead of é, try specifying encoding = \"UTF-8\".\nIf your data appears in a single column, check if your file uses semicolons (;) instead of commas (,) as separators.\nIf numeric values are incorrect, verify whether the file uses commas or periods as decimal separators.\n\n\n\n\n\n\n\n\n\n\n\n\n\nImport from Excel\n\n\n\n\n\nFor importing the Excel file, we’ll use the read_xlsx() function from the readxl package:\n\nimported_excel &lt;- read_xlsx(\"frequency_data.xlsx\")\nprint(imported_excel)\n\n# A tibble: 4 × 2\n  lemma frequency\n  &lt;chr&gt;     &lt;dbl&gt;\n1 start       418\n2 enjoy       139\n3 begin       337\n4 help        281\n\n\n\n\n\nThat’s it! Nevertheless, remember to always check your imported data to ensure it has been read in correctly, especially when working with CSV files.",
    "crumbs": [
      "2. Introduction to R",
      "2.6 Importing/Exporting"
    ]
  },
  {
    "objectID": "chapters/02-importing_exporting.html#a-convenient-alternative-rds-files",
    "href": "chapters/02-importing_exporting.html#a-convenient-alternative-rds-files",
    "title": "2.6 Import/export data",
    "section": "A convenient alternative: RDS files",
    "text": "A convenient alternative: RDS files\nIf the main goal is to save an intermediary result and make it available for later use, the most efficient solution is to save the object to a local R data file ending in .RDS. Since it compressed data, .RDS files can be considered analogous to .zip files, which are very commonly used for other data types.\nIn practice, we use the saveRDS() function and supply it with …\n\n… an R object (e.g., a vector, data frame, matrix, graphs, statistical models – anything goes!) as well as\n… the desired name of the file.\n\n\n# Save data frame \"data\" to the file \"frequency_data.RDS\"\nsaveRDS(data, \"frequency_data.RDS\")\n\nTo read a file back in, we need to indicate the file name (or the full file path if the file is located in a different folder).\n\n# Read in \"frequency_data.RDS\" and assign the contents to \"data2\"\ndata2 &lt;- readRDS(\"frequency_data.RDS\")\n\n# Verify contents\nprint(data2)\n\n  lemma frequency\n1 start       418\n2 enjoy       139\n3 begin       337\n4  help       281",
    "crumbs": [
      "2. Introduction to R",
      "2.6 Importing/Exporting"
    ]
  },
  {
    "objectID": "chapters/02-importing_exporting.html#troubleshooting",
    "href": "chapters/02-importing_exporting.html#troubleshooting",
    "title": "2.6 Import/export data",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nImporting data into R is often a challenging and error-prone task, made more difficult by the wide range of potential issues. AI tools can be immensely helpful for identifying and addressing problems specific to your use case. This handout provides some practical guidance, including tips for resolving file-loading errors.",
    "crumbs": [
      "2. Introduction to R",
      "2.6 Importing/Exporting"
    ]
  },
  {
    "objectID": "chapters/02-importing_exporting.html#exercises",
    "href": "chapters/02-importing_exporting.html#exercises",
    "title": "2.6 Import/export data",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nSolutions\n\n\n\nYou can find the solutions to the exercises here.\n\n\n\nExercise 1 Download the file SCOPE_reduced.RDS from this repository and read it into a variable named SCOPE. It contains data from the the South Carolina Psycholinguistic Metabase (Gao, Shinkareva, and Desai 2022), specifically:\n\nNumber of meanings (Nsenses_WordNet)\nEmotional valence ratings, which describe the pleasantness of a lexical stimulus on a scale from 1 to 9 (Valence_Warr)\nData for nearly 200,000 words\n\n\n\nExercise 2 Using this database, retrieve\n\nthe number of meanings for the verbs start, enjoy, begin, help. Store them in a data frame with the name senses_df.\nemotional valence ratings for the words fun, love, vacation, war, politics, failure, table. Store them in a data frame, and name it valence_df.\n\nWhat do you notice about the valence ratings? Do they align with your intuitions about these words’ emotional content?\n\n\n\n\n\n\nTip\n\n\n\nThis task is very similar to ?@exr-df-3!\n\n\n\n\nExercise 3 Export senses_df and valence_df both as .csv and .xlsx files, and read them back into R.\n\n\nExercise 4 Separators determine how tabular data is stored internally. Investigate what happens when you read in frequency_data.csv with different separator settings:\n\n# Comma separator (default)\nimported_csv1 &lt;- read.csv(\"frequency_data.csv\", sep = \",\")\nprint(imported_csv1)\n\n# Tab separator\nimported_csv2 &lt;- read.csv(\"frequency_data.csv\", sep = \"\\t\")\nprint(imported_csv2)\n\n# Semi-colon separator\nimported_csv3 &lt;- read.csv(\"frequency_data.csv\", sep = \";\")\nprint(imported_csv3)",
    "crumbs": [
      "2. Introduction to R",
      "2.6 Importing/Exporting"
    ]
  },
  {
    "objectID": "chapters/07-PCA.html",
    "href": "chapters/07-PCA.html",
    "title": "7.3 Principal Components Analysis",
    "section": "",
    "text": "For linguists:\n\nLevshina (2015: Chapter 18)\n\nGeneral:\n\nMair (2018: Chapter 6)",
    "crumbs": [
      "7. Machine Learning",
      "7.3 PCA"
    ]
  },
  {
    "objectID": "chapters/07-PCA.html#recommended-reading",
    "href": "chapters/07-PCA.html#recommended-reading",
    "title": "7.3 Principal Components Analysis",
    "section": "",
    "text": "For linguists:\n\nLevshina (2015: Chapter 18)\n\nGeneral:\n\nMair (2018: Chapter 6)",
    "crumbs": [
      "7. Machine Learning",
      "7.3 PCA"
    ]
  },
  {
    "objectID": "chapters/07-PCA.html#sec-pca-prep",
    "href": "chapters/07-PCA.html#sec-pca-prep",
    "title": "7.3 Principal Components Analysis",
    "section": "Preparation",
    "text": "Preparation\nThis unit relies on psycholinguistic data from the South Carolina Psycholinguistic Metabase (Gao, Shinkareva, and Desai 2022).1 Detailed descriptions of the variables can be found here.\n1 One exception is the variable Resnik_strength [Resnik (1996)], which was computed manually and appended to the data frame.The data frame scope_sem_df contains semantic ratings for a sample of 1,702 transitive verbs. Note that all columns have been standardised (cf. ?scale() for details).\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(purrr)\nlibrary(lattice)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\nlibrary(GPArotation)\n\n\nAttaching package: 'GPArotation'\n\nThe following objects are masked from 'package:psych':\n\n    equamax, varimin\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n# Load data\nscope_sem_df &lt;- readRDS(\"../datasets/scope_sem.RDS\")\n\n# Select subset\nscope_sem_sub &lt;- scope_sem_df[,1:11]\n\n# Overview\nglimpse(scope_sem_sub)\n\nRows: 1,702\nColumns: 11\n$ Verb               &lt;chr&gt; \"abstain\", \"abstract\", \"abuse\", \"accelerate\", \"acce…\n$ Resnik_strength    &lt;dbl&gt; 0.40909889, 0.18206692, 0.12473608, -0.76972217, -1…\n$ Conc_Brys          &lt;dbl&gt; -0.94444378, -1.92983639, -0.59478833, 0.22107437, …\n$ Nsenses_WordNet    &lt;dbl&gt; -0.68843996, 0.27755219, 0.00155443, -0.68843996, 0…\n$ Nmeanings_Websters &lt;dbl&gt; -0.95559835, 0.73781281, 0.73781281, -0.27823388, 0…\n$ Visual_Lanc        &lt;dbl&gt; -2.2545455, 0.6103733, 1.3354358, -0.4342084, -0.34…\n$ Auditory_Lanc      &lt;dbl&gt; -0.84225787, -0.35605108, 1.54797548, 0.18795651, 1…\n$ Haptic_Lanc        &lt;dbl&gt; -0.75523987, -0.29089287, 1.25099360, -0.18911818, …\n$ Olfactory_Lanc     &lt;dbl&gt; -0.14444936, -0.37350419, -0.53335522, -0.37350419,…\n$ Gustatory_Lanc     &lt;dbl&gt; 0.27698988, -0.10105698, -0.36148925, -0.52110903, …\n$ Interoceptive_Lanc &lt;dbl&gt; 1.08153427, -0.06560311, 1.64313895, 1.45452985, 0.…",
    "crumbs": [
      "7. Machine Learning",
      "7.3 PCA"
    ]
  },
  {
    "objectID": "chapters/07-PCA.html#descriptive-overview",
    "href": "chapters/07-PCA.html#descriptive-overview",
    "title": "7.3 Principal Components Analysis",
    "section": "Descriptive overview",
    "text": "Descriptive overview\nA popular descriptive measure for associations between continuous variables \\(x\\) and \\(y\\) is the Pearson product-moment correlation coefficient (or simply Pearson’s \\(r\\); cf. Equation 1). It varies on a scale from \\(-1\\) to \\(1\\) and indicates the extent to which two variables form a straight-line relationship (Heumann, Schomaker, and Shalabh 2022: 153-154). One of its core components is the covariance between \\(x\\) and \\(y\\) which “measures the average tendency of two variables to covary (change together)” (Baguley 2012: 206).\n\\[\nr_{xy} = \\frac{Cov(x, y)}{\\sqrt{Var(x)}\\sqrt{Var(y)}}= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}.\n\\tag{1}\\]\nIn R, we can compute Pearson’s \\(r\\) by using the cor() function.\n\n# Check correlation between number of senses and concreteness\ncor(scope_sem_sub[,-1]$Nsenses_WordNet, scope_sem_sub[,-1]$Conc_Brys) # low\n\n[1] 0.2351554\n\n# Check correlation between haptic experience and concreteness\ncor(scope_sem_sub[,-1]$Haptic_Lanc, scope_sem_sub[,-1]$Conc_Brys) # high\n\n[1] 0.5676945\n\n\nIf the data frame consists of numeric columns only (i.e., if it is a matrix), we can apply cor() to the full dataset and obtain the correlation matrix (also known as covariance matrix).\n\n# Generate correlation matrix\ncor_mat1 &lt;- cor(scope_sem_sub[,-1])\n\nhead(cor_mat1)\n\n                   Resnik_strength  Conc_Brys Nsenses_WordNet\nResnik_strength         1.00000000  0.1166670     -0.37442983\nConc_Brys               0.11666697  1.0000000      0.23515537\nNsenses_WordNet        -0.37442983  0.2351554      1.00000000\nNmeanings_Websters     -0.34225250  0.2023356      0.68509560\nVisual_Lanc             0.05471417  0.5519836      0.17154846\nAuditory_Lanc          -0.11162700 -0.2683646     -0.02960745\n                   Nmeanings_Websters Visual_Lanc Auditory_Lanc  Haptic_Lanc\nResnik_strength            -0.3422525  0.05471417   -0.11162700  0.008260683\nConc_Brys                   0.2023356  0.55198358   -0.26836458  0.567694470\nNsenses_WordNet             0.6850956  0.17154846   -0.02960745  0.239470104\nNmeanings_Websters          1.0000000  0.14597243   -0.04656650  0.193226862\nVisual_Lanc                 0.1459724  1.00000000   -0.11674896  0.404536416\nAuditory_Lanc              -0.0465665 -0.11674896    1.00000000 -0.289586292\n                   Olfactory_Lanc Gustatory_Lanc Interoceptive_Lanc\nResnik_strength        0.05131717    0.015087346      -9.459551e-02\nConc_Brys              0.21354305    0.123459754      -3.257702e-01\nNsenses_WordNet       -0.03353627   -0.018262178      -1.392934e-02\nNmeanings_Websters    -0.01898766    0.001409646      -4.460375e-05\nVisual_Lanc            0.15319007    0.055176064      -3.424087e-01\nAuditory_Lanc         -0.06123191   -0.047086877       1.799479e-01\n\n# Plot correlation matrix\ncorrplot(cor_mat1, col = topo.colors(200), tl.col = \"darkgrey\", number.cex = 0.5, tl.cex = 0.5)\n\n\n\n\n\n\n\n\nSince the upper triangle mirrors the lower one, it is enough to only examine one of them. The diagonal values are not particularly insightful and can be ignored.\n\n# Levelplot\nseq1 &lt;- seq(-1, 1, by = 0.01)\n\nlevelplot(cor_mat1, aspect = \"fill\", col.regions = topo.colors(length(seq1)),\n          at = seq1, scales = list(x = list(rot = 45)),\n          xlab = \"\", ylab = \"\")\n\n\n\n\n\n\n\n\nNeedless to say, the above correlation matrices are hard to interpret – even more so if the number of variables were to increase further.\nPrincipal Components Analysis offers a technique to break down a high-dimensional dataset into a much smaller set of “meta-variables”, i.e., principle components (PCs) which capture the bulk of the variance in the data. This is also known as dimension reduction, which allows researchers to see overarching patterns in the data and re-use the output for further analysis (e.g., clustering or predictive modelling).",
    "crumbs": [
      "7. Machine Learning",
      "7.3 PCA"
    ]
  },
  {
    "objectID": "chapters/07-PCA.html#basics-of-pca",
    "href": "chapters/07-PCA.html#basics-of-pca",
    "title": "7.3 Principal Components Analysis",
    "section": "Basics of PCA",
    "text": "Basics of PCA\nPCA “repackages” large sets of variables by forming uncorrelated linear combinations of them, yielding \\(k\\) principal components \\(Z_1, ..., Z_k\\) (PCs hf.) of the dataset (for \\(1, ..., k\\)). PCs are ordered such that the first PC explains the most variance in the data, with each subsequent PC explaining the maximum remaining variance while being uncorrelated with previous PCs.\nEach PC comprises a set of loadings (or weights) \\(w_{nm}\\), which are comparable to the coefficients of regression equations. For instance, the first PC has the general form shown in Equation 2, where \\(x_m\\) stand for continuous input variables in the \\(n \\times m\\) data matrix \\(\\mathbf{X}\\).\n\\[\nZ_{1} = w_{11}\n\\begin{pmatrix}\nx_{11} \\\\\nx_{21} \\\\\n\\vdots \\\\\nx_{n1}\n\\end{pmatrix}\n+ w_{21}\n\\begin{pmatrix}\nx_{12} \\\\\nx_{22} \\\\\n\\vdots \\\\\nx_{n2}\n\\end{pmatrix}\n+ \\dots + w_{m1}\n\\begin{pmatrix}\nx_{1m} \\\\\nx_{2m} \\\\\n\\vdots \\\\\nx_{nm}\n\\end{pmatrix}\n\\tag{2}\\]\nIf a feature positively loads on a principal component (i.e., \\(w &gt; 0\\)), it means that as the value of this feature increases, the score for this principal component also increases. The magnitude of \\(w\\) indicates the strength of this relationship. Conversely, negative loadings (\\(w &lt; 0\\)) indicate that as the feature value increases, the PC score decreases as well.\n\n\n\n\n\n\nHow do we find PCs?\n\n\n\n\n\nPCs are identified using common techniques from matrix algebra, namely singular value decomposition and eigenvalue decomposition. By breaking down the input data into products of several further matrices, it becomes possible to characterise the exact ‘shape’ of its variance (Mair 2018: 181).\n\n\n\nThe figure below offers a visual summary of PCA:\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning in geom_segment(aes(x = center[1], y = center[2], xend = center[1] + : All aesthetics have length 1, but the data has 100 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 100 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = center[1], y = center[2], xend = center[1] + : All aesthetics have length 1, but the data has 100 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.",
    "crumbs": [
      "7. Machine Learning",
      "7.3 PCA"
    ]
  },
  {
    "objectID": "chapters/07-PCA.html#application-in-r",
    "href": "chapters/07-PCA.html#application-in-r",
    "title": "7.3 Principal Components Analysis",
    "section": "Application in R",
    "text": "Application in R\n\nFitting the model and identifying number of PCs\nFirst, we fit a PCA object with the number of PCs equivalent to the number of columns in scope_sem_sub.\n\n# Fit initial PCA\npca1 &lt;- principal(scope_sem_sub[,-1],\n                  nfactors = ncol(scope_sem_sub[,-1]),\n                  rotate = \"none\")\n\n# Print loadings\nloadings(pca1)\n\n\nLoadings:\n                   PC1    PC2    PC3    PC4    PC5    PC6    PC7    PC8   \nResnik_strength            0.666 -0.271 -0.100  0.250  0.627              \nConc_Brys           0.813  0.210 -0.173         0.149        -0.170 -0.260\nNsenses_WordNet     0.523 -0.696  0.124                0.241              \nNmeanings_Websters  0.493 -0.683  0.149                0.343              \nVisual_Lanc         0.691  0.168 -0.236  0.382  0.152 -0.127  0.484  0.136\nAuditory_Lanc      -0.388 -0.228  0.199  0.734  0.413        -0.208       \nHaptic_Lanc         0.728  0.113        -0.272  0.401 -0.254 -0.266  0.120\nOlfactory_Lanc      0.324  0.444  0.671  0.163 -0.200               -0.347\nGustatory_Lanc      0.256  0.377  0.759        -0.160                0.377\nInteroceptive_Lanc -0.341 -0.164  0.577 -0.366  0.543         0.265 -0.100\n                   PC9    PC10  \nResnik_strength                 \nConc_Brys          -0.378       \nNsenses_WordNet            0.405\nNmeanings_Websters        -0.373\nVisual_Lanc                     \nAuditory_Lanc                   \nHaptic_Lanc         0.266       \nOlfactory_Lanc      0.234       \nGustatory_Lanc     -0.187       \nInteroceptive_Lanc -0.120       \n\n                 PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9  PC10\nSS loadings    2.629 1.898 1.595 0.937 0.806 0.657 0.461 0.378 0.330 0.309\nProportion Var 0.263 0.190 0.160 0.094 0.081 0.066 0.046 0.038 0.033 0.031\nCumulative Var 0.263 0.453 0.612 0.706 0.787 0.852 0.898 0.936 0.969 1.000\n\n\nIt is common practice to retain only those PCs with eigenvalues (variances) \\(&gt; 1\\) (cf. scree plot).\n\n# Scree plot\nbarplot(pca1$values, main = \"Scree plot\", ylab = \"Variances\", xlab = \"PC\", # first three PCs\n        names.arg = 1:length(pca1$values))\n  abline(h = 1, col = \"blue\", lty = \"dotted\")\n\n\n\n\n\n\n\n\nAlternatively, one can perform parallel analysis to identify statistically significant PCs whose variances are “larger than the 95% quantile […] of those obtained from random or resampled data” (Mair 2018: 31). The corresponding function is fa.parallel() from the psych package.\n\npca.pa &lt;- fa.parallel(scope_sem_sub[,-1], # raw data\n                     fa = \"pc\", # Use PCA instead of factor analysis\n                     cor = \"cor\",  # Use Pearson correlations (default for PCA)\n                     n.iter = 200, # Number of iterations (increase for more stable results)\n                     quant = 0.95, # Use 95th percentile (common choice)\n                     fm = \"minres\") # Factor method\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  3 \n\n\n\n\nAccessing and visualising the loadings\nSince three PCs appear to be enough to explain the majority of variance in the data, we will refit the model with nfactors = 3.\n\npca2 &lt;- principal(scope_sem_sub[,-1],\n                  nfactors = 3,\n                  rotate = \"none\")\n\nA convenient function for printing the PCA loadings is loadings(). Weights close to \\(0\\) are not displayed.\n\nloadings(pca2)\n\n\nLoadings:\n                   PC1    PC2    PC3   \nResnik_strength            0.666 -0.271\nConc_Brys           0.813  0.210 -0.173\nNsenses_WordNet     0.523 -0.696  0.124\nNmeanings_Websters  0.493 -0.683  0.149\nVisual_Lanc         0.691  0.168 -0.236\nAuditory_Lanc      -0.388 -0.228  0.199\nHaptic_Lanc         0.728  0.113       \nOlfactory_Lanc      0.324  0.444  0.671\nGustatory_Lanc      0.256  0.377  0.759\nInteroceptive_Lanc -0.341 -0.164  0.577\n\n                 PC1   PC2   PC3\nSS loadings    2.629 1.898 1.595\nProportion Var 0.263 0.190 0.160\nCumulative Var 0.263 0.453 0.612\n\n\nIn order to see what features load particularly strongly on the PCs, we can draw a path diagram with diagram(). Note that the red arrows indicate negative weights (i.e., negative “regression coefficients”).\n\ndiagram(pca2, main = NA)\n\n\n\n\n\n\n\n\nThe generic plot method returns a scatterplot of the loadings:\n\nplot(pca2, labels = colnames(scope_sem_sub[,-1]), main = NA)\n\n\n\n\n\n\n\n\nFinally, you can obtain the PC scores for each observation in the input data by accessing the $scores element:\n\nhead(pca2$scores, n = 15)\n\n              PC1         PC2         PC3\n [1,] -1.45999990  0.38323657  0.61549383\n [2,] -0.32170158 -0.60027352 -0.08271852\n [3,]  0.12196548 -0.68757984  0.33959072\n [4,] -0.57929327 -0.35785887  0.26877126\n [5,] -0.34097381 -1.35963060  0.54717808\n [6,] -0.04799048 -0.34404820 -0.19668070\n [7,] -0.33873248  0.52372694 -0.28318588\n [8,] -1.11868861  0.26178424 -0.66465979\n [9,]  0.15263031  0.60489417 -0.84324699\n[10,] -1.75834143 -0.47957110  0.44313029\n[11,] -1.26440095 -1.15766536  0.46594800\n[12,]  0.10641410  0.05075197  0.48556702\n[13,] -1.26133394 -0.35022899 -0.36512925\n[14,] -0.28070472  0.60992380 -1.29547347\n[15,] -0.72805598 -0.45777808  0.56031788\n\n\nBiplots offer juxtaposed visualisations of PC scores (points) and loadings (arrows).\n\n# PC1 and PC2\nbiplot(pca2, choose = c(1, 2), \n       main = NA, pch = 20, col = c(\"darkgrey\", \"blue\"))\n\n\n\n\n\n\n\n# PC2 and PC3\nbiplot(pca2, choose = c(2, 3), \n       main = NA, pch = 20, col = c(\"darkgrey\", \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the PCA output\n\n\n\nAfter inspecting the loadings and biplots, we can see the following patterns:\n\nExternal sensation: Higher ratings in concreteness (i.e., direct perception with one’s senses) as well as the visual and haptic dimensions of verbs are associated with an increase in PC1.\nSenses and selection: PC2 displays notable negative loadings in features relating to the number of meanings a verb has and how much information it carries about the meaning of its objects. PC2 scores decrease if a verb has fewer meanings, but they increase if it displays higher selectional preference strength.\nInternal sensation: PC3 captures variance in olfactory, gustatory and interoceptive2 ratings.\n\n2 Here interoceptive means “[t]o what extent one experiences the referent by sensations inside one’s body” (Gao, Shinkareva, and Desai 2022: 2859).",
    "crumbs": [
      "7. Machine Learning",
      "7.3 PCA"
    ]
  },
  {
    "objectID": "chapters/04-categorical_data.html",
    "href": "chapters/04-categorical_data.html",
    "title": "4.3 Categorical data",
    "section": "",
    "text": "Script\n\n\n\nYou can find the full R script associated with this unit here.\n\n\nPlease download the file Paquot_Larsson_2020_data.xlsx (Paquot and Larsson 2020)1 and store it in the same folder as your currently active R-script. Then run the code lines below:\n1 The original supplementary materials can be downloaded from the publisher’s website [Last accessed April 28, 2024].\n# Libraries\nlibrary(readxl)\nlibrary(tidyverse)\n# For publication-ready tables\nlibrary(crosstable)\nlibrary(flextable)\n\n# Load data from working directory\ncl.order &lt;- read_xlsx(\"../datasets/Paquot_Larsson_2020_data.xlsx\")\n\n# Check the structure of the data frame\nhead(cl.order)",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.3 Categorical data"
    ]
  },
  {
    "objectID": "chapters/04-categorical_data.html#preparation",
    "href": "chapters/04-categorical_data.html#preparation",
    "title": "4.3 Categorical data",
    "section": "",
    "text": "Script\n\n\n\nYou can find the full R script associated with this unit here.\n\n\nPlease download the file Paquot_Larsson_2020_data.xlsx (Paquot and Larsson 2020)1 and store it in the same folder as your currently active R-script. Then run the code lines below:\n1 The original supplementary materials can be downloaded from the publisher’s website [Last accessed April 28, 2024].\n# Libraries\nlibrary(readxl)\nlibrary(tidyverse)\n# For publication-ready tables\nlibrary(crosstable)\nlibrary(flextable)\n\n# Load data from working directory\ncl.order &lt;- read_xlsx(\"../datasets/Paquot_Larsson_2020_data.xlsx\")\n\n# Check the structure of the data frame\nhead(cl.order)",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.3 Categorical data"
    ]
  },
  {
    "objectID": "chapters/04-categorical_data.html#describing-categorical-data",
    "href": "chapters/04-categorical_data.html#describing-categorical-data",
    "title": "4.3 Categorical data",
    "section": "Describing categorical data",
    "text": "Describing categorical data\nA categorical variable is made up of two or more discrete values. An intuitive way to describe categorical data would be to count how often each category occurs in the sample. These counts are then typically summarised in frequency tables and accompanied by suitable graphs (e.g., barplots).\n\nFrequency tables (one variable)\nAssume we are interested in how often each clause ordering type ( \"mc-sc\" vs. \"sc-mc\") is attested in our data. In R, we can obtain their frequencies by inspecting the ORDER column of the cl.order dataset. Since manual counting isn’t really an option, we will make use of the convenient functions table() and xtabs().\n\n\n\n\n\n\nThe workhorse: table()\n\n\n\n\n\nThis function requires a character vector. We use the notation cl.order$ORDER to subset the cl.order data frame according to the column ORDER (cf. data frames). We store the results in the variable order_freq1 (you may choose a different name if you like) and display the output by applying to it the print() function.\n\n# Count occurrences of ordering types (\"mc-sc\" and \"sc-mc\") in the data frame\norder_freq1 &lt;- table(cl.order$ORDER) \n\n# Print table\nprint(order_freq1)\n\n\nmc-sc sc-mc \n  275   128 \n\n\n\n\n\n\n\n\n\n\n\nMore detailed: xtabs()\n\n\n\n\n\nAlternatively, you could use xtabs() to achieve the same result. The syntax is a little different, but it returns a slightly more more detailed table with explicit variable label(s).\n\n# Count occurrences of ordering types (\"mc-sc\" and \"sc-mc\")\norder_freq2 &lt;- xtabs(~ ORDER, cl.order)\n\n# Print table\nprint(order_freq2)\n\nORDER\nmc-sc sc-mc \n  275   128 \n\n\n\n\n\n\n\nFrequency tables (\\(\\geq\\) 2 variables)\nIf we are interested in the relationship between multiple categorical variables, we can cross-tabulate the frequencies of their categories. For example, what is the distribution of clause order depending on the type of subordinate clause? The output is also referred to as a contingency table.\n\n\n\n\n\n\nThe table() way\n\n\n\n\n\n\n# Get frequencies of ordering tpyes (\"mc-sc\" vs. \"sc-mc\") depending on the type of subordinate clause (\"caus\" vs. \"temp\")\norder_counts1 &lt;- table(cl.order$ORDER, cl.order$SUBORDTYPE)\n\n# Print contingency table\nprint(order_counts1)\n\n       \n        caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n\n\n\n\n\n\n\n\n\n\n\nThe xtabs() way\n\n\n\n\n\n\n# Cross-tabulate ORDER and SUBORDTYPE\norder_counts2 &lt;- xtabs(~ ORDER + SUBORDTYPE, cl.order)\n\n# Print cross-table\nprint(order_counts2)\n\n       SUBORDTYPE\nORDER   caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n\n\n\n\n\n\n\nPercentage tables\nThere are several ways to compute percentages for your cross-tables, but by far the simplest is via the prop.table() function. As it only provides proportions, you can multiply the output by 100 to obtain real percentages.\n\n\n\n\n\n\nGet percentages for a table() object\n\n\n\n\n\n\n# Convert to % using the prop.table() function\npct1 &lt;- prop.table(order_counts1) * 100\n\n# Print percentages\nprint(pct1)\n\n       \n             caus      temp\n  mc-sc 45.657568 22.580645\n  sc-mc  3.722084 28.039702\n\n\n\n\n\n\n\n\n\n\n\nGet percentages for an xtabs() object\n\n\n\n\n\n\n# Convert to % using the prop.table() function\npct2 &lt;- prop.table(order_counts2) * 100\n\n# Print percentages\nprint(pct2)\n\n       SUBORDTYPE\nORDER        caus      temp\n  mc-sc 45.657568 22.580645\n  sc-mc  3.722084 28.039702\n\n\nNotice how pct2 still carries the variable labels SUBORDTYPE and ORDER, which is very convenient.",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.3 Categorical data"
    ]
  },
  {
    "objectID": "chapters/04-categorical_data.html#plotting-categorical-data",
    "href": "chapters/04-categorical_data.html#plotting-categorical-data",
    "title": "4.3 Categorical data",
    "section": "Plotting categorical data",
    "text": "Plotting categorical data\nThis section demonstrates both the in-built plotting functions of R (‘Base R’) as well as the more modern versions provided by the tidyverse package.\n\n\n\n\n\n\nMosaicplots (raw counts)\n\n\n\n\n\nA straightforward way to visualise a contingency table is the mosaicplot:\n\n# Works with raw counts and percentages\n# Using the output of xtabs() as input\nmosaicplot(order_counts2, color = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarplots (raw counts)\n\n\n\n\n\nThe workhorse of categorical data analysis is the barplot. Base R functions usually require a table object as input, whereas ggplot2 can operate on the raw dataset.\n\nOne variable\n\nBase Rggplot2\n\n\n\nBase R barplot with barplot(); requires the counts as computed by tables() or xtabs()\n\n\n# Generate cross-table\norder_freq1 &lt;- table(cl.order$ORDER)\n\n# Create barplot\nbarplot(order_freq1)\n\n\n\n\n\n\n\n\n\n\n\nBarplot with geom_bar() using the raw input data\n\n\n# Requirement: library(tidyverse)\n\n# Raw input data\nhead(cl.order)\n\n# A tibble: 6 × 8\n   CASE ORDER SUBORDTYPE LEN_MC LEN_SC LENGTH_DIFF CONJ     MORETHAN2CL\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      \n1  4777 sc-mc temp            4     10          -6 als/when no         \n2  1698 mc-sc temp            7      6           1 als/when no         \n3   953 sc-mc temp           12      7           5 als/when yes        \n4  1681 mc-sc temp            6     15          -9 als/when no         \n5  4055 sc-mc temp            9      5           4 als/when yes        \n6   967 sc-mc temp            9      5           4 als/when yes        \n\n# Create barplot\nggplot(cl.order, aes(x = ORDER)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo variables\nBivariate barplots can be obtained by either supplying a contingency table (Base R) or by mapping the second variable onto the fill argument using the raw data.\n\nBase RBase R (fully customised)ggplot2ggplot2 (fully customised)\n\n\n\n# Generate cross-table with two variables\norder_counts2 &lt;- xtabs(~ ORDER + SUBORDTYPE, cl.order)\n\n# Create simple barplot\nbarplot(order_counts2, \n        beside = TRUE,  # Make bars side-by-side\n        legend = TRUE)  # Add a legend\n\n\n\n\n\n\n\n\n\n\n\n# Generate cross-table with two variables\norder_counts2 &lt;- xtabs(~ ORDER + SUBORDTYPE, cl.order)\n\n# Customise barplot with axis labels, colours and legend\nbarplot(order_counts2, \n        beside = TRUE,  # Make bars dodged (i.e., side by side)\n        main = \"Distribution of ORDER by SUBORDTYPE (Base R)\", \n        xlab = \"ORDER\", \n        ylab = \"Frequency\", \n        col = c(\"lightblue\", \"lightgreen\"), # Customize colors\n        legend = TRUE,  # Add a legend\n        args.legend = list(title = \"SUBORDTYPE\", x = \"topright\"))\n\n\n\n\n\n\n\n\n\n\n\n# Requirement: library(tidyverse)\n\n# Create simple barplot with the ggplot() function\nggplot(cl.order, aes(x = ORDER, fill = SUBORDTYPE)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\n# Requirement: library(tidyverse)\n\n# Fully customised ggplot2 object\nggplot(cl.order, aes(x = ORDER, fill = SUBORDTYPE)) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    title = \"Clause order by subordinate clause type\",\n    x = \"Clause order\",\n    y = \"Frequency\",\n    fill = \"Type of subordinate clause\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarplots (percentages)\n\n\n\n\n\n\nBase Rggplot2\n\n\nIn very much the same way as with the raw counts:\n\n# Create simple barplot with a percentage table as input\nbarplot(pct1, \n        beside = TRUE,  # Make bars side-by-side\n        legend = TRUE)  # Add a legend\n\n\n\n\n\n\n\n\n\n\nHere, a few tweaks are necessary. Because the ggplot() function prefers to works with data frames rather than cross-tables, we’ll have to coerce it into one first:\n\n# Convert a percentage table to a data frame\n# My recommendation: Use the pct2 object, which was generated using xtabs() because it will keep the variable names\npct2_df &lt;- as.data.frame(pct2)\n\nprint(pct2_df)\n\n  ORDER SUBORDTYPE      Freq\n1 mc-sc       caus 45.657568\n2 sc-mc       caus  3.722084\n3 mc-sc       temp 22.580645\n4 sc-mc       temp 28.039702\n\n\nNow we can plot the percentages with geom_col(). This geom (= ‘geometric object’) allows us to manually specify what should be mapped onto the y-axis:\n\n# Requirement: library(tidyverse)\n\n# Create barplot with user-defined y-axis, which requires geom_col() rather than geom_bar()\nggplot(pct2_df, aes(x = ORDER, y = Freq, fill = SUBORDTYPE)) +\n  geom_col(position = \"dodge\") +\n  labs(y = \"Frequency (in %)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBubble plot (percentages)\n\n\n\n\n\n\n# Requirement: library(tidyverse)\n\n# Bubble plot\nggplot(pct2_df, aes(x = ORDER, y = SUBORDTYPE, size = Freq)) +\n  geom_point(color = \"skyblue\", alpha = 0.7) +\n  scale_size_continuous(range = c(5, 20)) +  # Adjust bubble size range\n  labs(title = \"Bubble Plot of ORDER by SUBORDTYPE\",\n       x = \"ORDER\",\n       y = \"SUBORDTYPE\",\n       size = \"Percentage\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlluvial plot (percentages)\n\n\n\n\n\n\n# Make sure to install this library prior to running the code below \nlibrary(ggalluvial)\n\nggplot(pct2_df,\n       aes(axis1 = ORDER, axis2 = SUBORDTYPE, y = Freq)) +\n  geom_alluvium(aes(fill = ORDER)) +\n  geom_stratum(fill = \"gray\") +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  labs(title = \"Alluvial Plot of ORDER by SUBORDTYPE\",\n       x = \"Categories\", y = \"Percentage\") +\n  theme_minimal()",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.3 Categorical data"
    ]
  },
  {
    "objectID": "chapters/04-categorical_data.html#exporting-tables-to-ms-word",
    "href": "chapters/04-categorical_data.html#exporting-tables-to-ms-word",
    "title": "4.3 Categorical data",
    "section": "Exporting tables to MS Word",
    "text": "Exporting tables to MS Word\nThe crosstable and flextable packages make it very easy to export elegant tables to MS Word.\n\n\n\n\n\n\nClean and to the point: crosstable()\n\n\n\n\n\nThis is perhaps the most elegant solution. Generate a crosstable() object by supplying at the very least …\n\nthe original dataset (data = cl.order),\nthe dependent variable (cols = ORDER), and\nthe independent variable (by = SUBORDTYPE).\n\nYou can further specify …\n\nwhether to include column totals, row totals or both (here: total = both),\nthe rounding scheme (here: percent_digits = 2),\n…\n\n\n# Required libraries:\n# library(crosstable)\n# library(flextable)\n\n# Create the cross table\noutput1 &lt;- crosstable(data = cl.order,\n                      cols = ORDER, \n                      by = SUBORDTYPE, \n                      total = \"both\",\n                      percent_digits = 2)\n\n# Generate file\nas_flextable(output1)\n\nlabelvariableSUBORDTYPETotalcaustempORDERmc-sc184 (66.91%)91 (33.09%)275 (68.24%)sc-mc15 (11.72%)113 (88.28%)128 (31.76%)Total199 (49.38%)204 (50.62%)403 (100.00%)\n\n\n\n\n\n\n\n\n\n\n\nHow much info do you need? Yes.\n\n\n\n\n\nIt also possible to use as_flextable() without pre-processing the data with crosstable(); supplying a table preferably created with xtabs() is sufficient. Without any doubt, the output is extremely informative, yet it is everything but reader-friendly.\nFor this reason, I recommend relying on the less overwhelming crosstable() option above if a plain and easy result is desired. However, readers who would like to leverage the full capabilities of the flextable() package and familiarise themselves with the abundant options for customisation, can find the detailed documentation here.\n\n# Requires the following library:\n# library(flextable)\n\n# Create a table\ntab1 &lt;- xtabs(~ ORDER + SUBORDTYPE, cl.order)\n\n# Directly convert a table to a flextable with as_flextable()\noutput_1 &lt;- as_flextable(tab1)\n\n# Print output\nprint(output_1)",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.3 Categorical data"
    ]
  },
  {
    "objectID": "chapters/04-categorical_data.html#workflow-exercises",
    "href": "chapters/04-categorical_data.html#workflow-exercises",
    "title": "4.3 Categorical data",
    "section": "Workflow exercises",
    "text": "Workflow exercises\n\n\n\n\n\n\nSolutions\n\n\n\nYou can find the solutions to the exercises here.\n\n\n\nExercise 1 Download the dataset objects.xlsx from https://osf.io/j2mnx. Load it into R and store it in a variable objects. Make sure to load all the necessary libraries.\n\n\nExercise 2 Many rows from are irrelevant for the analysis. Exclude all rows from that are marked as containing passive clauses (see Clause_voice column). Store this reduced subset in a new variable objects_filtered.\n\n\nExercise 3 Investigate the relationship between\n\nObject_realisation and Register as well as\nObject_realisation and Lemma\n\nby computing frequency tables and percentages based on objects_filtered. Plot your results and export your tables and figures to a Microsoft Word document.\n\n\nExercise 4 Which verb has the highest \\(\\frac{\\text{null}}{\\text{null} + \\text{overt}}\\) ratio and which one has the lowest?",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.3 Categorical data"
    ]
  },
  {
    "objectID": "chapters/02-vectors.html",
    "href": "chapters/02-vectors.html",
    "title": "2.3 Vectors",
    "section": "",
    "text": "Script\n\n\n\nYou can find the full R script associated with this unit here.",
    "crumbs": [
      "2. Introduction to R",
      "2.3 Vectors"
    ]
  },
  {
    "objectID": "chapters/02-vectors.html#preparation",
    "href": "chapters/02-vectors.html#preparation",
    "title": "2.3 Vectors",
    "section": "",
    "text": "Script\n\n\n\nYou can find the full R script associated with this unit here.",
    "crumbs": [
      "2. Introduction to R",
      "2.3 Vectors"
    ]
  },
  {
    "objectID": "chapters/02-vectors.html#recommended-reading",
    "href": "chapters/02-vectors.html#recommended-reading",
    "title": "2.3 Vectors",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nWinter (2020): Chapter 1.1–1.9\n\nSuggested video tutorial:\n\nHow to Create and Name Vectors in R (DataCamp; 5min)",
    "crumbs": [
      "2. Introduction to R",
      "2.3 Vectors"
    ]
  },
  {
    "objectID": "chapters/02-vectors.html#word-frequencies-i",
    "href": "chapters/02-vectors.html#word-frequencies-i",
    "title": "2.3 Vectors",
    "section": "Word frequencies I",
    "text": "Word frequencies I\nIn usage-based linguistics, it is very common to work with word frequency data of the kind shown in Table 1.\n\n\n\n\nTable 1: Verb lemma frequencies\n\n\n\n\n\n\nVerb\nFrequency\n\n\n\n\nstart\n418\n\n\nenjoy\n139\n\n\nbegin\n337\n\n\nhelp\n281\n\n\n\n\n\n\n\n\n\n\nWhile this table is relatively small and easy to interpret, it is usually a good idea to supply readers with a simple visual representation as well. The more complex the data becomes, the greater the value of clear visualisations will be. When dealing with counts of distinct categories as it is the case here, we can draw on the primary workhorse of categorical data analysis – the barplot.\n\nStoring data in R\nTo create a two-dimensional barplot, we will first need to generate two objects in R: one for the individual lemmas (\\(x\\)-axis) and one for the frequency counts (\\(y\\)-axis).\nFirst, let’s combine lemmas start, enjoy, begin and help into a single virtual object using R’s c() function, which you can read as ‘concatenate’ or ‘combine’. We will call this new object lemma. Enter the following line into a new R script and click on Run (or simply press Ctrl+Enter/Cmd+Enter).\n\nlemma &lt;- c(\"start\", \"enjoy\", \"begin\", \"help\")\n\nTo make sure everything is working as intended, we can apply the print() function to the lemma object, in order to display all the elements contained within to the console:\n\nprint(lemma)\n\n[1] \"start\" \"enjoy\" \"begin\" \"help\" \n\n\nNaturally, we can also combine numeric data with c():\n\nfrequency &lt;- c(418, 139, 337, 281)\n\nOnce again, the print() functions allows us to inspect the contents of frequency:\n\nprint(frequency)\n\n[1] 418 139 337 281\n\n\n\n\n\n\n\n\nWhen do I use quotation marks?\n\n\n\n\n\nLetters and numbers represent two distinct data types in R. Anything that should be understood as a simple sequence of letters must be enclosed by quotation marks \"...\". A linguistic item such as start will be will be evaluated as a string if it’s encoded as \"start\".\nNumbers (or integers), by contrast, appear without quotation marks.\n\n\n\nOur linguistic data is now stored in two variables lemma and frequency, which you can conceptualise as virtual container-like objects. These ‘containers’ are now showing in the Environment tab in the top right corner of your RStudio interface (cf. Figure 1).\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nCreating the barplot\nA function in R can take one or more arguments to which it will be applied. R’s most basic barplot function (which is, unsurprisingly, called barplot()) needs at the very least …\n\na height argument, i.e., our y-axis values and\na names.arg argument, i.e., our x-axis labels.\n\nThe function arguments must be enclosed by parentheses and separated by commas:\n\nbarplot(frequency, names.arg = lemma)\n\n\n\n\n\n\n\n\nThis plotting function supports several additional arguments which can be used to customise the plot further. Typing ?barplot into the console opens a (mildy overwhelming) help tab that offers a detailed breakdown of all customisation options. After some tinkering, our barplot looks more presentable:\n\nbarplot(frequency, names.arg = lemma, \n        main = \"Frequency of Lemmas\", # title\n        xlab = \"Lemmas\",  # label for x-axis\n        ylab = \"Frequency\", # label for y-axis\n        col = \"steelblue\") # color\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does ‘#’ mean? On comments in R\n\n\n\n\n\nIn R, everything followed by the hashtag # will be interpreted as a comment and won’t be evaluated by the R compiler. While comments don’t affect the output of our code in the slightest, they are crucial to any kind of programming project.\nAdding prose annotations will make your code not only easier to understand for others but also for your future self. Poor documentation is a common, yet unnecessary source of frustration for all parties involved …\n\n\n\nIn RStudio, you now have the option to save the plot to your computer. Once the figure has appeared in your “Plots” panel, you can click on “Export” in the menu bar below and proceed to choose the desired output format and file directory.\n\n\nEssential R concepts\nThe example above demonstrates one of the most important data structures in R: vectors. They form the cornerstone of various more complex objects such as data frames, and are essential to handling large data sets (e.g., corpora). And yet, vectors are very simple in that they are merely one-dimensional sequences of characters or numbers — no more, no less.\n\nprint(lemma)\n\n[1] \"start\" \"enjoy\" \"begin\" \"help\" \n\nprint(frequency)\n\n[1] 418 139 337 281\n\n\nThe individual elements in these two vectors are not randomly jumbling around in virtual space, but are in fact following a clear order. Each element comes with an “ID” (or index), by which it can be accessed. For example, if we want to print the first lemma in our lemma variable, we append square brackets [ ] to it. This will allow us to subset it.\n\nlemma[1]\n\n[1] \"start\"\n\n\nSimilarly, we can subset frequency according to, for example, its third element:\n\nfrequency[3]\n\n[1] 337\n\n\nIt is also possible to obtain entire ranges of elements, such as everything from the second to the fourth element:\n\nfrequency[2:4]\n\n[1] 139 337 281\n\n\nTo check the number of elements in a vector, we use length():\n\nlength(lemma)\n\n[1] 4\n\nlength(frequency)\n\n[1] 4",
    "crumbs": [
      "2. Introduction to R",
      "2.3 Vectors"
    ]
  },
  {
    "objectID": "chapters/02-vectors.html#tier-1",
    "href": "chapters/02-vectors.html#tier-1",
    "title": "2.3 Vectors",
    "section": "Tier 1",
    "text": "Tier 1\n\nExercise 1 Create a vector that lists the third person personal pronouns of English (subject and object forms). Store them in a variable pp3.\n\n\nExercise 2 Now print …\n\n… the fourth element in pp3.\n… elements 3 through 5.\n… all elements.\n… elements 1, 3 and 5.",
    "crumbs": [
      "2. Introduction to R",
      "2.3 Vectors"
    ]
  },
  {
    "objectID": "chapters/02-vectors.html#tier-2",
    "href": "chapters/02-vectors.html#tier-2",
    "title": "2.3 Vectors",
    "section": "Tier 2",
    "text": "Tier 2\n\nExercise 3 When working with large datasets, we often do not know whether an element is in the vector to begin with, let alone its position. For instance, if we wanted to check whether they is in pp3 or not, we could use the handy notation below, which returns a TRUE or FALSE value:\n\n\"they\" %in% pp3\n\nAscertain whether the following items are in pp3:\n\nhim\nyou\nit and them\nwe, us and me\n\n\n\nExercise 4 Once we are sure that an element is in the vector of interest, another common problem that arises is finding its location. In this case, we can use which() to return the index of an element.\n\nwhich(pp3 == \"they\")\n\nYou can read the code above as “Which element in pp3 is they?”. Note that the index number depends on the order of elements you’ve chosen when creating pp3. Find the positions of it and them in pp3!\n\n\nExercise 5 Consider the vector numbers.\n\nnumbers &lt;- c(500:1000)\n\n\nExplain the difference in output for the following two code lines:\n\n\nwhich(numbers &gt; 600)\n\n\nnumbers[which(numbers &gt; 600)]\n\n\nExamine the output of the code chunks below, and try to establish the meaning of the operators !=, &, and |.\n\n\nnumbers[numbers != 500]\n\n\nnumbers[numbers &gt; 500 & numbers &lt; 550]\n\n\nnumbers[numbers &lt; 510 | numbers &gt; 990]",
    "crumbs": [
      "2. Introduction to R",
      "2.3 Vectors"
    ]
  },
  {
    "objectID": "chapters/02-vectors.html#tier-3",
    "href": "chapters/02-vectors.html#tier-3",
    "title": "2.3 Vectors",
    "section": "Tier 3",
    "text": "Tier 3\n\nExercise 6 Consider our frequency data again. When analysing linguistic patterns, we often need to transform our data. Starting with:\n\nlemma &lt;- c(\"start\", \"enjoy\", \"begin\", \"help\")\nfrequency &lt;- c(418, 139, 337, 281)\n\n\nCreate a new vector relative_freq that shows each frequency as a percentage of the total.\nCreate a vector log_freq containing the natural logarithm of each frequency value.\nSuppose these frequencies come from a corpus of 10,000 words. Create a vector freq_per_thousand that shows how many times each verb appears per 1,000 words.\n\n\n\nExercise 7 The function strsplit() can be used to split up a string into substrings.\n\nwords &lt;- c(\"read\", \"write\")\n\nsplit_words &lt;- strsplit(words, split = NULL)\n\nprint(split_words)\n\n[[1]]\n[1] \"r\" \"e\" \"a\" \"d\"\n\n[[2]]\n[1] \"w\" \"r\" \"i\" \"t\" \"e\"\n\n\nHere the output is a list with two objects enclosed by double square brackets ([[1]] and [[2]]). These can be used to subset split_words:\n\n# Get first list object\nsplit_words[[1]]\n\n[1] \"r\" \"e\" \"a\" \"d\"\n\n# Get second list object\nsplit_words[[2]]\n\n[1] \"w\" \"r\" \"i\" \"t\" \"e\"\n\n\nWrite an R script that identifies the number of (orthographic) vowels and consonants for the words consequence and parsimonious.\n\n\nExercise 8 R allows users to write their own functions in order to automate certain tasks. Say, we need a function that automatically computes percentages for a numeric vector. If we break down the individual steps, it has to\n\ntake a numeric vector as input,\ncompute the sum of its values,\ndivide the vector values by the sum to get relative frequencies.\nmultiply the relative frequencies by 100, and\nreturn the final vector with the percentages.\n\n\n# 1. Define function with input\nget_pct &lt;- function(vector) {\n\n  # 2. Compute sum\n  sum_values &lt;- sum(vector)\n  \n  # 3. Divide vector values by the sum\n  relative_freqs &lt;- vector/sum_values\n  \n  # 4. Multiply relative frequencies by 100\n  percentages &lt;- relative_freqs * 100\n  \n  # 5. Return the percentages\n  return(percentages)\n}\n\nNow we can apply it to a numeric vector of our choice:\n\nget_pct(frequency)\n\n[1] 35.57447 11.82979 28.68085 23.91489\n\n\nWrite a count_vowels() function and a count_consonants() function that immediately computes the number of vowels or consonants, respectively, for any word supplied.",
    "crumbs": [
      "2. Introduction to R",
      "2.3 Vectors"
    ]
  },
  {
    "objectID": "chapters/01-basics.html",
    "href": "chapters/01-basics.html",
    "title": "1.1 Basics",
    "section": "",
    "text": "New quotatives in American English: be like, go, be all:\n\n“I was like ‘well if it’s not leaking, I don’t care.’”\n“Do you, I asked I was running out and I go, ‘you’re trying to deliver a pizza here?’”\n“My sister’s all ‘excuse me would you mind if I gave you, if I want your autograph.’”\n\n\n\n\nPrevious results with regard to age: Be like is known to be preferred by younger speakers.\nPrevious results with regard to sex are inconclusive.\n\n\n\n\n\n“The study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all” (Barbieri 2007: 26).\n\n\n\n\n\nChoice of corpus: 450,000 words taken from the Conversation component of the Longman Spoken and Written English Corpus (all files where the age and gender of speakers were available), i.e., about 50 hours of casual conversation coded for speakers’ age and sex.\nData extraction: All forms of be like, be all, go, say extracted using corpus linguistic software; data entered into spreadsheet software; word counts of texts added via Delphi program.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuotative\nAge group\nMale use (per 100,000 words)\nFemale use (per 100,000 words)\n\n\n\n\nsay\n16–26\n115\n149\n\n\n\n27–40\n101\n117\n\n\n\n41–54\n55\n214\n\n\n\n55+\n67\n433\n\n\nbe like\n16–26\n32\n329\n\n\n\n27–40\n66\n25\n\n\n\n41–54\n-\n1\n\n\n\n55+\n-\n19\n\n\ngo\n16–26\n25\n113\n\n\n\n27–40\n61\n18\n\n\n\n41–54\n-\n15\n\n\n\n55+\n-\n7\n\n\nbe all\n16–26\n14\n33\n\n\n\n\n\n\n\n\n\nReproduction of Barbieri’s Figure 5 ‘Proportional quotative use by men and women aged 16–26 and 27–40’\n\n\n\n\n\n\n\n“In sum, the patterns of proportional use and the results of the statistical analyses show that there are striking differences in the way that men and women under the age of forty use quotative verbs. In contrast, in conforming to the traditional quotative say, men and women over forty displayed similar behavior” (Barbieri 2007: 38–39).\n\n\n\n\n\n“In favoring be like over other quotatives, men in their late 20s and in their 30s display an ‘affinity’ with slightly younger women, women in their early 20s, rather than with women of their same age” (Barbieri 2007: 41).\n\n\n\n\n\n\nObjectivity — Independence from researchers or devices (→ replicability!)\nReliability — Studies should be replicable.\nValidity — A study must actually address the problem formulated in the research question.\n\n\n\n\n\nExercise 1 As you read through the sections above, think about what might go wrong at each stage of the study!\n\n\nExercise 2 Read another sociolinguistic study (e.g., Unuabonah and Gut 2018). As you read, identify similar key sections or “building blocks” used in this study (e.g., introduction, research question etc.). What purpose does each section serve in the study’s overall structure?",
    "crumbs": [
      "1. Fundamentals",
      "1.1 Basics"
    ]
  },
  {
    "objectID": "chapters/01-basics.html#defining-the-variable",
    "href": "chapters/01-basics.html#defining-the-variable",
    "title": "1.1 Basics",
    "section": "",
    "text": "New quotatives in American English: be like, go, be all:\n\n“I was like ‘well if it’s not leaking, I don’t care.’”\n“Do you, I asked I was running out and I go, ‘you’re trying to deliver a pizza here?’”\n“My sister’s all ‘excuse me would you mind if I gave you, if I want your autograph.’”\n\n\n\n\nPrevious results with regard to age: Be like is known to be preferred by younger speakers.\nPrevious results with regard to sex are inconclusive.\n\n\n\n\n\n“The study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all” (Barbieri 2007: 26).\n\n\n\n\n\nChoice of corpus: 450,000 words taken from the Conversation component of the Longman Spoken and Written English Corpus (all files where the age and gender of speakers were available), i.e., about 50 hours of casual conversation coded for speakers’ age and sex.\nData extraction: All forms of be like, be all, go, say extracted using corpus linguistic software; data entered into spreadsheet software; word counts of texts added via Delphi program.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuotative\nAge group\nMale use (per 100,000 words)\nFemale use (per 100,000 words)\n\n\n\n\nsay\n16–26\n115\n149\n\n\n\n27–40\n101\n117\n\n\n\n41–54\n55\n214\n\n\n\n55+\n67\n433\n\n\nbe like\n16–26\n32\n329\n\n\n\n27–40\n66\n25\n\n\n\n41–54\n-\n1\n\n\n\n55+\n-\n19\n\n\ngo\n16–26\n25\n113\n\n\n\n27–40\n61\n18\n\n\n\n41–54\n-\n15\n\n\n\n55+\n-\n7\n\n\nbe all\n16–26\n14\n33\n\n\n\n\n\n\n\n\n\nReproduction of Barbieri’s Figure 5 ‘Proportional quotative use by men and women aged 16–26 and 27–40’\n\n\n\n\n\n\n\n“In sum, the patterns of proportional use and the results of the statistical analyses show that there are striking differences in the way that men and women under the age of forty use quotative verbs. In contrast, in conforming to the traditional quotative say, men and women over forty displayed similar behavior” (Barbieri 2007: 38–39).\n\n\n\n\n\n“In favoring be like over other quotatives, men in their late 20s and in their 30s display an ‘affinity’ with slightly younger women, women in their early 20s, rather than with women of their same age” (Barbieri 2007: 41).",
    "crumbs": [
      "1. Fundamentals",
      "1.1 Basics"
    ]
  },
  {
    "objectID": "chapters/01-basics.html#principles-of-empirical-linguistics",
    "href": "chapters/01-basics.html#principles-of-empirical-linguistics",
    "title": "1.1 Basics",
    "section": "",
    "text": "Objectivity — Independence from researchers or devices (→ replicability!)\nReliability — Studies should be replicable.\nValidity — A study must actually address the problem formulated in the research question.",
    "crumbs": [
      "1. Fundamentals",
      "1.1 Basics"
    ]
  },
  {
    "objectID": "chapters/01-basics.html#exercises",
    "href": "chapters/01-basics.html#exercises",
    "title": "1.1 Basics",
    "section": "",
    "text": "Exercise 1 As you read through the sections above, think about what might go wrong at each stage of the study!\n\n\nExercise 2 Read another sociolinguistic study (e.g., Unuabonah and Gut 2018). As you read, identify similar key sections or “building blocks” used in this study (e.g., introduction, research question etc.). What purpose does each section serve in the study’s overall structure?",
    "crumbs": [
      "1. Fundamentals",
      "1.1 Basics"
    ]
  },
  {
    "objectID": "chapters/02-libraries.html",
    "href": "chapters/02-libraries.html",
    "title": "2.5 Libraries",
    "section": "",
    "text": "Winter (2020): Chapter 1.13",
    "crumbs": [
      "2. Introduction to R",
      "2.5 Libraries"
    ]
  },
  {
    "objectID": "chapters/02-libraries.html#recommended-reading",
    "href": "chapters/02-libraries.html#recommended-reading",
    "title": "2.5 Libraries",
    "section": "",
    "text": "Winter (2020): Chapter 1.13",
    "crumbs": [
      "2. Introduction to R",
      "2.5 Libraries"
    ]
  },
  {
    "objectID": "chapters/02-libraries.html#working-with-packages-in-r",
    "href": "chapters/02-libraries.html#working-with-packages-in-r",
    "title": "2.5 Libraries",
    "section": "Working with packages in R",
    "text": "Working with packages in R\nPackages expand the basic functionality of R by providing numerous quality-of-life improvements that not only considerably simplify common data wrangling tasks but which also provide frameworks for state-of-the-art methods for statistical analysis and natural language processing (NLP), among many other things.\n\nInstallation\n\n\n\n\n\n\nHow do I install a library?\n\n\n\n\n\nNavigate to Packages &gt; Install and verify that the pop-up window says Install from: Repository (CRAN). You can now type in the name of the package you would like to install under Packages.\nVideo tutorial on YouTube\n\n\n\nThis reader will use functions from a variety of R packages. Please install the following ones:\n\n\n\nPackage\nPurpose\nSession\n\n\n\n\nreadxl\nImporting Microsoft Excel files\nImporting/exporting\n\n\nwritexl\nExporting Microsoft Excel files\nImporting/exporting\n\n\nquanteda\nAnalysis of text data\nConcordancing\n\n\nlattice\nData visualisation\nConcordancing\n\n\ntidyverse\nFramework for data manipulation and visualisation\nCategorical data, Continuous data\n\n\ncrosstable\nCreating contingency tables\nCategorical data\n\n\nflextable\nExporting contingency tables\nCategorical data\n\n\nconfintr\nEffect size measure for categorical data\nChi-squared test\n\n\n\n\n\nLoading packages\nOnce the installation has been completed, you can proceed to load the libraries using the code below. You can ignore the warning messages.\n\nlibrary(readxl)\nlibrary(writexl)\nlibrary(quanteda)\n\nPackage version: 3.3.1\nUnicode version: 14.0\nICU version: 70.1\n\n\nParallel computing: 8 of 8 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(lattice)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(crosstable)\n\n\nAttaching package: 'crosstable'\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\nlibrary(flextable)\n\n\nAttaching package: 'flextable'\n\nThe following object is masked from 'package:purrr':\n\n    compose\n\nlibrary(confintr)\n\n\n\n\n\n\n\nActivating libraries\n\n\n\n\n\nWhenever you start a new R session (i.e., open RStudio), your libraries and their respective functions will be inactive. To re-activate a library, either use the library() function or simply select it in the Packages tab.\n\n\n\nIt is good practice to only activate those packages that are necessary for your analysis. While it won’t be a problem for the small set of packages as shown here, loading dozens of packages increases the risk of obtaining “homonymous” functions which have the same name but perform different operations. In this case, it might be helpful to “disambiguate” them by directly indicating which package a function is from:\n\nreadxl::read_xlsx(...)\n\n\n\nCiting R and R packages\nWhenever we draw on ideas other than our own, we give credit to the respective source by citing it appropriately. The same applies to R, RStudio as well as all the packages we rely on throughout our analyses.\nFor R, an up-to-date citation can be generated as follows:\n\ncitation()\n\n\nTo cite R in publications use:\n\n  R Core Team (2023). R: A language and environment for statistical\n  computing. R Foundation for Statistical Computing, Vienna, Austria.\n  URL https://www.R-project.org/.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2023},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nTo cite a specific package, simply supply the package name as an argument.\n\ncitation(\"quanteda\")\n\n\nTo cite package 'quanteda' in publications use:\n\n  Benoit K, Watanabe K, Wang H, Nulty P, Obeng A, Müller S, Matsuo A\n  (2018). \"quanteda: An R package for the quantitative analysis of\n  textual data.\" _Journal of Open Source Software_, *3*(30), 774.\n  doi:10.21105/joss.00774 &lt;https://doi.org/10.21105/joss.00774&gt;,\n  &lt;https://quanteda.io&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {quanteda: An R package for the quantitative analysis of textual data},\n    journal = {Journal of Open Source Software},\n    author = {Kenneth Benoit and Kohei Watanabe and Haiyan Wang and Paul Nulty and Adam Obeng and Stefan Müller and Akitaka Matsuo},\n    doi = {10.21105/joss.00774},\n    url = {https://quanteda.io},\n    volume = {3},\n    number = {30},\n    pages = {774},\n    year = {2018},\n  }",
    "crumbs": [
      "2. Introduction to R",
      "2.5 Libraries"
    ]
  },
  {
    "objectID": "chapters/02-exploring_rstudio.html",
    "href": "chapters/02-exploring_rstudio.html",
    "title": "2.2 Exploring RStudio",
    "section": "",
    "text": "Once you’ve opened RStudio, you will see several empty panes similar to this:\n\n\n\n\n\n\n\nLet’s focus on the console window on the left. This is where we can directly communicate with R by entering “code”. The way this code is written follows certain arbitrary conventions – just like natural languages such as English or German. Here is an example in the R programming language:\n\nprint(\"Hello world!\")\n\n[1] \"Hello world!\"\n\n\nEntering this command into your console and pressing ENTER will display the sentence “Hello world!” in a new line.\nAs we can see, anything that R should understand as a simple sequence of letters or words must be enclosed by quotation marks \"...\". Anything inside them will be interpreted as a so-called string. Their counterpart are numbers or integers, as illustrated here:\n\n2 + 2 - 1\n\n[1] 3\n\n\nNaturally, you can use R for more sophisticated computations:\n\n\\(2 + (3 \\times 3)\\)\n\n2 + (3 * 3)\n\n[1] 11\n\n\n\\(\\sqrt{9}\\)\n\nsqrt(9)\n\n[1] 3\n\n\n\\(\\frac{16}{2^3}\\)\n\n16 / 2 ^ 3\n\n[1] 2\n\n\n\n\n\n\nWhile it is certainly useful to print text or numbers to your console, it may sometimes make more sense to (at least temporally) store them somewhere, so you can re-use them later. In fact, R gives us a way of storing data in virtual, container-like objects: variables. We can assign strings or numbers to a variable by using the assignment operator &lt;-.\nWhen you run the commands below, you will (hopefully) notice two items popping up in your Environment/Workspace tab in the top right corner.\n\ngreeting &lt;- \"Hello world!\"\n\nquick_math &lt;- 2 + 2 - 1\n\nIf we now want to display the content in the console, we can simply apply the print() function to the variable:\n\nprint(greeting)\n\n[1] \"Hello world!\"\n\nprint(quick_math)\n\n[1] 3\n\n\nWe can also embed variables in other statements. For example, let’s take the content of quick_math and multiply it with 2.\n\nhard_math &lt;- quick_math * 2\n\nprint(hard_math)\n\n[1] 6\n\n\n\n\n\nWorking with the console has a very plain, yet important disadvantage: Once we close RStudio, the console is wiped clean, erasing everything we’ve typed into it during our precious R session.\nThe remedy for this nuisance are scripts. Essentially, a script is the programmer’s equivalent of a Word document: It allows you to save all the code you’ve written to a file, which you can seamlessly continue working on the next time you open it.\n\nTo create a new R script you can either go to:\n\n“File” \\(\\rightarrow\\) “New” \\(\\rightarrow\\) “R Script” or …\nclick on the icon with the + sign and select “R Script” …\nor simply press Ctrl+Shift+N (MacOS: Cmd+Shift+N)\n\n\nDon’t forget to save your script with Ctrl+S (Cmd+S)! It is good practice to save your files regularly.",
    "crumbs": [
      "2. Introduction to R",
      "2.2 Exploring R Studio"
    ]
  },
  {
    "objectID": "chapters/02-exploring_rstudio.html#the-rstudio-interface",
    "href": "chapters/02-exploring_rstudio.html#the-rstudio-interface",
    "title": "2.2 Exploring RStudio",
    "section": "",
    "text": "Once you’ve opened RStudio, you will see several empty panes similar to this:\n\n\n\n\n\n\n\nLet’s focus on the console window on the left. This is where we can directly communicate with R by entering “code”. The way this code is written follows certain arbitrary conventions – just like natural languages such as English or German. Here is an example in the R programming language:\n\nprint(\"Hello world!\")\n\n[1] \"Hello world!\"\n\n\nEntering this command into your console and pressing ENTER will display the sentence “Hello world!” in a new line.\nAs we can see, anything that R should understand as a simple sequence of letters or words must be enclosed by quotation marks \"...\". Anything inside them will be interpreted as a so-called string. Their counterpart are numbers or integers, as illustrated here:\n\n2 + 2 - 1\n\n[1] 3\n\n\nNaturally, you can use R for more sophisticated computations:\n\n\\(2 + (3 \\times 3)\\)\n\n2 + (3 * 3)\n\n[1] 11\n\n\n\\(\\sqrt{9}\\)\n\nsqrt(9)\n\n[1] 3\n\n\n\\(\\frac{16}{2^3}\\)\n\n16 / 2 ^ 3\n\n[1] 2\n\n\n\n\n\n\nWhile it is certainly useful to print text or numbers to your console, it may sometimes make more sense to (at least temporally) store them somewhere, so you can re-use them later. In fact, R gives us a way of storing data in virtual, container-like objects: variables. We can assign strings or numbers to a variable by using the assignment operator &lt;-.\nWhen you run the commands below, you will (hopefully) notice two items popping up in your Environment/Workspace tab in the top right corner.\n\ngreeting &lt;- \"Hello world!\"\n\nquick_math &lt;- 2 + 2 - 1\n\nIf we now want to display the content in the console, we can simply apply the print() function to the variable:\n\nprint(greeting)\n\n[1] \"Hello world!\"\n\nprint(quick_math)\n\n[1] 3\n\n\nWe can also embed variables in other statements. For example, let’s take the content of quick_math and multiply it with 2.\n\nhard_math &lt;- quick_math * 2\n\nprint(hard_math)\n\n[1] 6\n\n\n\n\n\nWorking with the console has a very plain, yet important disadvantage: Once we close RStudio, the console is wiped clean, erasing everything we’ve typed into it during our precious R session.\nThe remedy for this nuisance are scripts. Essentially, a script is the programmer’s equivalent of a Word document: It allows you to save all the code you’ve written to a file, which you can seamlessly continue working on the next time you open it.\n\nTo create a new R script you can either go to:\n\n“File” \\(\\rightarrow\\) “New” \\(\\rightarrow\\) “R Script” or …\nclick on the icon with the + sign and select “R Script” …\nor simply press Ctrl+Shift+N (MacOS: Cmd+Shift+N)\n\n\nDon’t forget to save your script with Ctrl+S (Cmd+S)! It is good practice to save your files regularly.",
    "crumbs": [
      "2. Introduction to R",
      "2.2 Exploring R Studio"
    ]
  },
  {
    "objectID": "chapters/04-variables.html",
    "href": "chapters/04-variables.html",
    "title": "4.1 Data, variables, samples",
    "section": "",
    "text": "Heumann, Schomaker, and Shalabh (2022): Chapters 1 & 7\nBaguley (2012): Chapter 1\nAgresti and Kateri (2022): Chapter 1.2",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.1 Data, variables, samples"
    ]
  },
  {
    "objectID": "chapters/04-variables.html#recommended-reading",
    "href": "chapters/04-variables.html#recommended-reading",
    "title": "4.1 Data, variables, samples",
    "section": "",
    "text": "Heumann, Schomaker, and Shalabh (2022): Chapters 1 & 7\nBaguley (2012): Chapter 1\nAgresti and Kateri (2022): Chapter 1.2",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.1 Data, variables, samples"
    ]
  },
  {
    "objectID": "chapters/04-variables.html#preparation",
    "href": "chapters/04-variables.html#preparation",
    "title": "4.1 Data, variables, samples",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nScript\n\n\n\nYou can find the full R script associated with this unit here.\n\n\nPlease download the file Paquot_Larsson_2020_data.xlsx (Paquot and Larsson 2020) and store it in your working directory.\n\n# Load libraries\nlibrary(readxl)\nlibrary(tidyverse)\n\n# Load data\ncl.order &lt;- read_xlsx(\"../datasets/Paquot_Larsson_2020_data.xlsx\")\n\n# Inspect data\nstr(cl.order)\nhead(cl.order)\n\n\n\n\n\n\n\nWhat’s in the file? The str() function\n\n\n\n\n\nThe easiest way to get a general overview of the full data set is to apply the str() function to the respective data frame.\n\nstr(cl.order)\n\ntibble [403 × 8] (S3: tbl_df/tbl/data.frame)\n $ CASE       : num [1:403] 4777 1698 953 1681 4055 ...\n $ ORDER      : chr [1:403] \"sc-mc\" \"mc-sc\" \"sc-mc\" \"mc-sc\" ...\n $ SUBORDTYPE : chr [1:403] \"temp\" \"temp\" \"temp\" \"temp\" ...\n $ LEN_MC     : num [1:403] 4 7 12 6 9 9 9 4 6 4 ...\n $ LEN_SC     : num [1:403] 10 6 7 15 5 5 12 2 24 11 ...\n $ LENGTH_DIFF: num [1:403] -6 1 5 -9 4 4 -3 2 -18 -7 ...\n $ CONJ       : chr [1:403] \"als/when\" \"als/when\" \"als/when\" \"als/when\" ...\n $ MORETHAN2CL: chr [1:403] \"no\" \"no\" \"yes\" \"no\" ...\n\n\nThis shows us that the data frame has 8 columns, as the $ operators indicate ($ Case, $ ORDER, …). The column names are followed by\n\nthe data type (num for numeric and chr for character strings)\nthe number of values (`[1:403]`) and\nthe first few observations.\n\nAnother intuitive way to display the structure of a data matrix is to simply show the first few rows:\n\nhead(cl.order)\n\n# A tibble: 6 × 8\n   CASE ORDER SUBORDTYPE LEN_MC LEN_SC LENGTH_DIFF CONJ     MORETHAN2CL\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      \n1  4777 sc-mc temp            4     10          -6 als/when no         \n2  1698 mc-sc temp            7      6           1 als/when no         \n3   953 sc-mc temp           12      7           5 als/when yes        \n4  1681 mc-sc temp            6     15          -9 als/when no         \n5  4055 sc-mc temp            9      5           4 als/when yes        \n6   967 sc-mc temp            9      5           4 als/when yes        \n\n\n\n\n\n\n\n\n\n\n\nFurther details on the variables\n\n\n\n\n\n\nORDER: Does the subordinate clause come before or after the main clause?\nSUBORDTYPE: Is the subordinate clause temporal or causal?\nMORETHAN2CL: Are there more clauses in the sentence than just one subordinate clause and one main clause?\nLEN_MC: How many words does the main clause contain?\nLEN_SC: How many words does the subordinate clause contain?\nLENGTH_DIFF: What is the length difference in words between the main clause and subordinate clause?",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.1 Data, variables, samples"
    ]
  },
  {
    "objectID": "chapters/04-variables.html#samples-and-populations",
    "href": "chapters/04-variables.html#samples-and-populations",
    "title": "4.1 Data, variables, samples",
    "section": "Samples and populations",
    "text": "Samples and populations\nIn order to investigate one or more linguistic features, we need to collect relevant observations. Actions that generate observations are called experiments, such as decision tasks or even corpus queries. Since it is not feasible to examine, for instance, the entirety of all linguistic utterances ever produced, i.e., the virtually infinite population, we rely on subsets of it: samples. Good research is characterised by good sampling procedures that limit the bias present in any sample, thus improving the generalisability of potential findings.\nTo state it more formally, let \\(\\omega\\) (‘lower-case Omega’) denote a single observation of interest. The full set of theoretically possible observations is contained in \\(\\Omega\\) (‘upper-case Omega’), with\n\\[\n\\Omega = \\{\\omega_1, \\omega_2, \\dots, \\omega_n \\}.\n\\]\nHere \\(n\\) represents the \\(n\\)-th observation and should be a natural number. A sample is then simply a selection of elements from \\(\\Omega\\).",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.1 Data, variables, samples"
    ]
  },
  {
    "objectID": "chapters/04-variables.html#random-variables",
    "href": "chapters/04-variables.html#random-variables",
    "title": "4.1 Data, variables, samples",
    "section": "(Random) Variables",
    "text": "(Random) Variables\nThe concept of the variable is very handy in that it enables researchers to quantify different aspects of linguistic observations, such as the age or gender of the speaker, the register of the speech situation, the variety of English, the length of an utterance, a syntactic construction or a grammatical category, among many other theoretically possible features.\nFor each observation, we should be able to assign a specific outcome to our variables. While the variable Genitive could have a small set of outcomes such as ‘s’ and ‘of’, other variables such as Utterance length or Reaction time could technically assume any value in the range [0, \\(\\infty\\)). The set of all possible outcomes in an experiment is known as the sample space \\(S\\).\nGiven a single observed NP which is marked for possessive case, we may characterise its formal marking as either ‘s’ (synthetic genitive) or as ‘of’ (analytic/periphrastic). For simplicity, we will call these options \\(S\\) and \\(O\\). The sample space of Genitive could be thus described as\n\\[\nS_{\\text{Genitive}} = \\{\\text{S}, \\text{O}\\}.\n\\]\nExperiments normally do not terminate after obtaining the first observation. As a result, the sample space becomes considerably more complex (and unwieldy), the more observations are gathered. For \\(n = 3\\) possessive NPs, the sample space would comprise the following combinations of outcomes:\n\\[\nS_{\\text{Genitive (3 NPs)}} = \\{\\text{SSS}, \\text{SSO},\\text{SOS}, \\text{SOO},\\text{OSS}, \\text{OSO}, \\text{OOS}, \\text{OOO}\\}.\n\\]\nWith \\(n = 10\\) we’d be dealing with \\(2^{10} = 1024\\) possible outcomes. If, for instance, we’re only interested in the total number of s-genitives, this isn’t a particularly efficient approach.\nRandom variables allow us to map outcomes from the sample space onto the real numbers \\(\\mathbb{R}\\). Quite conveniently, we could define a discrete random variable \\(X\\) that counts the total number of s-genitives in a sample of size \\(n\\). Note, however, that the random variable is not random – it is the result of a random process. Other random variables could be\n\nthe number of ‘heads’ after tossing a coin 30 times,\ngetting a ‘6’ when rolling a die 500 times, or\nhow many years it takes for your TV to break down.\n\n\n\n\n\n\n\nRandom variables: Formal definition\n\n\n\n\n\nA random variable is a function \\(X\\) which maps onto each outcome \\(s \\in S\\) exactly one number \\(X(s) = x\\) with \\(x \\in \\mathbb{R}\\).\nIn formal terms:\n\\[\nX : S \\to \\mathbb{R}.\n\\]",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.1 Data, variables, samples"
    ]
  },
  {
    "objectID": "chapters/04-variables.html#datasets",
    "href": "chapters/04-variables.html#datasets",
    "title": "4.1 Data, variables, samples",
    "section": "Datasets",
    "text": "Datasets\nInformation on variables and their values is conventionally arranged in a dataset, which is essentially a matrix with \\(n\\) rows (= observations) and \\(p\\) columns (= variables). The data matrix has the general form in Equation 1 (see Heumann (2022: Chap. 1.4) for details).\n\\[\n\\begin{pmatrix}\n  \\text{Observation }  \\omega & \\text{Variable } X_1 & \\text{Variable } X_2 & \\cdots &  X_p \\\\\n    1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n    2 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n    \\vdots & \\vdots & \\vdots &  & \\vdots \\\\\n    n & x_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{pmatrix}\n\\tag{1}\\]",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.1 Data, variables, samples"
    ]
  },
  {
    "objectID": "chapters/04-variables.html#data-types",
    "href": "chapters/04-variables.html#data-types",
    "title": "4.1 Data, variables, samples",
    "section": "Data types",
    "text": "Data types\nIn general, we distinguish between discrete variables, which can only take a limited set of unique values, and continuous variables, which can take infinitely many values within a specified range.\nWe can further subdivide discrete and continuous variables into into nominal, ordinal, interval-scaled and ratio-scaled ones:\n\n\n\n\n\n\nNominal/categorical\n\n\n\n\n\nThese variables comprise a limited number of discrete categories which cannot be ordered in a meaningful way. For instance, the genitive forms of and ’s cannot be ordered like the numbers (e.g., 1, 2, 3 …).\n\n\n\n\n\n\n\n\n\nOrdinal/categorical\n\n\n\n\n\nOrdinal variables are ordered. However, the intervals between their individual values are not interpretable. Heumann (2022: 6) provides a pertinent example:\n\n[T]he satisfaction with a product (unsatisfied–satisfied–very satisfied) is an ordinal variable because the values this variable can take can be ordered but the differences between ‘unsatisfied–satisfied’ and ‘satisfied–very satisfied’ cannot be compared in a numerical way.\n\n\n\n\n\n\n\n\n\n\nInterval-scaled/continuous\n\n\n\n\n\n\nIn the case of interval-scaled variables, the differences between the values can be interpreted, but their ratios must be treated with caution. A temperature of 4°C is 6 degrees warmer than -2°C; however, this does not imply that 4°C is three times warmer than -2°C. This is because the temperature scale has no true zero point; 0°C simply signifies another point on the scale and not the absence of temperature altogether.\n\n\n\n\n\n\n\n\n\n\nRatio-scaled/continuous\n\n\n\n\n\n\nRatio-scaled variables allow both a meaningful interpretation of the differences between their values and (!) of the ratios between them. Within the context of clause length, LENGTH_DIFF values such as 4 and 8 not only suggest that the latter is four units greater than the former but also that their ratio \\(\\frac{8}{4} = 2\\) is a valid way to describe the relationship between these values. Here a LENGTH_DIFF of 0 can be clearly viewed as the absence of a length difference.",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.1 Data, variables, samples"
    ]
  },
  {
    "objectID": "chapters/04-variables.html#dependent-vs.-independent-variables",
    "href": "chapters/04-variables.html#dependent-vs.-independent-variables",
    "title": "4.1 Data, variables, samples",
    "section": "Dependent vs. independent variables",
    "text": "Dependent vs. independent variables\nIn empirical studies, it is often of interest whether one variable is linked to changes in the values of another variable. When exploring such associations (or correlations), we need to take another heuristic step to clarify the direction of the influence.\nIn a linguistic context, we denote the variable whose usage patterns we’d like to explain as the dependent or response variable. A list of possible dependent variables is provided in the section on Linguistic variables).\nIts outcomes are said to depend on one or more independent variables. These are also often referred to as explanatory variables as they are supposed to explain variation in the response variable. In (socio-)linguistics, these include but are not limited to a speaker’s age, gender, socio-economic class (e.g., working class vs. middle class), their local variety, or the register of the speech context (e.g., casual exchange vs. business transaction).",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.1 Data, variables, samples"
    ]
  },
  {
    "objectID": "chapters/04-variables.html#tier-1",
    "href": "chapters/04-variables.html#tier-1",
    "title": "4.1 Data, variables, samples",
    "section": "Tier 1",
    "text": "Tier 1\n\nExercise 1 Consider the following statement:\n\nThis paper examines the influence of clause length on the ordering of main and subordinate clauses.\n\n\nWhat is the dependent variable?\nWhat is the independent variable?\nWhat would it mean if they were reversed?\n\n\n\nExercise 2 Explain why frequency data is actually discrete.",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.1 Data, variables, samples"
    ]
  },
  {
    "objectID": "chapters/04-variables.html#tier-2",
    "href": "chapters/04-variables.html#tier-2",
    "title": "4.1 Data, variables, samples",
    "section": "Tier 2",
    "text": "Tier 2\n\nExercise 3 Download the dataset Paquot_Larsson_2020_data.xlsx from the supplementary materials and store it in your working directory.1\n1 The dataset was published as part of Paquot & Larsson (2020).Load it into R using the code below (see also 2.6 Importing/Exporting for details). Then identify the variable types (nominal, ordinal etc.) for all columns in the cl.order dataset.\n\n# Load libraries\nlibrary(readxl) # install this library prior to loading\nlibrary(tidyverse)\n\n# Load data\ncl.order &lt;- read_xlsx(\"Paquot_Larsson_2020_data.xlsx\")\n\n# Inspect data\nstr(cl.order)\nhead(cl.order)\n\n\n\n\n\n\n\nWhat’s in the file? The str() function\n\n\n\n\n\nThe easiest way to get a general overview of the full data set is to apply the str() function to the respective data frame.\n\nstr(cl.order)\n\ntibble [403 × 8] (S3: tbl_df/tbl/data.frame)\n $ CASE       : num [1:403] 4777 1698 953 1681 4055 ...\n $ ORDER      : chr [1:403] \"sc-mc\" \"mc-sc\" \"sc-mc\" \"mc-sc\" ...\n $ SUBORDTYPE : chr [1:403] \"temp\" \"temp\" \"temp\" \"temp\" ...\n $ LEN_MC     : num [1:403] 4 7 12 6 9 9 9 4 6 4 ...\n $ LEN_SC     : num [1:403] 10 6 7 15 5 5 12 2 24 11 ...\n $ LENGTH_DIFF: num [1:403] -6 1 5 -9 4 4 -3 2 -18 -7 ...\n $ CONJ       : chr [1:403] \"als/when\" \"als/when\" \"als/when\" \"als/when\" ...\n $ MORETHAN2CL: chr [1:403] \"no\" \"no\" \"yes\" \"no\" ...\n\n\nThis shows us that the data frame has 8 columns, as the $ operators indicate ($ Case, $ ORDER, …). The column names are followed by\n\nthe data type (num for numeric and chr for character strings)\nthe number of values (`[1:403]`) and\nthe first few observations.\n\nAnother intuitive way to display the structure of a data matrix is to simply show the first few rows:\n\nhead(cl.order)\n\n# A tibble: 6 × 8\n   CASE ORDER SUBORDTYPE LEN_MC LEN_SC LENGTH_DIFF CONJ     MORETHAN2CL\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      \n1  4777 sc-mc temp            4     10          -6 als/when no         \n2  1698 mc-sc temp            7      6           1 als/when no         \n3   953 sc-mc temp           12      7           5 als/when yes        \n4  1681 mc-sc temp            6     15          -9 als/when no         \n5  4055 sc-mc temp            9      5           4 als/when yes        \n6   967 sc-mc temp            9      5           4 als/when yes        \n\n\n\n\n\n\n\n\n\n\n\nFurther details on the variables\n\n\n\n\n\n\nORDER: Does the subordinate clause come before or after the main clause?\nSUBORDTYPE: Is the subordinate clause temporal or causal?\nMORETHAN2CL: Are there more clauses in the sentence than just one subordinate clause and one main clause?\nLEN_MC: How many words does the main clause contain?\nLEN_SC: How many words does the subordinate clause contain?\nLENGTH_DIFF: What is the length difference in words between the main clause and subordinate clause?\n\n\n\n\n\n\n\n\n\n\nNumber of (unique) values in R\n\n\n\n\n\nTo count the number of items in a vector, which correspond to the total number \\(n\\) of attested values of \\(X\\), we can use length():\n\nlength(cl.order$SUBORDTYPE)\n\n[1] 403\n\n\nIn fact, it is equivalent to the number of rows in the full data frame:\n\nnrow(cl.order)\n\n[1] 403\n\n\nThe function unique() shows all unique items (= types) in a vector, reflecting the possible outcomes for a single observation:\n\nunique(cl.order$SUBORDTYPE)\n\n[1] \"temp\" \"caus\"\n\n\n\n\n\n\n\nExercise 4 For each of the following linguistic variables, identify whether it is discrete or continuous and then specify the type (nominal, ordinal, interval, or ratio):\n\nParticle placement (Cut off the flowers vs. Cut the flowers off)\nNumber of subordinate clauses in a paragraph\nSpeaker’s country of origin\nReaction time in a language processing task (measured in milliseconds)\nLikert scale rating of grammaticality (possible ratings: 1, 2, 3, 4, 5)",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.1 Data, variables, samples"
    ]
  },
  {
    "objectID": "chapters/04-variables.html#tier-3",
    "href": "chapters/04-variables.html#tier-3",
    "title": "4.1 Data, variables, samples",
    "section": "Tier 3",
    "text": "Tier 3\n\nExercise 5 The tidyverse function slice_sample() allows users to randomly sample rows from a data frame, i.e., with every row having the same probability of being selected. We will do so without replacement for the cl.order data, choosing 50 rows at random.\n\nlibrary(tidyverse)\n\n# For reproducibility, it is highly recommended to set a random seed. This makes sure we will get the \"same\" random result every time we execute our code.\nset.seed(123)\n\n# Extract a random sample of 50 rows\ncl.order.random &lt;- cl.order %&gt;%\n  slice_sample(n = 50, replace = FALSE)\n\nprint(cl.order.random)\n\n# A tibble: 50 × 8\n    CASE ORDER SUBORDTYPE LEN_MC LEN_SC LENGTH_DIFF CONJ          MORETHAN2CL\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;      \n 1  3490 sc-mc temp           15     10           5 nachdem/after no         \n 2  4608 mc-sc temp            5     11          -6 als/when      yes        \n 3  2458 sc-mc temp           29      4          25 nachdem/after no         \n 4  3178 sc-mc caus            4     19         -15 weil/because  yes        \n 5  3616 sc-mc temp            6      5           1 bevor/before  yes        \n 6  2405 mc-sc caus            6      5           1 weil/because  no         \n 7  1799 mc-sc caus            3      7          -4 weil/because  no         \n 8  2752 mc-sc caus           13     16          -3 weil/because  no         \n 9   350 mc-sc caus            8     11          -3 weil/because  no         \n10   342 mc-sc caus           10     15          -5 weil/because  no         \n# ℹ 40 more rows\n\n\nAnother common approach is stratified sampling, which is a method of sampling from a population where the population is divided into distinct subgroups (strata) based on a characteristic, and a sample is drawn from each subgroup to ensure proportional representation.\n\nlibrary(sampling)\nlibrary(tidyverse)\n\nstrat_sample &lt;- function(data, variable, size) {\n  \n  # Capture the variable symbol\n  var_sym &lt;- enquo(variable)\n  var_name &lt;- rlang::as_name(var_sym)\n  \n  # Proportions in the population (= sampling weights)\n  data_prop &lt;- table(pull(data, !!var_sym)) / nrow(data)\n\n  # Sizes of the stratified sample\n  strat_sample_sizes &lt;- round(size * data_prop)\n\n  # Convert variable of interest to factor\n  data[[var_name]] &lt;- as.factor(data[[var_name]])\n\n  # Draw the sample\n  clause_strat_sample &lt;- strata(data, \n                                stratanames = var_name,\n                                size = strat_sample_sizes, \n                                # Stratified sampling without replacement\n                                method = \"srswor\")\n\n  # Output df\n  output_sample &lt;- tibble(getdata(data, clause_strat_sample)) %&gt;% \n    select(-Prob, -Stratum, -ID_unit)\n\n  return(output_sample)\n}\n\n# Apply the function\ncl.order.strat &lt;- strat_sample(cl.order, ORDER, size = 50)\n\nLet’s compare the results of the different sampling procedures:\n\n# Check proportions in the original dataset\ncl.order %&gt;%\n  count(ORDER) %&gt;% \n  mutate(pct = n/sum(n))\n\n# A tibble: 2 × 3\n  ORDER     n   pct\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;\n1 mc-sc   275 0.682\n2 sc-mc   128 0.318\n\n# Check proportions in the random sample\ncl.order.random %&gt;% \n   count(ORDER) %&gt;% \n   mutate(pct = n/sum(n))\n\n# A tibble: 2 × 3\n  ORDER     n   pct\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;\n1 mc-sc    29  0.58\n2 sc-mc    21  0.42\n\n# Check proportions in the stratified sample\ncl.order.strat %&gt;% \n   count(ORDER) %&gt;% \n   mutate(pct = n/sum(n))\n\n# A tibble: 2 × 3\n  ORDER     n   pct\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;\n1 mc-sc    16  0.32\n2 sc-mc    34  0.68\n\n\nDiscuss possible linguistic research scenarios when stratified sampling may be preferable over random sampling, and vice versa.\n\n\nExercise 6 Assuming three randomly chosen observations from the cl.order dataset, describe the sample space \\(S\\) for the variable ORDER. What random variables could you define with domain \\(S\\)?",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.1 Data, variables, samples"
    ]
  },
  {
    "objectID": "chapters/06-gradient_boosting.html",
    "href": "chapters/06-gradient_boosting.html",
    "title": "7.2 Gradient boosting",
    "section": "",
    "text": "James et al. (2021): Chapter 8.2\nHastie, Tibshirani, and Friedman (2017): Chapters 9 & 10",
    "crumbs": [
      "7. Machine Learning",
      "7.2 Gradient boosting"
    ]
  },
  {
    "objectID": "chapters/06-gradient_boosting.html#recommended-reading",
    "href": "chapters/06-gradient_boosting.html#recommended-reading",
    "title": "7.2 Gradient boosting",
    "section": "",
    "text": "James et al. (2021): Chapter 8.2\nHastie, Tibshirani, and Friedman (2017): Chapters 9 & 10",
    "crumbs": [
      "7. Machine Learning",
      "7.2 Gradient boosting"
    ]
  },
  {
    "objectID": "chapters/06-gradient_boosting.html#preparation",
    "href": "chapters/06-gradient_boosting.html#preparation",
    "title": "7.2 Gradient boosting",
    "section": "Preparation",
    "text": "Preparation",
    "crumbs": [
      "7. Machine Learning",
      "7.2 Gradient boosting"
    ]
  },
  {
    "objectID": "chapters/06-gradient_boosting.html#boosting",
    "href": "chapters/06-gradient_boosting.html#boosting",
    "title": "7.2 Gradient boosting",
    "section": "Boosting",
    "text": "Boosting\nThe core idea of boosting is as simple as it is intuitive: By aggregating the insights of multiple weak models, a much more powerful complex model can be formed. The new model ensemble is generally superior in terms of predictive performance. While there are various computational implementations of boosting, we will restrict our scope to decision trees as introduced in the previous unit.\n\nTrees revisited\nA single tree \\(T\\) splits up the feature space into prediction regions \\(R_j\\) for \\(j = 1, \\dots, J\\). Each observation \\(x\\) from the training data is assigned to a prediction region \\(R_j\\) and receives a constant value \\(\\gamma_j\\). The final prediction \\(\\hat{y}\\) is a function of the constants returned from all regions:\n\\[\n\\hat{y} = f(x) = \\sum_{j= 1}^{J}\\gamma_jI(x \\in R_j)\n\\tag{1}\\]\nThus, the tree \\(T\\) is determined by the input \\(x\\) and the parameter space \\(\\Theta = \\{R_j, \\gamma_j\\}_{1}^{J}\\) which contains said regions and constant terms. A tree ensemble can the be described as the sum of \\(M\\) trees \\(T(x; \\Theta)\\), as summarised in Equation 2.\n\\[\nf_M(x) = \\sum_{m=1}^{M}T(x;\\Theta_m)\n\\tag{2}\\]\nIn contrast to random forests, where the goal is to attain maximum variance among trees, boosted trees actually learn from each other. Each subsequent tree should improve on the errors made by the current one. It does so by finding the parameter values that minimise the loss function \\(L(f)\\).1\n1 There is no analytical solution to this which can be computed via a single formula. Instead, it poses an optimisation problem that requires specialised numerical techniques. In the end, these can only provide approximate solutions.\n\nLoss functions and gradient descent\nGiven \\(N\\) observations, let \\(y_i\\) represent the labels of the response variable and \\(f(x_i)\\) the prediction function for a data point \\(x_i\\). There are numerous metrics that can be used to quantify the “loss” \\(L(y_i, f(x_i))\\), such as squared errors for regression or deviance for classification. The full loss function \\(L(f)\\) is given by Equation 3.\n\\[\nL(f) = \\sum_{i=1}^N L(y_i, f(x_i))\n\\tag{3}\\]\nThe multi-dimensional space can be conceptualised as a mountain range where we are looking for paths that lead us back to level ground. The loss function quantifies the amount of energy we have to expend during this endeavor. For this reason, it would be a good idea to keep track of all the paths we could take. As fearless as we are, we want to opt for the steepest paths that lead us back down as quickly as possible.\nMathematically, this can be done via the gradient \\(\\mathbf{g}_m\\), which is a vector of partial derivatives. It captures how a function with multiple variables changes in different directions. Essentially, it is akin to a map that indicates all paths and how steep they are. One possible path, a gradient component \\(g_{mi}\\), would have the general form in Equation 4.\n\\[\ng_{mi} = \\frac{\\partial L(y, f(x_i))}{\\partial f(x_i)}\n\\tag{4}\\]\nThe steepest path, and thus the most rapid decrease in the loss function, is the negative gradient \\(-\\mathbf{g}_m\\), determining the predictions of the next boosted tree. It is, therefore, no surprise that this procedure is also known as gradient descent.",
    "crumbs": [
      "7. Machine Learning",
      "7.2 Gradient boosting"
    ]
  },
  {
    "objectID": "chapters/06-gradient_boosting.html#implementation-in-r",
    "href": "chapters/06-gradient_boosting.html#implementation-in-r",
    "title": "7.2 Gradient boosting",
    "section": "Implementation in R",
    "text": "Implementation in R\n\n# Load libraries\nlibrary(gbm)\n\nLoaded gbm 2.1.8.1\n\nlibrary(xgboost)\n\n# Load data\n\n\n\n\n\n\n\nWarning\n\n\n\nThis page is still under construction. More content will be added soon!",
    "crumbs": [
      "7. Machine Learning",
      "7.2 Gradient boosting"
    ]
  },
  {
    "objectID": "chapters/03-regex.html",
    "href": "chapters/03-regex.html",
    "title": "3.2 Regular expressions",
    "section": "",
    "text": "Script\n\n\n\nYou can find the full R script associated with this unit here.",
    "crumbs": [
      "3. NLP with R",
      "3.2 Regular expressions"
    ]
  },
  {
    "objectID": "chapters/03-regex.html#preparation",
    "href": "chapters/03-regex.html#preparation",
    "title": "3.2 Regular expressions",
    "section": "",
    "text": "Script\n\n\n\nYou can find the full R script associated with this unit here.",
    "crumbs": [
      "3. NLP with R",
      "3.2 Regular expressions"
    ]
  },
  {
    "objectID": "chapters/03-regex.html#recommended-reading",
    "href": "chapters/03-regex.html#recommended-reading",
    "title": "3.2 Regular expressions",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nLange and Leuckert (2020): Chapter 3.7\nDetailed cheatsheet (DataCamp)",
    "crumbs": [
      "3. NLP with R",
      "3.2 Regular expressions"
    ]
  },
  {
    "objectID": "chapters/03-regex.html#regular-expressions",
    "href": "chapters/03-regex.html#regular-expressions",
    "title": "3.2 Regular expressions",
    "section": "Regular expressions",
    "text": "Regular expressions\nRegular expressions (or ‘regex’) help us find more complex patterns in strings of text. Suppose we are interested in finding all inflectional forms of the lemma PROVIDE in a corpus, i.e., provide, provides, providing and provided. Insteading of searching for all forms individually, we can construct a regular expression of the form\n\\[\n\\text{provid(e(s)? | ing | ed)}\n\\] which can be read as\n\n‘Match the sequence of letters &lt;provid&gt; as well as when it is followed by the groups of letters &lt;es&gt; or &lt;ing&gt; or &lt;ed&gt;. Also make the &lt;s&gt; in &lt;es&gt; optional.’\n\nNotice how optionality is signified by the ? operator and alternatives by |.\nTo activate regular expression in a kwic() query, the valuetype argument has to be set to \"regex\":\n\n# Load library and corpus\nlibrary(quanteda)\n\nPackage version: 3.3.1\nUnicode version: 14.0\nICU version: 70.1\n\n\nParallel computing: 8 of 8 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\nICE_GB &lt;- readRDS(\"../datasets/ICE_GB.RDS\")\n\n# Perform query\nkwic_provide &lt;- kwic(ICE_GB,\n                     \"provid(e(s)?|ing|ed)\",\n                     valuetype = \"regex\",\n                     window = 20)\n\nThe number of hits has more than doubled. However, upon closer inspection, we’ll notice a few false positives, namely providential, provider and providers:\n\ntable(kwic_provide$keyword)\n\n\n     provide     provided     Provided    Provident providential     provider \n         165          118            5            1            1            1 \n   providers     provides    providing    Providing \n           3           72           52            1 \n\n\nThere are two ways to handle the output:\n\nRefine the search expression further to only match those cases of interest.\nManually sort out irrelevant cases during qualitative annotation in a spreadsheet software.\n\nAs a rule of thumb, you should consider improving your search expression if you obtain hundreds or even thousands of false hits. Should there be only few false positives, it’s usually easier to simply mark them as “irrelevant” in your spreadsheet. A full workflow is demonstrated in unit 13. Data annotation.",
    "crumbs": [
      "3. NLP with R",
      "3.2 Regular expressions"
    ]
  },
  {
    "objectID": "chapters/03-regex.html#a-regex-cheatsheet",
    "href": "chapters/03-regex.html#a-regex-cheatsheet",
    "title": "3.2 Regular expressions",
    "section": "A RegEx Cheatsheet",
    "text": "A RegEx Cheatsheet\n\nBasic functions\n\n\n\nCommand\nDefinition\nExample\nFinds\n\n\n\n\n\n\npython\npython\n\n\n.\nAny character\n.ython\naython, bython…\n\n\n\n\n\nCharacter classes and alternatives\n\n\n\n\n\n\n\n\n\nCommand\nDefinition\nExample\nFinds\n\n\n\n\n[abc]\nClass of characters\n[jp]ython\njython, python\n\n\n[^pP]\nExcluded class of characters\n[^pP]ython\neverything but python, Python\n\n\n(...|...)\nAlternatives linked by logical operator or\nP(ython|eter)\nPython, Peter\n\n\n\n\n\nQuantifiers\n\n\n\n\n\n\n\n\n\nCommand\nDefinition\nExample\nFinds\n\n\n\n\n?\nOne or zero instances of the preceding symbol\nPy?thon\nPython, Pthon\n\n\n*\nNo matter how many times — also zero\nPy*thon\nPython, Pthon, Pyyyython…\n\n\n\n\nP[Yy]*thon\nPython, Pthon, PyYYython…\n\n\n+\nNo matter how many times but at least once\nPy+thon\nPython, Pyyython, Pyyyython\n\n\n{1,3}\n{min, max}\nPy{1,3}thon\nPython, Pyython, Pyyython\n\n\n\n\n\nPre-defined character classes\n\n\n\n\n\n\nNote\n\n\n\nThe double backslashes (\\\\) shown here are specific to the quanteda R package. In most other programming languages including Python, you only need a single backslash (e.g., \\w, \\d, \\s). This double-escaping is an R-specific requirement due to how R handles string literals.\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDefinition\nExample\nFinds\n\n\n\n\n\\\\w\nAll alphanumeric characters (A-Z, a-z, 0-9)\n\\\\w+ing\nwalking, running, 42ing\n\n\n\\\\W\nAll non-alphanumeric characters\nhello\\\\W+world\nhello world, hello!!!world\n\n\n\\\\d\nAll decimal numbers (0-9)\n\\\\d{3}-\\\\d{4}\n555-1234, 867-5309\n\n\n\\\\D\nEverything which is not a decimal number\n\\\\D+\nHello!, Python_code\n\n\n\\\\s\nEmpty space\nword\\\\s+word\nword word\n\n\n\\\\b\nWord boundary\n\\\\bpython\\\\b\nMatches python as a whole word",
    "crumbs": [
      "3. NLP with R",
      "3.2 Regular expressions"
    ]
  },
  {
    "objectID": "chapters/03-regex.html#querying-parsed-corpora",
    "href": "chapters/03-regex.html#querying-parsed-corpora",
    "title": "3.2 Regular expressions",
    "section": "Querying parsed corpora",
    "text": "Querying parsed corpora\nThe range of linguistic patterns to be matched can be extended further if the corpus contains additional metadata, such as the part of speech (POS) of a token. POS-tagged corpora open up the option of looking for more abstract patterns, such as all instances of the verb eat that are followed by a pronoun or noun:\n\n# Load library and corpus\nICE_GB_POS &lt;- readRDS(\"../datasets/ICE_GB_POS.RDS\")\n\n# Perform query\nkwic_provide_POS2 &lt;- kwic(ICE_GB_POS,\n                     phrase(\"\\\\b(ate|eat(s|ing|en)?)_VERB\\\\b _(PRON|NOUN)\"),\n                     valuetype = \"regex\",\n                     window = 5)\n\nhead(kwic_provide_POS2)\n\n\n\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nS1A-009.txt\n1198\n1199\nI_PRON must_AUX &lt; , &gt;\neat_VERB them_PRON\n&lt; ICE-GB:S1A-009 #71 : 1\n\\b(ate|eat(s|ing|en)?)_VERB\\b _(PRON|NOUN)\n\n\nS1A-010.txt\n958\n959\nto &lt; , &gt; actually_ADV\neat_VERB it_PRON\nfor_ADP one_NOUN ' s_PART own_ADJ\n\\b(ate|eat(s|ing|en)?)_VERB\\b _(PRON|NOUN)\n\n\nS1A-011.txt\n3245\n3246\n: A &gt; I_PRON have_AUX\neaten_VERB my_PRON\nway_NOUN round_ADP the_DET Yorkshire_PROPN Dales_PROPN\n\\b(ate|eat(s|ing|en)?)_VERB\\b _(PRON|NOUN)\n\n\nS1A-011.txt\n4159\n4160\nI_PRON ended_VERB up_ADP uhm_NOUN just_ADV\neating_VERB sort_NOUN\nof_ADP_ADP lumps_NOUN of chicken_NOUN and_CCONJ\n\\b(ate|eat(s|ing|en)?)_VERB\\b _(PRON|NOUN)\n\n\nS1A-018.txt\n455\n456\norder_VERB on_ADPe_NUM first_ADJ and_CCONJ_CCONJ then_ADV_ADV\neat_VERB it_PRON\nand then sort_ADV of_ADV carry_VERB\n\\b(ate|eat(s|ing|en)?)_VERB\\b _(PRON|NOUN)\n\n\nS1A-019.txt\n1038\n1039\nA &gt; and_CCONJ everybody_PRON was_AUX\neating_VERB something_PRON\n&lt; ICE-GB:S1A-019 #76 : 1\n\\b(ate|eat(s|ing|en)?)_VERB\\b _(PRON|NOUN)\n\n\n\n\n\n\n\nOne R package that supplies functions for tokenisation, POS-tagging and even dependency parsing for dozens of languages is udpipe. They all rely on one common set of tags known as Universal Dependencies, which are listed here:\n\n\n\n\n\n\nUniversial dependencies – Tagset\n\n\n\n\n\nCf. https://universaldependencies.org/u/pos/.\n\n\n\n\n\n\n\nPOS Tag\nDescription\n\n\n\n\nADJ\nAdjective: describes a noun (e.g., big, old, green, first)\n\n\nADP\nAdposition: prepositions and postpositions (e.g., in, to, over)\n\n\nADV\nAdverb: modifies verbs, adjectives, or other adverbs (e.g., quickly, very)\n\n\nAUX\nAuxiliary: helps form verb tenses, moods, or voices (e.g., is, have, will)\n\n\nCCONJ\nCoordinating conjunction: links words, phrases, or clauses (e.g., and, or, but)\n\n\nDET\nDeterminer: introduces nouns (e.g., the, a, some, my)\n\n\nINTJ\nInterjection: expresses emotion or reaction (e.g., oh, wow, hello)\n\n\nNOUN\nNoun: person, place, thing, or concept (e.g., cat, city, idea)\n\n\nNUM\nNumeral: expresses a number or ranking (e.g., one, two, second)\n\n\nPART\nParticle: adds meaning without being an independent word class (e.g., not, to as in to run)\n\n\nPRON\nPronoun: replaces nouns (e.g., he, she, they, it)\n\n\nPROPN\nProper noun: names specific entities (e.g., London, Vladimir)\n\n\nPUNCT\nPunctuation: marks boundaries in text (. , ! ?)\n\n\nSCONJ\nSubordinating conjunction: links clauses, often indicating dependency (e.g., if, because, although)\n\n\nSYM\nSymbol: non-alphanumeric symbol (e.g., %, &, #)\n\n\nVERB\nVerb: action or state (e.g., run, be, have)\n\n\nX\nOther: used when a word doesn’t fit into other categories",
    "crumbs": [
      "3. NLP with R",
      "3.2 Regular expressions"
    ]
  },
  {
    "objectID": "chapters/03-regex.html#working-with-other-corpora",
    "href": "chapters/03-regex.html#working-with-other-corpora",
    "title": "3.2 Regular expressions",
    "section": "Working with other corpora",
    "text": "Working with other corpora\nSome corpus platforms such as BNCweb, CQPweb or our local KU corpora1 support a specialised query syntax known as CQP (Corpus Query Processor), which enables users to query for strings with specific meta-attributes (text category, age, gender etc.).\n1 Note that the corpus platform of the Catholic University of Eichstätt-Ingolstadt, which was set up by Dr. Thomas Brunner, is only accessible to students or staff via the local eduroam network or a VPN client which holds their credentials.\nBasic use\nOnce signed in on any of these platforms, the user interface will generally follow this layout:\n\n\n\nCQP query of PROVIDE in the GloWbE corpus\n\n\n\nTo select a different corpus, navigate to CQPweb main menu under About CQPweb in the left menu bar.\nTo restrict search results to specific parts of the corpus, select Restricted query and choose the relevant text categories, varieties, etc.\n\nYou can now enter a search expression into the white box using the following pattern:\n\n[attribute = \"property\"]\n\nFor example, to retrieve all inflectional forms2 of the verb provide, enter:\n2 Some corpora do not support the lemma attribute and require manual listing of all inflectional forms via regular expressions, such as [word = \"provid(e(s)?|d|ing)\"] for provide.\n[lemma = \"provide\"]\n\nTo search for a word with a specific part-of-speech (POS) tag – such as like used as a preposition – use:\n\n[word = \"like\" & pos = \"ii\"]\n\nFor a full list of POS-tags, refer to:\n\nCLAWS5 (for BNC)\nCLAWS7 (for COCA and GloWbE)\n\n\n\n\n\n\n\nNote\n\n\n\nIf the exact word string is not important and you are only interested in part-of-speech sequences, you can supply the regular expression .+ to the word attribute, which means ‘match everything’. For example, to find all common nouns (NN) that are followed by a general preposition io, we can write\n\n[word = \".+\" & pos = \"nn\"][word = \".+\" & pos = \"io\"]\n\nRegular expressions also allow you to match multiple POS-tags, such as all noun categories (nn, nn1, nn2, nna, etc.) or all prepositions (if, ii, io). The search term below generalises the previous example, making it more inclusive:\n\n[word = \".+\" & pos = \"n.+\"][word = \".+\" & pos = \"i.+\"]\n\nHowever, be cautious with overly broad search patterns, as they might return (nearly) the entire corpus! The last one yields 22,294,282 matches.\n\n\n\n\nExporting and importing your results\nTo export your KWIC-hits, select Download in the top-right corner and specify your output options (e.g., the size of the search window or the speaker metadata). While you can immediately read the downloaded concordance.txt file into R, it’s advisable to first perform some manual clean-up in spreadsheet software (e.g., MS Excel). This ensures all columns and rows are complete and properly formatted.\nTo do this in Excel:\n\nNavigate to File &gt; Import &gt; Text file.\nSelect your file and choose Delimited.\nClick Next, select Tab as the delimiter, then click Next &gt; Finish.\n\nMake sure to save your file, ideally with the extension .xlsx.\nFrom here, please refer to the unit Import/export data for further steps.",
    "crumbs": [
      "3. NLP with R",
      "3.2 Regular expressions"
    ]
  },
  {
    "objectID": "chapters/03-regex.html#exercises",
    "href": "chapters/03-regex.html#exercises",
    "title": "3.2 Regular expressions",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nSolutions\n\n\n\nYou can find the solutions to the exercises here.\n\n\n\nExercise 1 How could you refine the search expression for PROVIDE \"provid(e(s)?|ing|ed)\" to get rid of the irrelevant cases?\n\n\nExercise 2 Write elegant regular expressions which find all inflectional forms of the following verbs:\n\naccept\nattach\nswim\nknow\nforget\n\n\n\nExercise 3 Find all nouns ending in -er.\n\n\nExercise 4 Find all four-digit numbers.\n\n\nExercise 5 Find all verbs that are followed by a preposition.",
    "crumbs": [
      "3. NLP with R",
      "3.2 Regular expressions"
    ]
  },
  {
    "objectID": "chapters/06-poisson_regression.html",
    "href": "chapters/06-poisson_regression.html",
    "title": "6.4 Poisson regression",
    "section": "",
    "text": "Winter (2020): Chapter 13\nBaguley (2012): Chapter 17.5",
    "crumbs": [
      "6. Statistical Modelling",
      "6.4 Poisson regression"
    ]
  },
  {
    "objectID": "chapters/06-poisson_regression.html#recommended-reading",
    "href": "chapters/06-poisson_regression.html#recommended-reading",
    "title": "6.4 Poisson regression",
    "section": "",
    "text": "Winter (2020): Chapter 13\nBaguley (2012): Chapter 17.5",
    "crumbs": [
      "6. Statistical Modelling",
      "6.4 Poisson regression"
    ]
  },
  {
    "objectID": "chapters/06-poisson_regression.html#preparation",
    "href": "chapters/06-poisson_regression.html#preparation",
    "title": "6.4 Poisson regression",
    "section": "Preparation",
    "text": "Preparation\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggeffects)\nlibrary(readxl)\nlibrary(car)\n\n# Load datasets\nverbs &lt;- read_xlsx(\"winter_2020_visual.xlsx\")",
    "crumbs": [
      "6. Statistical Modelling",
      "6.4 Poisson regression"
    ]
  },
  {
    "objectID": "chapters/06-poisson_regression.html#the-poisson-family",
    "href": "chapters/06-poisson_regression.html#the-poisson-family",
    "title": "6.4 Poisson regression",
    "section": "The Poisson family",
    "text": "The Poisson family\nFrequency data is ubiquitous in corpus linguistics. Given its numeric nature, it seems tempting to model such data using linear regression, but doing so is bound to cause problems. Partially owing to the fact that count data is always positive, the residuals more often than not deviate from normality and additionally display non-constant variance. The figures below illustrate these issues for a linear model fitted to simulated count data.\n\n\n\n\n\n\n\n\n\nA probability distribution that is much better equipped for this special case of discrete, yet numeric data is the Poisson distribution. Assuming a Poisson-distributed variable \\(X\\), its exact shape is determined by a single parameter, \\(\\lambda\\) (‘lambda’), which is both its expected value (i.e., the mean) and its variance. In shorthand notation,\n\\[\nX \\sim Pois(\\lambda).\n\\tag{1}\\]\nIts probability mass function has a notable negative skew, which becomes less conspicuous for increasingly higher parameter values.",
    "crumbs": [
      "6. Statistical Modelling",
      "6.4 Poisson regression"
    ]
  },
  {
    "objectID": "chapters/06-poisson_regression.html#poisson-regression",
    "href": "chapters/06-poisson_regression.html#poisson-regression",
    "title": "6.4 Poisson regression",
    "section": "Poisson regression",
    "text": "Poisson regression\nRecall how in linear regression, the dependent variable \\(Y\\) was modelled as a function of the linear sum of predictor terms \\(\\beta_pX_p\\) for \\(1, \\dots, p\\) independent variables.\nThe Poisson model is very similar to the linear regression model, with the main differences being the logarithmic transformation of the response and the exclusion of the error term:\n\\[ \\ln(Y) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p.\n\\tag{2}\\]\nTo remove the logarithm and obtain the model output on a more intuitive scale, we simply exponentiate both sides:\n\\[ Y = e^{\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p}.\n\\tag{3}\\]\n\nApplication in R\nThe data provided by Winter (2020) contains frequency data for hundreds of verbs (Freq column) as well as a variety of psycholinguistic ratings (Sight, Touch, Sound etc.), which were originally compiled by Lynott & Connell (2009).\n\nhead(verbs)\n\n# A tibble: 6 × 9\n  Word      DominantModality Sight Touch Sound  Taste  Smell Log10Freq  Freq\n  &lt;chr&gt;     &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 abrasive  Haptic            2.89 3.68  1.68  0.579  0.579      1.36     23\n2 absorbent Visual            4.14 3.14  0.714 0.476  0.476      0.903     8\n3 aching    Haptic            2.05 3.67  0.667 0.0476 0.0952     2.02    105\n4 acidic    Gustatory         2.19 1.14  0.476 4.19   2.90       1.04     11\n5 acrid     Olfactory         1.12 0.625 0.375 3      3.5        0.301     2\n6 adhesive  Haptic            3.67 4.33  1.19  0.905  1.76       1.67     47\n\n\nWe will examine of the effects of different sensory ratings on the frequency of a verb. When fitting the generalized linear model, it’s important to indicate the argument family = \"poisson\" to apply the correct (logarithmic) link function.\n\n# Fit Poisson regression model\nfreq.m1 &lt;- glm(Freq ~ Sight + Touch + Sound + Taste + Smell, data = verbs, family = \"poisson\")\n\n# Summarise model statistics\nsummary(freq.m1)\n\n\nCall:\nglm(formula = Freq ~ Sight + Touch + Sound + Taste + Smell, family = \"poisson\", \n    data = verbs)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-84.27  -51.06  -30.39   -8.21  606.43  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.1201624  0.0118785  262.67   &lt;2e-16 ***\nSight        0.8726608  0.0024550  355.46   &lt;2e-16 ***\nTouch        0.1490792  0.0009801  152.10   &lt;2e-16 ***\nSound        0.1692741  0.0011662  145.15   &lt;2e-16 ***\nTaste        0.0790246  0.0019106   41.36   &lt;2e-16 ***\nSmell       -0.0963918  0.0019746  -48.82   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1890910  on 361  degrees of freedom\nResidual deviance: 1633858  on 356  degrees of freedom\n  (61 observations deleted due to missingness)\nAIC: 1636345\n\nNumber of Fisher Scoring iterations: 7\n\n\nThe model has identified numerous significant effects, i.e., \\(\\beta\\)-values that are significantly different from 0. The low standard errors hint at very robust estimates, resulting in 95% confidence intervals that are barely visible in the effect plots.\n\n# Get predicted values for each predictor\npred_sight &lt;- ggpredict(freq.m1, terms = \"Sight\")\npred_touch &lt;- ggpredict(freq.m1, terms = \"Touch\")\npred_sound &lt;- ggpredict(freq.m1, terms = \"Sound\")\npred_taste &lt;- ggpredict(freq.m1, terms = \"Taste\")\npred_smell &lt;- ggpredict(freq.m1, terms = \"Smell\")\n\n# For plotting individual predictions, simply use:\n# plot(pred_sight) \n\n# Combine all predictions into one data frame\npred_all &lt;- rbind(\n  data.frame(pred_sight, predictor = \"Sight\"),\n  data.frame(pred_touch, predictor = \"Touch\"),\n  data.frame(pred_sound, predictor = \"Sound\"),\n  data.frame(pred_taste, predictor = \"Taste\"),\n  data.frame(pred_smell, predictor = \"Smell\")\n)\n\n# Create faceted plot\nggplot(pred_all, aes(x = x, y = predicted)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.5) +\n  facet_wrap(~predictor, scales = \"free_x\") +\n  labs(x = \"Rating\", y = \"Predicted verb frequency\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nInterpretation\nAs we can see, verbs with higher Sight ratings (i.e., highly visual verbs) are associated with the greatest increase in frequency of occurrence. Given a one-unit increase in \\(X\\), we can obtain the percentage increase (or decrease, respectively) using the formula \\(100(e^b - 1)\\). The predictor Sight has an estimate of approximately \\(0.87\\) (\\(p&lt; 0.001\\), 95% CI [0.87, 0.88]), so the proportional increase is \\(100(e^{0.87} - 1) = 138.69\\%\\).\nBy contrast, Smell is a associated with a lower frequency: \\(100(e^{0.10} -1) = -9.51\\%\\) is the drop in frequency for each one-unit increase in smell ratings. In essence: Smelly verbs are unpopular!",
    "crumbs": [
      "6. Statistical Modelling",
      "6.4 Poisson regression"
    ]
  },
  {
    "objectID": "chapters/07-clustering.html",
    "href": "chapters/07-clustering.html",
    "title": "7.5 Clustering",
    "section": "",
    "text": "James et al. (2021): Chapter 12\nHastie, Tibshirani, and Friedman (2017): Chapters 14.3.6, 14.3.10 & 14.3.12",
    "crumbs": [
      "7. Machine Learning",
      "7.5 Clustering"
    ]
  },
  {
    "objectID": "chapters/07-clustering.html#recommended-reading",
    "href": "chapters/07-clustering.html#recommended-reading",
    "title": "7.5 Clustering",
    "section": "",
    "text": "James et al. (2021): Chapter 12\nHastie, Tibshirani, and Friedman (2017): Chapters 14.3.6, 14.3.10 & 14.3.12",
    "crumbs": [
      "7. Machine Learning",
      "7.5 Clustering"
    ]
  },
  {
    "objectID": "chapters/07-clustering.html#preparation",
    "href": "chapters/07-clustering.html#preparation",
    "title": "7.5 Clustering",
    "section": "Preparation",
    "text": "Preparation",
    "crumbs": [
      "7. Machine Learning",
      "7.5 Clustering"
    ]
  },
  {
    "objectID": "chapters/07-clustering.html#clustering-algorithms",
    "href": "chapters/07-clustering.html#clustering-algorithms",
    "title": "7.5 Clustering",
    "section": "Clustering algorithms",
    "text": "Clustering algorithms\n\n\n\n\n\n\nWarning\n\n\n\nThis page is still under construction. More content will be added soon!\n\n\n\n\\(k\\)-means\n\n\nPartitioning around medoids (PAM)",
    "crumbs": [
      "7. Machine Learning",
      "7.5 Clustering"
    ]
  },
  {
    "objectID": "chapters/06-linear_regression.html",
    "href": "chapters/06-linear_regression.html",
    "title": "6.1 Linear regression",
    "section": "",
    "text": "For linguists:\n\nLevshina (2015: Chapter 7)\nWinter (2020: Chapter 4)\n\nGeneral:\n\nHeumann et al. (2022: Chapter 11)\nJames et al. (2021: Chapter 3)\nHastie et al. (2017: Chapter 3)",
    "crumbs": [
      "6. Statistical Modelling",
      "6.1 Linear regression"
    ]
  },
  {
    "objectID": "chapters/06-linear_regression.html#recommended-reading",
    "href": "chapters/06-linear_regression.html#recommended-reading",
    "title": "6.1 Linear regression",
    "section": "",
    "text": "For linguists:\n\nLevshina (2015: Chapter 7)\nWinter (2020: Chapter 4)\n\nGeneral:\n\nHeumann et al. (2022: Chapter 11)\nJames et al. (2021: Chapter 3)\nHastie et al. (2017: Chapter 3)",
    "crumbs": [
      "6. Statistical Modelling",
      "6.1 Linear regression"
    ]
  },
  {
    "objectID": "chapters/06-linear_regression.html#preparation",
    "href": "chapters/06-linear_regression.html#preparation",
    "title": "6.1 Linear regression",
    "section": "Preparation",
    "text": "Preparation\n\n# Load libraries\nlibrary(readxl) # for reading in Excel data\nlibrary(tidyverse) # data manipulation and visualisation framework\nlibrary(tidymodels)\nlibrary(broom) # converting models to data frames\nlibrary(sjPlot) # exporting regression tables\nlibrary(effects) # plot marginal effects\nlibrary(ggeffects) # generating predictions\nlibrary(ggrepel)\nlibrary(car) # model diagnostics\n\n# Load data\nELP &lt;- read_xlsx(\"../datasets/ELP.xlsx\")\n\n# Inspect data structure\nstr(ELP)",
    "crumbs": [
      "6. Statistical Modelling",
      "6.1 Linear regression"
    ]
  },
  {
    "objectID": "chapters/06-linear_regression.html#introduction",
    "href": "chapters/06-linear_regression.html#introduction",
    "title": "6.1 Linear regression",
    "section": "Introduction",
    "text": "Introduction\nThe ELP (English Lexicon Project) dataset (Balota et al. 2007) comprises reaction times (RT) on 880 English lemmas (Word). For each item, the data includes information on the part of speech (POS), normalised frequency (Freq), and the length in letters (Length).",
    "crumbs": [
      "6. Statistical Modelling",
      "6.1 Linear regression"
    ]
  },
  {
    "objectID": "chapters/06-linear_regression.html#linear-regression",
    "href": "chapters/06-linear_regression.html#linear-regression",
    "title": "6.1 Linear regression",
    "section": "Linear regression",
    "text": "Linear regression\nLinear regression is a simple approach to supervised machine learning where the continuous response variable is known.1 The predictors may be continuous or categorical.\n1 If the response variable is unknown or irrelevant, we speak of unsupervised machine learning. Unsupervised models are mostly concerned with finding patterns in high-dimensional datasets with dozens or even hundreds of variables.One of the core assumptions of linear regression models is that the dependence of \\(Y\\) on \\(X\\) is linear, i.e., their relationship is a straight line. Although it may seem overly simplistic, linear regression is extremely useful both conceptually and practically.\n\nModel with a single predictor \\(X\\)\nThe simple linear model has the general form\n\\[ Y = \\beta_0 + \\beta_1X + \\epsilon.\n\\tag{2}\\]\n\nThe model parameters (or coefficients) \\(\\beta_0\\) and \\(\\beta_1\\) specify the functional relationship \\(f\\) between \\(Y\\) and \\(X\\).\nThe first parameter \\(\\beta_0\\) determines the intercept of the regression line, and \\(\\beta_1\\) indicates the slope.\nOnce again, \\(\\epsilon\\) captures the model error, which is equivalent to the sum of all distances of the data points from the regression line.\n\n\n\n\n\n\n\n\n\n\nSuppose we are interested in modelling the relationship between RT and Length. Then the updated model equation would have the form\n\\[ \\text{Reaction time} = \\beta_0 + \\beta_1\\text{Length} + \\text{Model Error}. \\]\nBut how do we find the exact values of the intercept and the slope? In short: We can’t! We are dealing with population parameters and can, therefore, only provide an approximation of the true relationship between RT and Length.\nTo reflect the tentative nature of the model coefficients, we use the hat symbol ^ (e.g., \\(\\hat{\\beta_0}\\)) to indicate estimations rather than true values. The estimation procedure requires training data, based on which the algorithm “learns” the relationship between \\(Y\\) and \\(X\\) (hence the term “Machine Learning”).\n\n\nOrdinary least squares\nThe most common way of estimating parameters for linear models is the Least Squares approach. In essence, the parameters are chosen such that the residual sum of squares, i.e., the sum of the differences between observed and predicted values, is as low as possible. In other words, we are trying to minimise the distances between the data points and the regression line.\nThe slope can be computed using the equivalence in Equation 3:\n\\[ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i- \\bar{y})}{\\sum_{i=1}^n(x_i- \\bar{x})^2}.\n\\tag{3}\\]\n\n# Define X and Y\nx &lt;- ELP$Length\ny &lt;- ELP$RT\n\n# Compute means\nx_bar &lt;- mean(x)\ny_bar &lt;- mean(y)\n\n# Compute beta_1 (slope)\nbeta_1 &lt;- sum((x - x_bar) * (y - y_bar)) / sum((x - x_bar)^2)\n\nprint(beta_1)\n\n[1] 28.2816\n\n\nWe can then obtain the intercept via Equation 4.\n\\[\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta}_1\\bar{x}\n\\tag{4}\\]\n\n# Compute beta_0 (intercept)\nbeta_0 &lt;- y_bar - beta_1 * x_bar\n\nprint(beta_0)\n\n[1] 554.3454\n\n\nOnce we’ve fitted the model, we can then predict reaction times if we know the frequency of a lexical stimulus:\n\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x,\n\\tag{5}\\]\nwhere \\(\\hat{y}\\) indicates a prediction of \\(Y\\) on the basis of the predictor values \\(X = x\\). For example, what reaction time would we expect to see for a long word with 14 letters?\n\n# Make predictions\ny_hat &lt;- beta_0 + beta_1 * 14\n\nprint(y_hat)\n\n[1] 950.2879\n\n\n\n\nApplication in R\nWhile it is certainly possible to run a linear regression by hand, R’s lm() function provides a faster method method that additionally computes an abundanceo of helpful statistics.\n\n# Fit linear model\nrt.lm1 = lm(RT ~ Length, data = ELP)\n\n# View model data\nsummary(rt.lm1)\n\n\nCall:\nlm(formula = RT ~ Length, data = ELP)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-266.66  -79.34  -13.83   62.02  457.35 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  554.345     13.138   42.19   &lt;2e-16 ***\nLength        28.282      1.536   18.41   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 107.3 on 878 degrees of freedom\nMultiple R-squared:  0.2784,    Adjusted R-squared:  0.2776 \nF-statistic: 338.8 on 1 and 878 DF,  p-value: &lt; 2.2e-16\n\n\nThe model statistics comprise the following elements:\n\n\n\n\n\n\nCall\n\n\n\n\n\ni.e., the model formula.\n\n\n\n\n\n\n\n\n\nResiduals\n\n\n\n\n\nThese indicate the difference between the observed values in the data set and the values predicted by the model (= the fitted values). These correspond to the error term \\(\\epsilon\\). The lower the residuals, the better the model describes the data.\n\n# Show fitted values (= predictions) for the first six observations\nhead(rt.lm1$fitted.values)\n\n       1        2        3        4        5        6 \n752.3166 837.1615 837.1615 780.5982 724.0350 695.7534 \n\n# Show deviation of the fitted values from the observed values\nhead(rt.lm1$residuals)\n\n          1           2           3           4           5           6 \n  38.553355 -144.611456  123.288544   -9.468249  158.464958  -49.903438 \n\n\n\n\n\n\n\n\n\n\n\nCoefficients\n\n\n\n\n\nThe regression coefficients correspond to \\(\\hat{\\beta}_0\\) (“Intercept”) and \\(\\hat{\\beta}_1\\) (“log(Freq)”), respectively. The model shows that for a one-unit increase in log-frequency the log-reaction time decreases by approx. 0.05.\n\n# Convert coefficients to a tibble (= tidyverse-style data frame)\ntidy_model &lt;- tidy(rt.lm1)\n\ntidy_model\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    554.      13.1       42.2 2.03e-213\n2 Length          28.3      1.54      18.4 3.06e- 64\n\n\n\n\n\n\n\n\n\n\n\n\\(p\\)-values and \\(t\\)-statistic\n\n\n\n\n\nThe model tests the null hypothesis \\(H_0\\) that there is no relationship between RT and Length (i.e., \\(H_0: \\beta_1 = 0\\)). A \\(p\\)-value lower than 0.05 indicates that \\(\\beta_1\\) considerably deviates from 0, thus providing evidence for the alternative hypothesis \\(H_1: \\beta_1 \\ne 0\\). Since \\(p &lt; 0.001\\), we can reject \\(H_0\\).\nThe \\(p\\)-value itself crucially depends on the \\(t\\)-statistic2, which measures “the number of standard deviations that \\(\\hat{\\beta_1}\\) is away from 0” (James et al. 2021: 67). The standard error (SE) reflects how much an estimated coefficient differs on average from the true values of \\(\\beta_0\\) and \\(\\beta_1\\). They can be used to compute the 95% confidence interval \\[[\\hat{\\beta}_1 - 2 \\cdot SE(\\hat{β}_1), \\hat{\\beta}_1 + 2 \\cdot SE(\\hat{\\beta}_1)].\n\\tag{6}\\]\nThe true value of the parameter \\(\\beta_1\\) lies within the specified range 95% of the time.\n\n# Compute confidence intervals for intercept and log(Freq)\ntidy_model_ci &lt;- tidy(rt.lm1, conf.int = TRUE)\n\ntidy_model_ci\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    554.      13.1       42.2 2.03e-213    529.      580. \n2 Length          28.3      1.54      18.4 3.06e- 64     25.3      31.3\n\n\nThe estimated parameter for log(Freq), which is -0.049, thus has the 95% confidence interval [-0.053, -0.044].\n\n\n\n2 If the response variable is unknown or irrelevant, we speak of unsupervised machine learning. Unsupervised models are mostly concerned with finding patterns in high-dimensional datasets with dozens or even hundreds of variables.\n\n\n\n\n\nResidual standard error (RSE)\n\n\n\n\n\nThis is an estimation of the average deviation of the predictions from the observed values.\n\\[RSE = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n (y_i - \\hat{y_i}})^2\n\\tag{7}\\]\n\n\n\n\n\n\n\n\n\n\\(R^2\\)\n\n\n\n\n\nThe \\(R^2\\) score is important for assessing model fit because it “measures the proportion of variability in \\(Y\\) that can be explained using \\(X\\)” (James et al. 2021: 70), varying between 0 and 1.\n\\[R^2 = 1-\\frac{TSS}{RSS} = 1-\\frac{\\sum_{i=1}^n (y_i - \\hat{y_i})^2}{\\sum_{i=1}^n (y_i - \\bar{y_i})^2}\n\\tag{8}\\]\n\n\n\n\n\n\n\n\n\n\\(F\\)-statistic\n\n\n\n\n\nIt is used to measure the association between the dependent variable and the independent variable(s). Generally speaking, values greater than 1 indicate a possible correlation. A sufficiently low \\(p\\)-value suggests that the null hypothesis \\(H_0: \\beta_1 = 0\\) can be rejected. The \\(F\\) statistic is computed as shown below (cf. Agresti and Kateri 2022: 232) and follows an \\(F\\)-distribution with two different \\(df\\) values.\n\\[\nF = \\frac{(TSS - SSE) / p}{SSE / [n - (p + 1)]}\n\\tag{9}\\]\n\n\n\n\n\nMultiple linear regression\nIn multiple linear regression, more than one predictor variable is taken into account. For instance, modelling RT as a function of Length, Freq and POS requires a more complex model of the form\n\\[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon.\n\\tag{10}\\]\nIn more concrete terms, we need to the find the slope coefficients for all predictors in\n\\[ Y = \\beta_0 + \\beta_1 \\text{Length} + \\beta_2 \\text{Freq} + \\beta_3 \\text{POS} + \\epsilon,\n\\] which jointly minimise the distances between data points and the regression line.\n\n\n\n\n\n\nLeast squares in matrix form\n\n\n\n\n\nLet \\(\\mathbf{y}\\) denote the response vector\n\\[\\mathbf{y} =\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n... \\\\\ny_n\n\\end{bmatrix},\n\\]\nwhich contains the outcomes in the training data.\n\n# Response vector y\ny &lt;- ELP$RT\n\nThe predictors are stored in the the \\(N \\times (p + 1)\\) feature matrix \\(\\mathbf{X}\\):\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 &x_{11} & x_{12} & ... & x_{1p-1} \\\\\n1 &x_{21} & x_{22} & ... & x_{2p-1}\\\\\n1 &...  & ... & ... & ...\\\\\n1 & x_{n1} & x_{n2} & ... & x_{np-1}\n\\end{bmatrix}\n\\]\n\n# Design matrix X\n# Note that categorical variables are converted to binary indicator variables (1 = yes vs. 0 = no; also known as \"dummy variables\").\nX &lt;- model.matrix(RT ~ Length + Freq + POS, data = ELP)\n\nhead(X)\n\n  (Intercept) Length Freq POSNN POSVB\n1           1      7 0.96     1     0\n2           1     10 4.24     1     0\n3           1     10 0.04     0     1\n4           1      8 1.49     1     0\n5           1      6 1.06     1     0\n6           1      5 3.33     1     0\n\n\nWe are looking for the parameter vector\n\\[\n\\boldsymbol{\\beta} =\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n... \\\\\n\\beta_p\n\\end{bmatrix}\n\\] that minimises the residual sum of squares. The matrix form of the linear regression equations can thus be described as\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}.\n\\tag{11}\\] After some rearrangement and differentiation, the closed-form solution for the optimal model parameters can be obtained as follows:\n\\[\n\\boldsymbol{\\hat{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}  \\mathbf{X}^\\top \\mathbf{y}.\n\\tag{12}\\]\n\n# Apply the matrix formula\n# %*% indicates matrix multiplication\n# t() transposes (switches rows and columns of) a matrix (i.e. X^T)\n# solve() computes the matrix inverse (X^-1)\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\n\n# Print coefficient vector\nprint(beta_hat)\n\n                   [,1]\n(Intercept) 585.0528139\nLength       27.0930549\nFreq         -0.1027148\nPOSNN       -19.1010140\nPOSVB       -39.6039643\n\n\n\n\n\n\n\nApplication in R\nIn R, a multiple regression model is fitted as in the code example below:\n\n# Fit multiple regression model\nrt.lm2 &lt;- lm(RT ~ Length + Freq + POS, data = ELP)\n\n# View model statistics\nsummary(rt.lm2)\n\n\nCall:\nlm(formula = RT ~ Length + Freq + POS, data = ELP)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-263.71  -78.29  -11.43   60.72  454.08 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 585.05281   15.99324  36.581  &lt; 2e-16 ***\nLength       27.09305    1.54599  17.525  &lt; 2e-16 ***\nFreq         -0.10271    0.03945  -2.604  0.00938 ** \nPOSNN       -19.10101    9.64843  -1.980  0.04805 *  \nPOSVB       -39.60396   11.55762  -3.427  0.00064 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 106.3 on 875 degrees of freedom\nMultiple R-squared:  0.2944,    Adjusted R-squared:  0.2912 \nF-statistic: 91.29 on 4 and 875 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "6. Statistical Modelling",
      "6.1 Linear regression"
    ]
  },
  {
    "objectID": "chapters/06-linear_regression.html#visualising-regression-models",
    "href": "chapters/06-linear_regression.html#visualising-regression-models",
    "title": "6.1 Linear regression",
    "section": "Visualising regression models",
    "text": "Visualising regression models\n\nPlot coefficient estimates:\n\n\n\nCode\n# Tidy the model output\ntidy_model &lt;- tidy(rt.lm2, conf.int = TRUE)\n\n# Remove intercept\ntidy_model &lt;- tidy_model %&gt;% filter(term != \"(Intercept)\")\n\n# Create the coefficient plot\nggplot(tidy_model, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient Estimate\",\n    y = \"Predictor\",\n    title = \"Coefficient Estimates with Confidence Intervals\"\n  )\n\n\n\n\n\n\n\n\n\n\nPlot predictions:\n\n\n\nCode\nplot(ggeffect(rt.lm2, \"Freq\")) + geom_line(col = \"steelblue\") + labs(subtitle = \"Freq\", y = \"Predicted RT\")\n\n\n\n\n\n\n\n\n\nCode\nplot(ggeffect(rt.lm2, \"POS\")) + geom_line() + labs(y = \"Predicted RT\")\n\n\n\n\n\n\n\n\n\nCode\nplot(ggeffect(rt.lm2, \"Length\")) + geom_line(col = \"steelblue\") + labs(y = \"Predicted RT\")",
    "crumbs": [
      "6. Statistical Modelling",
      "6.1 Linear regression"
    ]
  },
  {
    "objectID": "chapters/06-linear_regression.html#model-assumptions-and-diagnostics",
    "href": "chapters/06-linear_regression.html#model-assumptions-and-diagnostics",
    "title": "6.1 Linear regression",
    "section": "Model assumptions and diagnostics",
    "text": "Model assumptions and diagnostics\nAs a parametric method, linear regression makes numerous assumptions about the training data. It is, therefore, essential to run further tests to rule out possible violations. Among other things, the model assumptions include:\n\nA linear relationship between the response and the quantitative predictors: The residuals should not display a clear pattern. For this reason, it is recommended to use component residual plots (e.g., crPlot() from the car library) for the visual identification of potentially non-linear trends.\n\n\n\nCode\n# pink line = main tendency vs. blue line = slope coefficients;\n# some minor non-linearity can be observed\n\ncrPlot(rt.lm2, var = \"Length\") # potentially problematic\n\n\n\n\n\n\n\n\n\n\nNo heteroscedasticity (i.e, non-constant variance of error terms): Visually, a violation of this assumption becomes apparent if the residuals form a funnel-like shape. It is also possible to conduct a non-constant variance test ncvTest(): If it returns \\(p\\)-values &lt; 0.05, it suggests non-constant variance.\n\n\n\nCode\nplot(rt.lm2, which = 1)\n\n\n\n\n\n\n\n\n\nCode\nncvTest(rt.lm2) # significant, meaning that errors do not vary constantly\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 29.97601, Df = 1, p = 4.3743e-08\n\n\n\nNo multicollinearity: Predictors should not be correlated with each other. In the model data, correlated variables have unusually high standard errors, thereby decreasing the explanatory power of both the coefficients and the model as a whole. Another diagnostic measure are variance inflation factors (VIF-scores); predictors with VIF scores &gt; 5 are potentially collinear. They can be computed using the vif() function.\n\n\n\nCode\nvif(rt.lm2) # vif &lt; 5 indicates that predictors are not correlated\n\n\n           GVIF Df GVIF^(1/(2*Df))\nLength 1.031876  1        1.015813\nFreq   1.021544  1        1.010714\nPOS    1.023296  2        1.005774\n\n\n\nNormally distributed residuals: The residuals should follow the normal distribution and be centered around 0:\n\n\\[\n\\epsilon \\sim N(0, \\sigma^2)\n\\tag{13}\\]\nUsually, a visual inspection using qqnorm() is sufficient, but the Shapiro-Wilke test shapiro.test() can also be run on the model residuals. Note that a \\(p\\)-value below 0.05 provides evidence for non-normality.\n\n\nCode\nplot(rt.lm2, which = 2)\n\n\n\n\n\n\n\n\n\nCode\nshapiro.test(residuals(rt.lm2)) # residuals are not normally distributed because p &lt; 0.05\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(rt.lm2)\nW = 0.97, p-value = 1.747e-12",
    "crumbs": [
      "6. Statistical Modelling",
      "6.1 Linear regression"
    ]
  },
  {
    "objectID": "chapters/02-first_steps.html",
    "href": "chapters/02-first_steps.html",
    "title": "2.1 First steps",
    "section": "",
    "text": "When it comes to data analysis, learning R offers an overwhelming number of short- and long-term advantages over conventional spreadsheet software such as Microsoft Excel or LibreOffice Calc:\n\nFirst of all, it’s completely free. There’s no need to obtain any expensive licenses, as it is the case for commercial software such as SPSS or MS Excel.\nR makes it very easy to document and share every step of the analysis, thereby facilitating reproducible workflows.\nLarge (and by that I mean extremely large!) datasets pose no problems whatsoever. Loading tabular data with hundreds of thousands (or even millions) of rows only takes a few seconds, whereas most other software would crash.\nThere are numerous extensions that provide tailored functions for corpus linguistics that aren’t available in general-purpose spreadsheet software. This allows us to work with corpora, use complex search expression, perform part-of-speech annotation, dependency parsing, and much more – all from within R.\nR’s ggplot2 offers an incredibly powerful framework for data visualisation. Don’t believe it? Check out the ggplot2 gallery.\nThe CRAN repository features more than 20,000 packages that can be installed to expand the functionality of R almost indefinitely. Should none of them meet your needs, R gives you the tools to comfortably write and share your own functions and packages.",
    "crumbs": [
      "2. Introduction to R",
      "2.1 First steps"
    ]
  },
  {
    "objectID": "chapters/02-first_steps.html#why-learn-r-to-begin-with",
    "href": "chapters/02-first_steps.html#why-learn-r-to-begin-with",
    "title": "2.1 First steps",
    "section": "",
    "text": "When it comes to data analysis, learning R offers an overwhelming number of short- and long-term advantages over conventional spreadsheet software such as Microsoft Excel or LibreOffice Calc:\n\nFirst of all, it’s completely free. There’s no need to obtain any expensive licenses, as it is the case for commercial software such as SPSS or MS Excel.\nR makes it very easy to document and share every step of the analysis, thereby facilitating reproducible workflows.\nLarge (and by that I mean extremely large!) datasets pose no problems whatsoever. Loading tabular data with hundreds of thousands (or even millions) of rows only takes a few seconds, whereas most other software would crash.\nThere are numerous extensions that provide tailored functions for corpus linguistics that aren’t available in general-purpose spreadsheet software. This allows us to work with corpora, use complex search expression, perform part-of-speech annotation, dependency parsing, and much more – all from within R.\nR’s ggplot2 offers an incredibly powerful framework for data visualisation. Don’t believe it? Check out the ggplot2 gallery.\nThe CRAN repository features more than 20,000 packages that can be installed to expand the functionality of R almost indefinitely. Should none of them meet your needs, R gives you the tools to comfortably write and share your own functions and packages.",
    "crumbs": [
      "2. Introduction to R",
      "2.1 First steps"
    ]
  },
  {
    "objectID": "chapters/02-first_steps.html#installing-r",
    "href": "chapters/02-first_steps.html#installing-r",
    "title": "2.1 First steps",
    "section": "2 Installing R",
    "text": "2 Installing R\nThe first step involves downloading the R programming language itself. The link will take you to the homepage of the Comprehensive R Archive Network (CRAN) where you can download the binary distribution. Choose the one that corresponds to your operating system (Windows/MAC/Linux).\n\n\n\n\n\n\nInstallation instructions for Windows users\n\n\n\n\n\nClick “Download R for Windows” \\(\\rightarrow\\) Select “base” \\(\\rightarrow\\) Click on “Download R-4.4.1 for Windows” (or whatever most recent version is currently displayed).\nOpen the set-up file you’ve just downloaded and simply follow the instructions on screen. It’s fine to go with the default options.\nVideo tutorial on YouTube\n\n\n\n\n\n\n\n\n\nInstallation instructions for MacOS users\n\n\n\n\n\nClick “Download R for macOS” \\(\\rightarrow\\) Select the latest release for your OS\nOpen the downloaded .pkg file and follow the instructions in the installation window.\nVideo tutorial on YouTube",
    "crumbs": [
      "2. Introduction to R",
      "2.1 First steps"
    ]
  },
  {
    "objectID": "chapters/02-first_steps.html#installing-rstudio",
    "href": "chapters/02-first_steps.html#installing-rstudio",
    "title": "2.1 First steps",
    "section": "3 Installing RStudio",
    "text": "3 Installing RStudio\nYou can now download and install RStudio. RStudio is a so-called “Integrated Development Environment” (IDE), which will provide us with a variety of helpful tools to write and edit code comfortably. If R was a musical instrument, then RStudio would be the recording studio, so-to-speak.",
    "crumbs": [
      "2. Introduction to R",
      "2.1 First steps"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "This collection of handouts provides a hands-on introduction to data analysis and statistical methods in quantitative corpus linguistics with R. It is designed with accessibility in mind, assuming no prior knowledge of programming or statistics. All you need to get started is a laptop; everything else will be explained within these pages.\nPrimarily, this reader is geared towards students attending the classes Language Variation (BA) and Statistics for Linguistics (MA) at the Catholic University of Eichstätt-Ingolstadt (Germany). However, it is also meant to equip students currently working on their BA/MA/PhD theses with the tools they need to conduct empirical studies on a wide array of linguistic phenomena. The methods presented here reflect the state-of-the-art in corpus-linguistic research, providing readers with current and relevant analytical techniques."
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Preface",
    "section": "",
    "text": "This collection of handouts provides a hands-on introduction to data analysis and statistical methods in quantitative corpus linguistics with R. It is designed with accessibility in mind, assuming no prior knowledge of programming or statistics. All you need to get started is a laptop; everything else will be explained within these pages.\nPrimarily, this reader is geared towards students attending the classes Language Variation (BA) and Statistics for Linguistics (MA) at the Catholic University of Eichstätt-Ingolstadt (Germany). However, it is also meant to equip students currently working on their BA/MA/PhD theses with the tools they need to conduct empirical studies on a wide array of linguistic phenomena. The methods presented here reflect the state-of-the-art in corpus-linguistic research, providing readers with current and relevant analytical techniques."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Preface",
    "section": "Overview",
    "text": "Overview\nWe begin by establishing the general structure of corpus-linguistic studies, accompanied by key theoretical and practical considerations. The second section introduces R as an analytical tool, covering its core functionality. With these fundamentals in place, we proceed to query corpora directly within R. Chapters four and five focus on describing discrete and continuous outputs, as well as identifying meaningful associations and differences in the data. Finally, to gain more nuanced insights into potential patterns, we apply common machine learning algorithms to fit, evaluate, and interpret statistical models.\nThroughout this journey, readers will develop the skills to conduct robust corpus-linguistic analyses, from basic querying to advanced statistical modeling."
  },
  {
    "objectID": "index.html#collaborators",
    "href": "index.html#collaborators",
    "title": "Preface",
    "section": "Collaborators",
    "text": "Collaborators\nVladimir Buskin, PhD student at the Department of English Language and Linguistics, Catholic University of Eichstätt-Ingolstadt\nThomas Brunner, Assistant Professor at the Department of English Language and Linguistics, Catholic University of Eichstätt-Ingolstadt\nPhilippa Adolf, PhD student at the Department of Romance Studies, University of Vienna"
  },
  {
    "objectID": "Basics.html",
    "href": "Basics.html",
    "title": "An example from sociolinguistics",
    "section": "",
    "text": "New quotatives in American English: be like, go, be all:\n\n“I was like ‘well if it’s not leaking, I don’t care.’”\n“Do you, I asked I was running out and I go, ‘you’re trying to deliver a pizza here?’”\n“My sister’s all ‘excuse me would you mind if I gave you, if I want your autograph.’”\n\n\n\n\n\nPrevious results with regard to age: Be like is known to be preferred by younger speakers.\nPrevious results with regard to sex are inconclusive.\n\n\n\n\n\n“The study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all” (Barbieri 2007: 26).\n\n\n\n\n\nChoice of corpus: 450,000 words taken from the Conversation component of the Longman Spoken and Written English Corpus (all files where the age and gender of speakers were available), i.e., about 50 hours of casual conversation coded for speakers’ age and sex.\nData extraction: All forms of be like, be all, go, say extracted using corpus linguistic software; data entered into spreadsheet software; word counts of texts added via Delphi program.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuotative\nAge group\nMale use (per 100,000 words)\nFemale use (per 100,000 words)\n\n\n\n\nsay\n16–26\n115\n149\n\n\n\n27–40\n101\n117\n\n\n\n41–54\n55\n214\n\n\n\n55+\n67\n433\n\n\nbe like\n16–26\n32\n329\n\n\n\n27–40\n66\n25\n\n\n\n41–54\n-\n1\n\n\n\n55+\n-\n19\n\n\ngo\n16–26\n25\n113\n\n\n\n27–40\n61\n18\n\n\n\n41–54\n-\n15\n\n\n\n55+\n-\n7\n\n\nbe all\n16–26\n14\n33\n\n\n\n\n\n\n\n\nReproduction of Barbieri’s Figure 5 ‘Proportional quotative use by men and women aged 16–26 and 27–40’\n\n\n\n\n\n\n\n\n“In sum, the patterns of proportional use and the results of the statistical analyses show that there are striking differences in the way that men and women under the age of forty use quotative verbs. In contrast, in conforming to the traditional quotative say, men and women over forty displayed similar behavior” (Barbieri 2007: 38–39).\n\n\n\n\n\n“In favoring be like over other quotatives, men in their late 20s and in their 30s display an ‘affinity’ with slightly younger women, women in their early 20s, rather than with women of their same age” (Barbieri 2007: 41)."
  },
  {
    "objectID": "Basics.html#barbierioldermenyounger2007-older-men-and-younger-women",
    "href": "Basics.html#barbierioldermenyounger2007-older-men-and-younger-women",
    "title": "An example from sociolinguistics",
    "section": "",
    "text": "New quotatives in American English: be like, go, be all:\n\n“I was like ‘well if it’s not leaking, I don’t care.’”\n“Do you, I asked I was running out and I go, ‘you’re trying to deliver a pizza here?’”\n“My sister’s all ‘excuse me would you mind if I gave you, if I want your autograph.’”\n\n\n\n\n\nPrevious results with regard to age: Be like is known to be preferred by younger speakers.\nPrevious results with regard to sex are inconclusive.\n\n\n\n\n\n“The study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all” (Barbieri 2007: 26).\n\n\n\n\n\nChoice of corpus: 450,000 words taken from the Conversation component of the Longman Spoken and Written English Corpus (all files where the age and gender of speakers were available), i.e., about 50 hours of casual conversation coded for speakers’ age and sex.\nData extraction: All forms of be like, be all, go, say extracted using corpus linguistic software; data entered into spreadsheet software; word counts of texts added via Delphi program.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuotative\nAge group\nMale use (per 100,000 words)\nFemale use (per 100,000 words)\n\n\n\n\nsay\n16–26\n115\n149\n\n\n\n27–40\n101\n117\n\n\n\n41–54\n55\n214\n\n\n\n55+\n67\n433\n\n\nbe like\n16–26\n32\n329\n\n\n\n27–40\n66\n25\n\n\n\n41–54\n-\n1\n\n\n\n55+\n-\n19\n\n\ngo\n16–26\n25\n113\n\n\n\n27–40\n61\n18\n\n\n\n41–54\n-\n15\n\n\n\n55+\n-\n7\n\n\nbe all\n16–26\n14\n33\n\n\n\n\n\n\n\n\nReproduction of Barbieri’s Figure 5 ‘Proportional quotative use by men and women aged 16–26 and 27–40’\n\n\n\n\n\n\n\n\n“In sum, the patterns of proportional use and the results of the statistical analyses show that there are striking differences in the way that men and women under the age of forty use quotative verbs. In contrast, in conforming to the traditional quotative say, men and women over forty displayed similar behavior” (Barbieri 2007: 38–39).\n\n\n\n\n\n“In favoring be like over other quotatives, men in their late 20s and in their 30s display an ‘affinity’ with slightly younger women, women in their early 20s, rather than with women of their same age” (Barbieri 2007: 41)."
  },
  {
    "objectID": "Basics.html#principles-of-empirical-linguistics",
    "href": "Basics.html#principles-of-empirical-linguistics",
    "title": "An example from sociolinguistics",
    "section": "Principles of Empirical Linguistics",
    "text": "Principles of Empirical Linguistics\n\nObjectivity — Independence from researchers or devices (→ replicability!)\nReliability — Studies should be replicable.\nValidity — A study must actually address the problem formulated in the research question."
  },
  {
    "objectID": "Basics.html#exercises",
    "href": "Basics.html#exercises",
    "title": "An example from sociolinguistics",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1 As you read through the sections above, think about what might go wrong at each stage of the study!\n\n\nExercise 2 Read another sociolinguistic study (e.g., Unuabonah and Gut 2018). As you read, identify similar key sections or “building blocks” used in this study (e.g., introduction, research question etc.). What purpose does each section serve in the study’s overall structure?"
  },
  {
    "objectID": "chapters/04-descriptive_stats.html",
    "href": "chapters/04-descriptive_stats.html",
    "title": "4.3 Descriptive statistics",
    "section": "",
    "text": "Theoretical introduction:\n\nBaguley (2012, chap. 1 & 6)\nHeumann et al. (2022: Chapter 3)\n\nApplication in R:\n\nWickham et al. (2023: Chapter 1)\n\nStyle guide:\n\ne.g., APA Numbers and Statistics Guide (7th ed.)",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.3 Descriptive statistics"
    ]
  },
  {
    "objectID": "chapters/04-descriptive_stats.html#preparation",
    "href": "chapters/04-descriptive_stats.html#preparation",
    "title": "4.3 Descriptive statistics",
    "section": "Preparation",
    "text": "Preparation\nThis unit draws on the genitive alternation data compiled by Grafmiller (2023) and previously used for publication in Grafmiller (2014).\n\n# Libraries\nlibrary(tidyverse) # for fancy plots and comfortable data manipulation\n# For publication-ready tables\nlibrary(crosstable)\nlibrary(flextable)\n\n# Load data from working directory\ngenitive &lt;- read.csv(\"Grafmiller_genitive_alternation.csv\", sep = \"\\t\")\n\n# Check the structure of the data frame\nhead(genitive)",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.3 Descriptive statistics"
    ]
  },
  {
    "objectID": "chapters/04-descriptive_stats.html#describing-categorical-data",
    "href": "chapters/04-descriptive_stats.html#describing-categorical-data",
    "title": "4.3 Descriptive statistics",
    "section": "Describing categorical data",
    "text": "Describing categorical data\nA categorical variable is made up of two or more discrete values. An intuitive way to describe categorical data would be to count how often each category occurs in the sample. These counts are then typically summarised in frequency tables and accompanied by suitable graphs (e.g., barplots).\n\nFrequency tables (one variable)\nAssume we are interested in how often each clause ordering type ( \"mc-sc\" vs. \"sc-mc\") is attested in our data. In R, we can obtain their frequencies by inspecting the ORDER column of the cl.order dataset. Since manual counting isn’t really an option, we will make use of the convenient functions table() and xtabs().\n\n\n\n\n\n\nThe workhorse: table()\n\n\n\n\n\nThis function requires a character vector. We use the notation cl.order$ORDER to subset the cl.order data frame according to the column ORDER (cf. data frames). We store the results in the variable order_freq1 (you may choose a different name if you like) and display the output by applying to it the print() function.\n\n# Count occurrences of ordering types (\"mc-sc\" and \"sc-mc\") in the data frame\norder_freq1 &lt;- table(cl.order$ORDER) \n\n# Print table\nprint(order_freq1)\n\n\nmc-sc sc-mc \n  275   128 \n\n\n\n\n\n\n\n\n\n\n\nMore detailed: xtabs()\n\n\n\n\n\nAlternatively, you could use xtabs() to achieve the same result. The syntax is a little different, but it returns a slightly more more detailed table with explicit variable label(s).\n\n# Count occurrences of ordering types (\"mc-sc\" and \"sc-mc\")\norder_freq2 &lt;- xtabs(~ ORDER, cl.order)\n\n# Print table\nprint(order_freq2)\n\nORDER\nmc-sc sc-mc \n  275   128 \n\n\n\n\n\n\n\nFrequency tables (\\(\\geq\\) 2 variables)\nIf we are interested in the relationship between multiple categorical variables, we can cross-tabulate the frequencies of their categories. For example, what is the distribution of clause order depending on the type of subordinate clause? The output is also referred to as a contingency table.\n\n\n\n\n\n\nThe table() way\n\n\n\n\n\n\n# Get frequencies of ordering tpyes (\"mc-sc\" vs. \"sc-mc\") depending on the type of subordinate clause (\"caus\" vs. \"temp\")\norder_counts1 &lt;- table(cl.order$ORDER, cl.order$SUBORDTYPE)\n\n# Print contingency table\nprint(order_counts1)\n\n       \n        caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n\n\n\n\n\n\n\n\n\n\n\nThe xtabs() way\n\n\n\n\n\n\n# Cross-tabulate ORDER and SUBORDTYPE\norder_counts2 &lt;- xtabs(~ ORDER + SUBORDTYPE, cl.order)\n\n# Print cross-table\nprint(order_counts2)\n\n       SUBORDTYPE\nORDER   caus temp\n  mc-sc  184   91\n  sc-mc   15  113\n\n\n\n\n\n\n\nPercentage tables\nThere are several ways to compute percentages for your cross-tables, but by far the simplest is via the prop.table() function. As it only provides proportions, you can multiply the output by 100 to obtain real percentages.\n\n\n\n\n\n\nGet percentages for a table() object\n\n\n\n\n\n\n# Convert to % using the prop.table() function\npct1 &lt;- prop.table(order_counts1) * 100\n\n# Print percentages\nprint(pct1)\n\n       \n             caus      temp\n  mc-sc 45.657568 22.580645\n  sc-mc  3.722084 28.039702\n\n\n\n\n\n\n\n\n\n\n\nGet percentages for an xtabs() object\n\n\n\n\n\n\n# Convert to % using the prop.table() function\npct2 &lt;- prop.table(order_counts2) * 100\n\n# Print percentages\nprint(pct2)\n\n       SUBORDTYPE\nORDER        caus      temp\n  mc-sc 45.657568 22.580645\n  sc-mc  3.722084 28.039702\n\n\nNotice how pct2 still carries the variable labels SUBORDTYPE and ORDER, which is very convenient."
  },
  {
    "objectID": "chapters/04-descriptive_stats.html#plotting-categorical-data",
    "href": "chapters/04-descriptive_stats.html#plotting-categorical-data",
    "title": "4.3 Descriptive statistics",
    "section": "Plotting categorical data",
    "text": "Plotting categorical data\nThis section demonstrates both the in-built plotting functions of R (‘Base R’) as well as the more modern versions provided by the tidyverse package.\n\n\n\n\n\n\nMosaicplots (raw counts)\n\n\n\n\n\nA straightforward way to visualise a contingency table is the mosaicplot:\n\n# Works with raw counts and percentages\n# Using the output of xtabs() as input\nmosaicplot(gen_counts2, color = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarplots (raw counts)\n\n\n\n\n\nThe workhorse of categorical data analysis is the barplot. Base R functions usually require a table object as input, whereas ggplot2 can operate on the raw dataset.\n\nOne variable\n\nBase Rggplot2\n\n\n\nBase R barplot with barplot(); requires the counts as computed by tables() or xtabs()\n\n\n# Generate cross-table\ngen_freq1 &lt;- table(genitive$Type)\n\n# Create barplot\nbarplot(gen_freq1)\n\n\n\n\n\n\n\n\n\n\n\nBarplot with geom_bar() using the raw input data\n\n\n# Requirement: library(tidyverse)\n\n# Create barplot\nggplot(genitive, aes(x = Type)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo variables\nBivariate barplots can be obtained by either supplying a contingency table (Base R) or by mapping the second variable onto the fill argument using the raw data.\n\nBase RBase R (fully customised)ggplot2ggplot2 (fully customised)\n\n\n\n# Generate cross-table with two variables\ngen_counts2 &lt;- xtabs(~ Type + Genre, genitive)\n\n# Create simple barplot\nbarplot(gen_counts2, \n        beside = TRUE,  # Make bars side-by-side\n        legend = TRUE)  # Add a legend\n\n\n\n\n\n\n\n\n\n\n\n# Generate cross-table with two variables\ngen_counts2 &lt;- xtabs(~ Type + Genre, genitive)\n\n# Customise barplot with axis labels, colours and legend\nbarplot(gen_counts2, \n        beside = TRUE,  # Make bars dodged (i.e., side by side)\n        main = \"Distribution of Type by Genre (Base R)\", \n        xlab = \"Type\", \n        ylab = \"Frequency\", \n        col = c(\"lightblue\", \"lightgreen\"), # Customize colors\n        legend = TRUE,  # Add a legend\n        args.legend = list(title = \"Genre\", x = \"topright\"))\n\n\n\n\n\n\n\n\n\n\n\n# Requirement: library(tidyverse)\n\n# Create simple barplot with the ggplot() function\nggplot(genitive, aes(x = Type, fill = Genre)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\n# Requirement: library(tidyverse)\n\n# Fully customised ggplot2 object\nggplot(genitive, aes(x = Type, fill = Genre)) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    title = \"Genitive by genre\",\n    x = \"Genitive\",\n    y = \"Frequency\",\n    fill = \"Genre\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarplots (percentages)\n\n\n\n\n\n\nBase Rggplot2\n\n\nIn very much the same way as with the raw counts:\n\n# Create simple barplot with a percentage table as input\nbarplot(pct1, \n        beside = TRUE,  # Make bars side-by-side\n        legend = TRUE)  # Add a legend\n\n\n\n\n\n\n\n\n\n\nHere, a few tweaks are necessary. Because the ggplot() function prefers to works with data frames rather than cross-tables, we’ll have to coerce it into one first:\n\n# Convert a percentage table to a data frame\n# My recommendation: Use the pct2 object, which was generated using xtabs() because it will keep the variable names\npct2_df &lt;- as.data.frame(pct2)\n\nprint(pct2_df)\n\n   Type             Genre     Freq\n1    of Adventure Fiction 54.98652\n2     s Adventure Fiction 45.01348\n3    of   General Fiction 62.22548\n4     s   General Fiction 37.77452\n5    of           Learned 76.63185\n6     s           Learned 23.36815\n7    of       Non-fiction 64.46154\n8     s       Non-fiction 35.53846\n9    of             Press 44.51411\n10    s             Press 55.48589\n\n\nNow we can plot the percentages with geom_col(). This geom (= ‘geometric object’) allows us to manually specify what should be mapped onto the y-axis:\n\n# Requirement: library(tidyverse)\n\n# Create barplot with user-defined y-axis, which requires geom_col() rather than geom_bar()\nggplot(pct2_df, aes(x = Type, y = Freq, fill = Genre)) +\n  geom_col(position = \"dodge\") +\n  labs(y = \"Frequency (in %)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBubble plot (percentages)\n\n\n\n\n\n\n# Requirement: library(tidyverse)\n\n# Bubble plot\nggplot(pct2_df, aes(x = Type, y = Genre, size = Freq)) +\n  geom_point(color = \"skyblue\", alpha = 0.7) +\n  scale_size_continuous(range = c(5, 20)) +  # Adjust bubble size range\n  labs(title = \"Bubble Plot of ORDER by SUBORDTYPE\",\n       x = \"Type\",\n       y = \"Genre\",\n       size = \"Percentage\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlluvial plot (percentages)\n\n\n\n\n\n\n# Make sure to install this library prior to running the code below \nlibrary(ggalluvial)\n\nggplot(pct2_df,\n       aes(axis1 = Type, axis2 = Genre, y = Freq)) +\n  geom_alluvium(aes(fill = Type)) +\n  geom_stratum(fill = \"gray\") +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  labs(title = \"Alluvial Plot of Type by Genre\",\n       x = \"Categories\", y = \"Percentage\") +\n  theme_minimal()",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.3 Descriptive statistics"
    ]
  },
  {
    "objectID": "chapters/04-descriptive_stats.html#exporting-tables-to-ms-word",
    "href": "chapters/04-descriptive_stats.html#exporting-tables-to-ms-word",
    "title": "4.3 Descriptive statistics",
    "section": "Exporting tables to MS Word",
    "text": "Exporting tables to MS Word\nThe crosstable and flextable packages make it very easy to export elegant tables to MS Word.\n\n\n\n\n\n\nClean and to the point: crosstable()\n\n\n\n\n\nThis is perhaps the most elegant solution. Generate a crosstable() object by supplying at the very least …\n\nthe original dataset (data = genitive),\nthe dependent variable (cols = Type), and\nthe independent variable (by = Genre).\n\nYou can further specify …\n\nwhether to include column totals, row totals or both (here: total = both),\nthe rounding scheme (here: percent_digits = 2),\n…\n\n\n# Required libraries:\n# library(crosstable)\n# library(flextable)\n\n# Create the cross table\noutput1 &lt;- crosstable(data = genitive,\n                      cols = Type, \n                      by = Genre, \n                      total = \"both\",\n                      percent_digits = 2)\n\n# Generate file\nas_flextable(output1)\n\nlabelvariableGenreTotalAdventure FictionGeneral FictionLearnedNon-fictionPressTypeof408 (13.15%)425 (13.70%)587 (18.92%)1257 (40.51%)426 (13.73%)3103 (60.87%)s334 (16.74%)258 (12.93%)179 (8.97%)693 (34.74%)531 (26.62%)1995 (39.13%)Total742 (14.55%)683 (13.40%)766 (15.03%)1950 (38.25%)957 (18.77%)5098 (100.00%)\n\n\n\n\n\n\n\n\n\n\n\nHow much info do you need? Yes.\n\n\n\n\n\nIt also possible to use as_flextable() without pre-processing the data with crosstable(); supplying a table preferably created with xtabs() is sufficient. Without any doubt, the output is extremely informative, yet it is everything but reader-friendly.\nFor this reason, I recommend relying on the less overwhelming crosstable() option above if a plain and easy result is desired. However, readers who would like to leverage the full capabilities of the flextable() package and familiarise themselves with the abundant options for customisation, can find the detailed documentation here.\n\n# Requires the following library:\n# library(flextable)\n\n# Create a table\ntab1 &lt;- xtabs(~ Type + Genre, genitive)\n\n# Directly convert a table to a flextable with as_flextable()\noutput_1 &lt;- as_flextable(tab1)\n\n# Print output\nprint(output_1)",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.3 Descriptive statistics"
    ]
  },
  {
    "objectID": "chapters/04-descriptive_stats.html#measures-of-central-tendency",
    "href": "chapters/04-descriptive_stats.html#measures-of-central-tendency",
    "title": "4.3 Descriptive statistics",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\nFrom here on out, we assume \\(X\\) is a continuous random variable with observations \\(\\{x_1, x_2, ..., x_n\\}\\) and sample size \\(n\\). Measures of central tendency offer convenient one-value-summaries of the distribution of \\(X\\) as well as estimations of population parameters, granted some corrective steps.\n\nThe sample mean\nThe population mean \\(\\mu\\) can be approximated rather well by the sample mean\n\\[\n\\hat{\\mu} = \\frac{x_1 + x_2 + ... + x_n}{n} \\\\ = \\frac{1}{n}\\sum_{i=1}^n{x_i}.\n\\tag{1}\\]\nIn R, we can obtain the average value of a numeric vector with the mean() function.\nIn the genitive data, “the type–token ratio (TTR) over the five sentences preceding and following each token was calculated” (Grafmiller 2014: 479). Thus, on average, each sentence has a lexical density of\n\nmean(genitive$Type_Token_Ratio)\n\n[1] 74.74926\n\n\nThe output returned by this function provides a one-value summary of all observations contained in Type_Token_Ratio. Because the the mean \\(\\bar{x}\\) takes into account all data points, it is prone to the influence of outliers, i.e., extreme values.\nThe distribution of continuous variables is best visualised in terms of histograms or density plots, which are illustrated for Type_Token_Ratio. The blue line indicates the sample mean.\n\nHistogram (ggplot2)Density plot (ggplot2)Histogram (Base R)Density plot (Base R)\n\n\n\n# Plot distribution of Type_Token_Ratio\ngen_hist &lt;- ggplot(genitive, aes(x = Type_Token_Ratio)) +\n                  geom_histogram(binwidth = 1)\n\ngen_hist +\n  # Add mean\n  geom_vline(aes(xintercept = mean(Type_Token_Ratio)),\n             color = \"steelblue\",\n             linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n# Plot distribution of Type_Token_Ratio\ngen_dens &lt;- ggplot(genitive, aes(x = Type_Token_Ratio)) +\n                  geom_density()\n\ngen_dens +\n  # Add mean\n  geom_vline(aes(xintercept = mean(Type_Token_Ratio)),\n             color = \"steelblue\",\n             linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nhist(genitive$Type_Token_Ratio)\n  abline(v=mean(genitive$Type_Token_Ratio),lwd=3, col = \"steelblue\")\n\n\n\n\n\n\n\n\n\n\n\nplot(density(genitive$Type_Token_Ratio))\n  abline(v=mean(genitive$Type_Token_Ratio),lwd=3, col = \"steelblue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe median\nThe median() function computes the “the halfway point of the data (50% of the data are above the median; 50% of the data are below” (Winter 2020: 58). As such, it is the measure of choice for data with many outliers as well as for ordinal data (e.g. Likert-scale ratings).\n\\[\n\\tilde{x}_{0.5} =\n\\begin{cases}\nx_{((n+1)/2)} & \\text{if } n \\text{ is odd.} \\\\\n\\frac{1}{2}(x_{n/2}+x_{(n/2+1)}) & \\text{if } n \\text{ is even.}\n\\end{cases}\n\\tag{2}\\]\n\nmedian(genitive$Type_Token_Ratio)\n\n[1] 75\n\n\nThe median of Type_Token_Ratio is represented by the red vertical line.\n\nHistogram (ggplot2)Density plot (ggplot2)Histogram (Base R)Density plot (Base R)\n\n\n\ngen_hist +\n  # Add mean\n  geom_vline(aes(xintercept = mean(Type_Token_Ratio)), color = \"steelblue\", linewidth = 1) +\n  # Add median\n  geom_vline(aes(xintercept = median(Type_Token_Ratio)), color = \"red\", linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\ngen_dens +\n  # Add mean\n  geom_vline(aes(xintercept = mean(Type_Token_Ratio)), color = \"steelblue\", linewidth = 1) +\n  # Add median\n  geom_vline(aes(xintercept = median(Type_Token_Ratio)), color = \"red\", linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nhist(genitive$Type_Token_Ratio)\n  abline(v=mean(genitive$Type_Token_Ratio),lwd=3, col = \"steelblue\")\n  abline(v=median(genitive$Type_Token_Ratio),lwd=3, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nplot(density(genitive$Type_Token_Ratio))\n  abline(v=mean(genitive$Type_Token_Ratio),lwd=3, col = \"steelblue\")\n  abline(v=mean(genitive$Type_Token_Ratio),lwd=3, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample variance and standard deviation\nIn order to assess how well the mean represents the data, it is instructive to compute the variance var() and the standard deviation sd() for a sample.\nThe unbiased estimator of the population variance \\(\\sigma^2\\) is defined as\n\\[\n\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n{(x_i - \\hat{\\mu})^2}}{n-1}.\n\\tag{3}\\]\nIn other words, it represents the average squared deviation of all observations from the sample mean.\n\nvar(genitive$Type_Token_Ratio)\n\n[1] 28.34916\n\n\nCorrespondingly, the standard deviation \\(\\sigma\\) of the population mean can be estimated via the square root of the sample variance:\n\\[\n\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2} = \\sqrt{\\frac{\\sum_{i=1}^n{(x_i - \\hat{\\mu})^2}}{n-1}}\n\\tag{4}\\]\n\nsd(genitive$Type_Token_Ratio)\n\n[1] 5.324393\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Comparison of different parameter values\n\n\n\n\nExample 1Example 2\n\n\n\ngen_hist +\n  # Add verticle line for the mean\n  geom_vline(aes(xintercept = mean(Type_Token_Ratio)), color = \"steelblue\", linewidth = 1) +\n  # Add -1sd\n  geom_vline(aes(xintercept = mean(Type_Token_Ratio) - sd(Type_Token_Ratio)), color = \"orange\", linewidth = 1) +\n  # Add +1sd\n  geom_vline(aes(xintercept = mean(Type_Token_Ratio) + sd(Type_Token_Ratio)), color = \"orange\", linewidth = 1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n# Create data frame with mean and sd for each TYPE\ngen_mean_sd &lt;- genitive %&gt;% \n  # Select variables of interest\n  select(Type, Type_Token_Ratio) %&gt;% \n  # Group results of following operations by TYPE\n  group_by(Type) %&gt;% \n    # Create grouped summary of mean and sd for each TYPE\n    summarise(mean = mean(Type_Token_Ratio),\n                sd = sd(Type_Token_Ratio))\n\n# Plot results \nggplot(gen_mean_sd, aes(x = Type, y = mean)) +\n  # Barplot with a specific variable mapped onto y-axis\n  geom_col() +\n  # Add mean and standard deviation to the plot\n  geom_errorbar(aes(x = Type,\n                    ymin = mean-sd,\n                    ymax = mean+sd), width = .2) +\n  theme_classic() +\n  labs(y = \"Mean type/token ratios by genitive type\", x = \"Genitive type\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantiles\nWhile median() divides the data into two equal sets (i.e., two 50% quantiles), the quantile() function makes it possible to partition the data further.\n\nquantile(genitive$Type_Token_Ratio)\n\n      0%      25%      50%      75%     100% \n49.00000 71.00000 75.00000 78.00000 95.16129 \n\n\nquantile(x, 0) and quantile(x, 1) thus show the minimum and maximum values, respectively.\n\nquantile(genitive$Type_Token_Ratio, 0)\n\n0% \n49 \n\nquantile(genitive$Type_Token_Ratio, 1)\n\n    100% \n95.16129 \n\n\n\n\nQuartiles and boxplots\nFor each genitive variant, the distribution of type/token ratios across the dataset is visualised in the boxplots below. The thick horizontal line within each box represents the median of the distribution. The box itself spans the interquartile range (IQR), extending from the 25th to the 75th percentile. Data points that lie more than 1.5 times the IQR above or below the box are classified as outliers and are shown as individual dots.\n\nBoxplot (Base R)Boxplot (ggplot2)\n\n\n\nboxplot(Type_Token_Ratio ~ Type, genitive)\n\n\n\n\n\n\n\n\nTip: You can extract the outliers from the boxplot and match them to the original rows in the dataset as follows:\n\n# Save the boxplot object\nbp &lt;- boxplot(Type_Token_Ratio ~ Type, genitive)\n# Extract the outlier values\noutlier_values &lt;- bp$out\n\n# View them\nprint(outlier_values)\n\n# Match to the original rows in the data\noutliers_df &lt;- genitive[genitive$Type_Token_Ratio %in% outlier_values, ]\n\n# Show the rows\nprint(outliers_df)\n\n\n\n\nggplot(genitive, aes(x = Type, y = Type_Token_Ratio)) +\n  geom_boxplot() +\n  theme_classic()",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.3 Descriptive statistics"
    ]
  },
  {
    "objectID": "chapters/04-descriptive_stats.html#tier-1",
    "href": "chapters/04-descriptive_stats.html#tier-1",
    "title": "4.3 Descriptive statistics",
    "section": "Tier 1",
    "text": "Tier 1\n\nExercise 1 Load the dataset psycho_data which contains several distributional and psycholinguistic measurements for 407 verbs.\n\nlibrary(readxl)\npsycho_data &lt;- read_xlsx(\"psycholing_data.xlsx\")\n\nWord frequencies follow a very characteristic distribution. Create a histogram of Frequency and characterise its distribution using the sample mean and median.\n\n\nExercise 2 How does the overall distribution as well as the position of the mean/median change if the frequency counts are log-transformed (cf. the Log_frequency column)? Why do log-transformations have this effect?\n\n\nExercise 3 Plot the verbs’ concreteness ratings (1 = abstract, 5 = concrete) against their number of meanings using a scatterplot (geom_point()), and calculate Pearson’s \\(r\\) and \\(r^2\\). Briefly describe the relationship between Concreteness and Number_meanings.",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.3 Descriptive statistics"
    ]
  },
  {
    "objectID": "chapters/04-descriptive_stats.html#tier-2",
    "href": "chapters/04-descriptive_stats.html#tier-2",
    "title": "4.3 Descriptive statistics",
    "section": "Tier 2",
    "text": "Tier 2\n\nExercise 4 Plot the following variables from the genitive data and characterise the figures briefly:\n\nType and Possessor_Animacy2 (in %)\nType and Possessor_Givenness (in %)\nType and Possessor_NP_Type (in %)\n\nHint: The tidyverse syntax offers numerous convenient functions to handle common data analysis tasks in an elegant fashion. For instance, a pipeline for computing frequencies and percentages for Type by Genre could look like this:\n\ntype_genre &lt;- genitive %&gt;% \n                  group_by(Genre) %&gt;% \n                  count(Type) %&gt;% \n                  mutate(pct = n/sum(n))\n\nprint(type_genre)\n\n# A tibble: 10 × 4\n# Groups:   Genre [5]\n   Genre             Type      n   pct\n   &lt;chr&gt;             &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;\n 1 Adventure Fiction of      408 0.550\n 2 Adventure Fiction s       334 0.450\n 3 General Fiction   of      425 0.622\n 4 General Fiction   s       258 0.378\n 5 Learned           of      587 0.766\n 6 Learned           s       179 0.234\n 7 Non-fiction       of     1257 0.645\n 8 Non-fiction       s       693 0.355\n 9 Press             of      426 0.445\n10 Press             s       531 0.555\n\n\n\ntype_genre %&gt;% \n  ggplot(aes(x = Type, y = pct, fill = Genre)) +\n  geom_col(pos = \"dodge\")\n\n\n\n\n\n\n\n\n\n\nExercise 5 Nearly all plotting functions that use the ggplot() graphics engine support positional arguments for more than two variables, rendering it an attractive option for multivariate plots. For instance, the distribution of Type by Type_Token_Ratio and Genre can be visualised by mapping Genre onto the col argument:\n\ngenitive %&gt;% \n  ggplot(aes(x = Type, y = Type_Token_Ratio, col = Genre)) +\n  geom_boxplot() \n\n\n\n\n\n\n\n\nAlternatively, for a slightly less cluttered representation, distinct subplots can be generated with facet_wrap(~Variable):\n\ngenitive %&gt;% \n  ggplot(aes(x = Type, y = Type_Token_Ratio)) +\n  geom_boxplot() +\n  facet_wrap(~Genre)\n\n\n\n\n\n\n\n\nVisualise the percentage of genitive types by Possessum_Animacy2 in each Genre! Provide a short assessment of the results.\n\n\nExercise 6 The grouping function group_by() from the tidyverse library allows performing statistical operations on a per-group basis. These are typically followed by summarise(). For instance, computing the mean type/token ratio for every genre could be achieved using the following syntax:\n\ngenre_ttr &lt;- genitive %&gt;% \n              group_by(Genre) %&gt;% \n              summarise(mean_TTR = mean(Type_Token_Ratio))\n\nprint(genre_ttr)\n\n# A tibble: 5 × 2\n  Genre             mean_TTR\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Adventure Fiction     75.6\n2 General Fiction       75.0\n3 Learned               72.2\n4 Non-fiction           74.4\n5 Press                 76.7\n\n\nExtend the above code to include the standard deviation of the mean type/token ratio for every genre. Based on this updated data frame, generate a suitable barplot with error bars that represent the standard deviation of the mean.",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.3 Descriptive statistics"
    ]
  },
  {
    "objectID": "chapters/04-descriptive_stats.html#tier-3",
    "href": "chapters/04-descriptive_stats.html#tier-3",
    "title": "4.3 Descriptive statistics",
    "section": "Tier 3",
    "text": "Tier 3\n\nExercise 7 The standard error of the mean (\\(\\hat{\\sigma}_{\\hat{\\mu}}\\)) tells us how precisely we know the population mean \\(\\mu\\) based on our sample. It is calculated as \\[\n\\hat{\\sigma}_{\\hat{\\mu}} = \\frac{\\hat{\\sigma}}{\\sqrt{n}},\n\\tag{7}\\]\nwhere \\(\\hat{\\sigma}\\) is the sample standard deviation and \\(n\\) is the sample size.\nCalculate and interpret the standard error for Type_Token_Ratio by genitive Type and Genre.\n\n\nExercise 8 Standard errors can be used to construct confidence intervals (CIs) for a parameter estimate \\(\\hat{\\theta}\\) (e.g., the sample mean). They have the general form:\n\\[\n\\hat{\\theta} \\pm \\text{Margin of Error}.\n\\]\nIf the sample variance is known, the CIs can be estimated from a normal distribution as follows:\n\\[\n\\hat{\\mu} \\pm z_{1-\\alpha/2} \\times \\hat{\\sigma}_{\\hat{\\mu}}.\n\\tag{8}\\]\nAs Baguley (2012: 79) explains, “\\(\\hat{\\mu}\\) is the usual sample estimate of the arithmetic mean, \\(z_{1-\\alpha/2}\\) is the required quantile of the standard normal distribution and \\(\\hat{\\sigma}_{\\hat{\\mu}}\\) is the sample estimate of the standard error of the mean”.\nExample: Given \\(n = 50, \\hat{\\mu} = 94.6, \\hat{\\sigma} = 19.6\\) and a confidence level \\(\\alpha = 0.05\\), the 95% CI would be [89.17, 100.03].\n\n# Mean - Margin of Error * SD / sqrt(N)\n94.6 - qnorm(1-0.05 / 2) * 19.6 / sqrt(50)\n\n[1] 89.16726\n\n# Mean + Margin of Error * SD / sqrt(N)\n94.6 + qnorm(1-0.05 / 2) * 19.6 / sqrt(50)\n\n[1] 100.0327\n\n\nCalculate the mean, standard deviation, standard error of the mean and the 95% confidence intervals of the mean for the Possessor_Length of each genitive Type in the genitive data! How do these measures contribute to our understanding of the genitive alternation?",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.3 Descriptive statistics"
    ]
  },
  {
    "objectID": "chapters/04-descriptive_stats.html#bivariate-statistics",
    "href": "chapters/04-descriptive_stats.html#bivariate-statistics",
    "title": "4.3 Descriptive statistics",
    "section": "Bivariate statistics",
    "text": "Bivariate statistics\n\nCovariance\nCovariance “measures the average tendency of two variables to covary (change together)”(Baguley 2012: 206). Recall the variance estimator from Equation 3, which has the expanded form\n\\[\n\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n{(x_i - \\hat{\\mu})(x_i - \\hat{\\mu})}}{n-1}.\n\\]\n\nvar(genitive$Possessor_Length)\n\n[1] 1.944548\n\n\nThe covariance is obtained by replacing one of the product terms with another variable \\(Y\\), i.e.,\n\\[\n\\hat{\\sigma}_{X,Y} = \\frac{\\sum_{i=1}^n{(x_i - \\hat{\\mu}_X)(y_i - \\hat{\\mu}_Y)}}{n-1}.\n\\tag{5}\\]\nThe covariance of Possessor_Length (John’s cat) and Possessum_Length (John’s cat) is negligible:\n\ncov(genitive$Possessor_Length, genitive$Possessum_Length)\n\n[1] 0.1693295\n\n\n\n\nCorrelation\nCovariance is typically used as an intermediary measure for the calculation of the correlation coefficient \\(r\\) (or \\(\\rho\\), also known as Pearson’s product-moment correlation coefficient), which involves dividing the covariance by the product of the standard deviations of \\(X\\) and \\(Y\\):\n\\[\nr_{X,Y} = \\frac{\\hat{\\sigma}_{X,Y}}{\\hat{\\sigma}_{X}\\hat{\\sigma}_{Y}}\n\\tag{6}\\]\nThis returns a measure in the interval \\([-1, 1]\\), with\n\n\\(0 &lt; r \\leq 1\\) suggesting a positive correlation (increasing \\(X\\)-values \\(\\sim\\) increasing \\(Y\\)-values) and\n\\(-1 \\leq r &lt; 0\\) a negative correlation (increasing \\(X\\)-values \\(\\sim\\) decreasing \\(Y\\)-values). It is best thought of the extent to which two variables form a straight-line (linear) relationship.\n\nFor example, the cor() function shows that the length of the possessor is weakly yet positively correlated with the length of the possessum.\n\ncor(genitive$Possessor_Length, genitive$Possessum_Length)\n\n[1] 0.1021913\n\n\nIts squared version \\(r^2\\) (or \\(R^2\\)) is known as the coefficient of determination and denotes “the proportion of variance in \\(Y\\) accounted for by \\(X\\) (or vice versa)” (Baguley 2012: 209). It turns out that possessor length only explains approximately 1% of the variance in possessum length:\n\ncor(genitive$Possessor_Length, genitive$Possessum_Length)^2\n\n[1] 0.01044307",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.3 Descriptive statistics"
    ]
  },
  {
    "objectID": "chapters/04-probability.html#preparation",
    "href": "chapters/04-probability.html#preparation",
    "title": "4.2 Probability theory",
    "section": "Preparation",
    "text": "Preparation\nThe framenet dataset observations from 193,349 annotated sentences in the FrameNet database.\n\n\n\n\n\n\nMore on FrameNet\n\n\n\n\n\nFrameNet emerged as part of Charles Fillmore’s research on frame semantics (Fillmore, Johnson, and Petruck 2003) and is currently maintained by Ruppenhofer et al. (2016). It is fundamentally based on on the idea that lexical items can be meaningfully grouped based on the semantic roles they evoke.\nFor instance, lexical units such as boil, brown, or fry are said to evoke the APPLY_HEAT frame, in which a COOK applies heat to FOOD using a HEATING_INSTRUMENT or a CONTAINER (cf.exs. a–c). Speakers may moreover include circumstantial elements like MANNER or PLACE.\n\n[They]\\(_\\text{COOK}\\) [boiled] [them]\\(_\\text{FOOD}\\) [in an iron saucepan]\\(_\\text{CONTAINER}\\).\n[You]\\(_\\text{COOK}\\) [can brown] [it]\\(_\\text{FOOD}\\) [in the hot fat]\\(_\\text{MEDIUM}\\).\n[She]\\(_\\text{COOK}\\) [was frying] [eggs and bacon and mushrooms]\\(_\\text{FOOD}\\) [on a camp stove]\\(_\\text{MEDIUM}\\) [in Woolley’s billet]\\(_\\text{PLACE}\\).\n\nElements that are essential for identifying as well as understanding a frame are called core; they can be considered “obligatory” complements of the verb (here: COOK and FOOD) By contrast, non-core are not unique to frames and can always be added or dropped (similar to adjuncts; here: PLACE), respectively.\n\n\n\n\nlibrary(tidyverse)\nlibrary(readxl)\n\nframenet &lt;- read_xlsx(\"FrameNet_full.xlsx\")\n\n\n# Overview\nstr(framenet)\nhead(framenet)\n\n# How many distinct verbs?\nlength(unique(framenet$Verb))\n\n# How many distinct frames?\nlength(unique(framenet$Frame))\n\n# How many distinct frame elements?\nlength(unique(framenet$Frame.Element))",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.2 Probability theory"
    ]
  },
  {
    "objectID": "chapters/04-probability.html#tier-1",
    "href": "chapters/04-probability.html#tier-1",
    "title": "4.2 Probability theory",
    "section": "Tier 1",
    "text": "Tier 1\n\nExercise 1 Make suggestions for probability distributions that could be helpful for modelling the following variables:\n\nAnalytic vs. synthetic comparatives (scarier vs. more scary)\nNumber of pronunciation errors per minute\nFrequencies of modal verbs in American English\nArticle choice (indefinite vs. definite vs. zero-article)\nThe time it takes to read a word aloud (in ms)\n\n\n\nExercise 2 Let Coreness be a binomially distributed variable, which is defined as\n\\[\n\\text{Coreness} = \\begin{cases}\n\\text{core} & \\text{with probability } \\pi \\\\\n\\text{non-core} & \\text{with probability } 1-\\pi\n\\end{cases}\n\\] for \\(0 &lt; \\pi &lt; 1\\).\nWe can model this variable using the binomial probability mass function, which has the form\n\\[\nf(x; n; \\pi) = \\binom{n}{x}\\pi^x(1-\\pi)^{n-x},\n\\] where \\(n\\) is the number of independent trials, \\(\\pi\\) the probability of ‘success’, and \\(x\\) the observed number of successes.\nBaguley (2012: 67) provides the code example below for plotting a discrete \\(pmf\\).\n\n# Vector containing the numbers of successes\nheads &lt;- 0:10 # observing 'heads' 0, ..., 10 times\n\n# Get probability mass for each success (fair coin -&gt; P(Heads) = 0.5)\nprob.mass &lt;- dbinom(heads, 10, 0.5) # throwing the coin 10 times\n\n# Plot the PMF\nplot(heads, prob.mass, pch = NA, xlab = \"Number of heads\", ylab = \"Probability\")\n\n# Create a spikey plot\nsegments(x0 = heads, y0 = 0, x1 = heads, y1 = prob.mass)\n\n\n\n\n\n\n\n\nAdjust the code to plot the \\(pmf\\) for Coreness where \\(\\pi = 0.85\\) and \\(n = 50\\). The binomial experiment is repeated 50 times. Provide a brief visual description of the distribution.\n\n\nExercise 3 Let \\(R\\) be a continuous random variable measuring reaction times in lexical decision tasks. We approximate the measurements with a normal distribution parametrised by\n\\[\nR \\sim \\mathcal{N}(\\mu = 500, \\sigma = 150),\n\\]\nwhich can be visualised as follows:\n\ncurve(dnorm(x, mean = 500, sd = 150), xlim = c(0, 1000))\n\n\n\n\n\n\n\n\nDescribe how the shape auf the bell curve changes for \\(\\sigma = 50\\), \\(\\sigma = 200\\) and \\(\\sigma = 350\\). What do these changes mean for the distribution of reaction times?",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.2 Probability theory"
    ]
  },
  {
    "objectID": "chapters/04-probability.html#exercises",
    "href": "chapters/04-probability.html#exercises",
    "title": "4.2 Probability theory",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.2 Probability theory"
    ]
  },
  {
    "objectID": "chapters/04-probability.html#tier-2",
    "href": "chapters/04-probability.html#tier-2",
    "title": "4.2 Probability theory",
    "section": "Tier 2",
    "text": "Tier 2\n\nExercise 4 If a variable follows a binomial distribution and we know the number of trials \\(n\\) and the probability of success \\(\\pi\\), we can compute:\n\nthe probability of observing a specific number of successes with dbinom():\n\n\n# Example: P(4 heads from 10 ten tosses of a fair coin)\ndbinom(x = 4, size = 10, prob = 0.5)\n\n[1] 0.2050781\n\n\n\nthe cumulative probability of up to \\(x\\) successes with pbinom():\n\n\n# Example: P(up to 4 heads from 10 ten tosses of a fair coin)\npbinom(q = 4, size = 10, prob = 0.5)\n\n[1] 0.3769531\n\n# P(5 or more heads from 10 ten tosses of a fair coin)\n1 - pbinom(q = 4, size = 10, prob = 0.5)\n\n[1] 0.6230469\n\n\n\nrandom simulated successes with rbinom():\n\n\n# Shows the numbers of successes in the first, second, third and fourth trial\nrbinom(n = 4, size = 10, prob = 0.5)\n\n[1] 3 3 3 5\n\n\nInterpret the output of the two code chunks below with reference to the Coreness variable from Exercise 2.\n\n1 - pbinom(q = 80, size = 100, prob = 0.85)\n\n[1] 0.8934557\n\n\n\nrbinom(n = 50, size = 15, prob = 0.15)\n\n [1] 2 3 2 0 0 1 4 1 2 3 6 1 3 1 2 0 7 2 7 1 1 1 3 2 2 3 2 2 2 1 1 3 3 5 5 3 2 1\n[39] 2 3 4 4 3 1 0 2 2 1 5 1\n\n\n\n\nExercise 5 Let \\(D\\) be a Poisson-distributed random variable which counts the total number of fillers such as uhm, er, and like per 100 words. It has the \\(pmf\\)\n\\[\nf(x, \\lambda) = \\frac{\\lambda^{x} e^{-\\lambda}}{x!}.\n\\]\nThe study by Bortfeld et al. (2001) suggests that \\(\\lambda = 2\\) would be a suitable rate parameter. Plot the \\(pmf\\) using dpois() for \\(x \\in \\{0, \\dots, 10\\}\\). (Tip: Check the documentation by typing ?dpois into the console.)\n\n\nExercise 6 Consider the distribution of frame elements for the verb eat (INGESTION):\n\neat_data\n\n# A tibble: 6 × 3\n  Frame.Element     n rel_freq\n  &lt;chr&gt;         &lt;int&gt;    &lt;dbl&gt;\n1 Ingestor         24   0.414 \n2 Ingestibles      23   0.397 \n3 Place             5   0.0862\n4 Manner            3   0.0517\n5 Source            2   0.0345\n6 Time              1   0.0172\n\n\nWhat probability distribution would be suitable for modelling the variable Frame.Element?",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.2 Probability theory"
    ]
  },
  {
    "objectID": "chapters/04-probability.html#tier-3-advanced-exercise---dispersion-and-information-theory",
    "href": "chapters/04-probability.html#tier-3-advanced-exercise---dispersion-and-information-theory",
    "title": "4.2 Probability theory",
    "section": "Tier 3: Advanced Exercise - Dispersion and Information Theory",
    "text": "Tier 3: Advanced Exercise - Dispersion and Information Theory\n\nExercise 7 Measuring Lexical Dispersion with Kullback-Leibler Divergence\nThe empirical study of dispersion has attracted significant attention in recent years Gries (2020). A key challenge is finding dispersion measures that are minimally correlated with token frequency. One such measure is the Kullback-Leibler divergence (KLD), which comes from information theory and is closely related to entropy.\nMathematically, KLD measures the difference between two probability distributions \\(p\\) and \\(q\\):\n\\[ KLD(p \\parallel q) = \\sum\\limits_{x \\in X} p(x) \\log \\frac{p(x)}{q(x)}\n\\tag{14}\\]\nFor corpus dispersion, we compare the posterior (actual) distribution of a word across corpus parts \\(\\frac{v_i}{f}\\) with the prior distribution that assumes uniform spread across parts (weighted by part size \\(s_i\\)):\n\\[ KLD = \\sum\\limits_{i=1}^n \\frac{v_i}{f} \\times \\log_2\\left({\\frac{v_i}{f} \\times \\frac{1}{s_i}}\\right)\n\\tag{15}\\]\nwhere: - \\(f\\) = total frequency of the word in the corpus\n- \\(v_i\\) = frequency of the word in corpus part \\(i\\) - \\(s_i\\) = size of corpus part \\(i\\) (as fraction of total corpus) - \\(n\\) = number of corpus parts\n\nPart A: Implementation and Basic Analysis\nFirst, let’s create simulated corpus data to work with:\n\nlibrary(tidyverse)\n\n# Create simulated corpus data\nset.seed(123)\n\n# Simulate a corpus with 5 parts of different sizes\ncorpus_parts &lt;- c(\"Fiction\", \"News\", \"Academic\", \"Spoken\", \"Web\")\npart_sizes &lt;- c(0.25, 0.20, 0.15, 0.25, 0.15)  # Relative sizes\ntotal_tokens &lt;- 1000000\n\n# Create some example words with different dispersion patterns\nwords_data &lt;- tibble(\n  word = rep(c(\"the\", \"however\", \"DNA\", \"like\", \"therefore\"), each = 5),\n  corpus_part = rep(corpus_parts, 5),\n  frequency = c(\n    # \"the\" - highly frequent, evenly distributed\n    c(12500, 10000, 7500, 12500, 7500),\n    # \"however\" - academic bias\n    c(150, 200, 800, 100, 50),\n    # \"DNA\" - strong academic bias  \n    c(10, 5, 200, 2, 8),\n    # \"like\" - spoken bias\n    c(200, 150, 50, 600, 300),\n    # \"therefore\" - academic/formal bias\n    c(50, 100, 300, 30, 20)\n  ),\n  part_size = rep(part_sizes, 5)\n)\n\nprint(words_data)\n\n# A tibble: 25 × 4\n   word    corpus_part frequency part_size\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n 1 the     Fiction         12500      0.25\n 2 the     News            10000      0.2 \n 3 the     Academic         7500      0.15\n 4 the     Spoken          12500      0.25\n 5 the     Web              7500      0.15\n 6 however Fiction           150      0.25\n 7 however News              200      0.2 \n 8 however Academic          800      0.15\n 9 however Spoken            100      0.25\n10 however Web                50      0.15\n# ℹ 15 more rows\n\n\nCompute the dispersion values of the, however, DNA, like, and therefore in the simulated corpus.\n\n# Get frequency distribution across files\nv_the &lt;- words_data %&gt;% \n  filter(word == \"the\") %&gt;% \n  mutate(rel_freq = frequency/sum(frequency)) %&gt;% \n  pull(rel_freq)\n\nv_the &lt;- words_data %&gt;% \n  filter(word == \"the\") %&gt;% \n  mutate(rel_freq = frequency/sum(frequency)) %&gt;% \n  pull(rel_freq)\n\n# Get total frequencies\nf &lt;- sum(words_data$frequency)\n\n# Get number of distinct text categories\nn_the &lt;- length(unique(words_data$corpus_part))\n\n# Get proportions of distinct text categories (s)\ns_the &lt;- words_data$part_size\n\n# Compute KLD\nsum(v_the/f * log2(v_the/f * 1/s_the))\n\n[1] -0.001472342",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.2 Probability theory"
    ]
  },
  {
    "objectID": "chapters/04-probability.html#tier-3",
    "href": "chapters/04-probability.html#tier-3",
    "title": "4.2 Probability theory",
    "section": "Tier 3",
    "text": "Tier 3\n\nExercise 7 The empirical study of dispersion has attracted significant attention in recent years (Sönning 2024; Gries 2024). A key challenge is finding dispersion measures that are minimally correlated with token frequency. One such measure is the Kullback-Leibler divergence (KLD), which comes from information theory and is closely related to entropy.\nMathematically, KLD measures the difference between two probability distributions \\(p\\) and \\(q\\):\n\\[\nKLD(p \\parallel q) = \\sum\\limits_{x} p(x) \\log \\frac{p(x)}{q(x)}\n\\]\nFor corpus dispersion (cf. Equation Equation 12), we compare the posterior (actual) distribution of a word across corpus parts \\(\\frac{v_i}{f}\\) with the prior distribution that assumes uniform spread across parts (weighted by part size \\(s_i\\)):\n\\[\nKLD = \\sum\\limits_{i=1}^n \\frac{v_i}{f} \\times \\log_2\\left({\\frac{v_i}{f} \\times \\frac{1}{s_i}}\\right)\n\\tag{12}\\]\nwhere:\n\n\\(f\\) = total frequency of the word in the corpus\n\\(v_i\\) = frequency of the word in corpus part \\(i\\)\n\\(s_i\\) = size of corpus part \\(i\\) (as fraction of total corpus)\n\\(n\\) = number of corpus parts\n\nFirst, let’s create simulated corpus data to work with:\n\nlibrary(tidyverse)\n\n# Create simulated corpus data\nset.seed(123)\n\n# Simulate a corpus with 5 parts of different sizes\ncorpus_parts &lt;- c(\"Fiction\", \"News\", \"Academic\", \"Spoken\", \"Web\")\n\npart_sizes &lt;- c(0.25, 0.20, 0.15, 0.25, 0.15)  # Sizes of the corpus parts\n\ntotal_tokens &lt;- 1000000 # Corpus size\n\n# Create some example words with different dispersion patterns\nwords_data &lt;- tibble(\n  word = rep(c(\"the\", \"however\", \"DNA\", \"like\", \"therefore\"), each = 5),\n  corpus_part = rep(corpus_parts, 5),\n  frequency = c(\n    # \"the\" - highly frequent, evenly distributed\n    c(12500, 10000, 7500, 12500, 7500),\n    # \"however\" - academic bias\n    c(150, 200, 800, 100, 50),\n    # \"DNA\" - strong academic bias  \n    c(10, 5, 200, 2, 8),\n    # \"like\" - spoken bias\n    c(200, 150, 50, 600, 300),\n    # \"therefore\" - academic/formal bias\n    c(50, 100, 300, 30, 20)\n  ),\n  part_size = rep(part_sizes, 5)\n)\n\n# Show full dataset\nprint(words_data, n = Inf)\n\n# A tibble: 25 × 4\n   word      corpus_part frequency part_size\n   &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n 1 the       Fiction         12500      0.25\n 2 the       News            10000      0.2 \n 3 the       Academic         7500      0.15\n 4 the       Spoken          12500      0.25\n 5 the       Web              7500      0.15\n 6 however   Fiction           150      0.25\n 7 however   News              200      0.2 \n 8 however   Academic          800      0.15\n 9 however   Spoken            100      0.25\n10 however   Web                50      0.15\n11 DNA       Fiction            10      0.25\n12 DNA       News                5      0.2 \n13 DNA       Academic          200      0.15\n14 DNA       Spoken              2      0.25\n15 DNA       Web                 8      0.15\n16 like      Fiction           200      0.25\n17 like      News              150      0.2 \n18 like      Academic           50      0.15\n19 like      Spoken            600      0.25\n20 like      Web               300      0.15\n21 therefore Fiction            50      0.25\n22 therefore News              100      0.2 \n23 therefore Academic          300      0.15\n24 therefore Spoken             30      0.25\n25 therefore Web                20      0.15\n\n\nCompute the dispersion values of the, however, DNA, like, and therefore based on the simulated corpus data. How does dispersion reflect their (lack of) register bias?",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.2 Probability theory"
    ]
  },
  {
    "objectID": "chapters/04-descriptive_stats.html#suggested-reading",
    "href": "chapters/04-descriptive_stats.html#suggested-reading",
    "title": "4.3 Descriptive statistics",
    "section": "",
    "text": "Theoretical introduction:\n\nBaguley (2012, chap. 1 & 6)\nHeumann et al. (2022: Chapter 3)\n\nApplication in R:\n\nWickham et al. (2023: Chapter 1)\n\nStyle guide:\n\ne.g., APA Numbers and Statistics Guide (7th ed.)",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.3 Descriptive statistics"
    ]
  },
  {
    "objectID": "chapters/06-linear_regression.html#ordinary-least-squares",
    "href": "chapters/06-linear_regression.html#ordinary-least-squares",
    "title": "6.1 Linear regression",
    "section": "Ordinary least squares",
    "text": "Ordinary least squares\nThe most common way of estimating parameters for linear models is the Least Squares approach. In essence, the parameters are chosen such that the residual sum of squares, i.e., the sum of the differences between observed and predicted values, is as low as possible. In other words, we are trying to minimise the distances between the data points and the regression line.\nIt can be computed using the equivalence in Equation 3.\n\\[ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i- \\bar{y})}{\\sum_{i=1}^n(x_i- \\bar{x})^2}.\n\\tag{3}\\]\nWe can then obtain the intercept via Equation 4.\n\\[\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta}_1\\bar{x}\n\\tag{4}\\]\nOnce we’ve fitted the model, we can then predict reaction times if we know the frequency of a lexical stimulus:\n\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x,\n\\tag{5}\\]\nwhere \\(\\hat{y}\\) indicates a prediction of \\(Y\\) on the basis of the predictor values \\(X = x\\).\n\nApplication in R\nIn R, we can fit a linear model with the lm() function.\n\n# Fit linear model\nrt.lm1 = lm(log(RT) ~ log(Freq), data = ELP)\n\n# View model data\nsummary(rt.lm1)\n\n\ntab_model(rt.lm1, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, show.fstat = TRUE, digits = 3)\n\n\n\n\n \nlog(RT)\n\n\nPredictors\nEstimates\nstd. Error\nCI\np\n\n\n(Intercept)\n6.633\n0.004\n6.625 – 6.642\n&lt;0.001\n\n\nFreq [log]\n-0.049\n0.002\n-0.053 – -0.044\n&lt;0.001\n\n\nObservations\n880\n\n\nR2 / R2 adjusted\n0.357 / 0.356\n\n\nDeviance\n13.385\n\n\nAIC\n10534.255\n\n\n\n\n\n\n\nThe model statistics comprise the following elements:\n\n\n\n\n\n\nCall\n\n\n\n\n\ni.e., the model formula.\n\n\n\n\n\n\n\n\n\nResiduals\n\n\n\n\n\nThese indicate the difference between the observed values in the data set and the values predicted by the model (= the fitted values). These correspond to the error term \\(\\epsilon\\). The lower the residuals, the better the model describes the data.\n\n# Show fitted values (= predictions) for the first six observations\nhead(rt.lm1$fitted.values)\n\n       1        2        3        4        5        6 \n6.635345 6.563152 6.789806 6.613980 6.630529 6.574894 \n\n# Show deviation of the fitted values from the observed values\nhead(rt.lm1$residuals)\n\n          1           2           3           4           5           6 \n 0.03778825 -0.02277165  0.07759556  0.03387713  0.15222949 -0.10432670 \n\n\n\n\n\n\n\n\n\n\n\nCoefficients\n\n\n\n\n\nThe regression coefficients correspond to \\(\\hat{\\beta}_0\\) (“Intercept”) and \\(\\hat{\\beta}_1\\) (“log(Freq)”), respectively. The model shows that for a one-unit increase in log-frequency the log-reaction time decreases by approx. 0.05.\n\n# Convert coefficients to a tibble (= tidyverse-style data frame)\ntidy_model &lt;- tidy(rt.lm1)\n\ntidy_model\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   6.63     0.00429    1548.  0       \n2 log(Freq)    -0.0486   0.00220     -22.1 2.88e-86\n\n\n\n\n\n\n\n\n\n\n\n\\(p\\)-values and \\(t\\)-statistic\n\n\n\n\n\n\\(p\\)-values and \\(t\\)-statistic: Given the null hypothesis \\(H_0\\) that there is no correlation between log(RT) and log(Freq) (i.e., \\(H_0: \\beta_1 = 0\\)), a \\(p\\)-value lower than 0.05 indicates that \\(\\beta_1\\) considerably deviates from 0, thus providing evidence for the alternative hypothesis \\(H_1: \\beta_1 \\ne 0\\). Since \\(p &lt; 0.001\\), we can reject \\(H_0\\).\nThe \\(p\\)-value itself crucially depends on the \\(t\\)-statistic2, which measures “the number of standard deviations that \\(\\hat{\\beta_1}\\) is away from 0” (James et al. 2021: 67). The standard error (SE) reflects how much an estimated coefficient differs on average from the true values of \\(\\beta_0\\) and \\(\\beta_1\\). They can be used to compute the 95% confidence interval \\[[\\hat{\\beta}_1 - 2 \\cdot SE(\\hat{β}_1), \\hat{\\beta}_1 + 2 \\cdot SE(\\hat{\\beta}_1)].\n\\tag{6}\\]\nThe true value of the parameter \\(\\beta_1\\) lies within the specified range 95% of the time.\n\n# Compute confidence intervals for intercept and log(Freq)\ntidy_model_ci &lt;- tidy(rt.lm1, conf.int = TRUE)\n\ntidy_model_ci\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   6.63     0.00429    1548.  0          6.62      6.64  \n2 log(Freq)    -0.0486   0.00220     -22.1 2.88e-86  -0.0529   -0.0443\n\n\nThe estimated parameter for log(Freq), which is -0.049, thus has the 95% confidence interval [-0.053, -0.044].\n\n\n\n2 If the response variable is unknown or irrelevant, we speak of unsupervised machine learning. Unsupervised models are mostly concerned with finding patterns in high-dimensional datasets with dozens or even hundreds of variables.\n\n\n\n\n\nResidual standard error (RSE)\n\n\n\n\n\nThis is an estimation of the average deviation of the predictions from the observed values.\n\\[RSE = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n (y_i - \\hat{y_i}})^2\n\\tag{7}\\]\n\n\n\n\n\n\n\n\n\n\\(R^2\\)\n\n\n\n\n\nThe \\(R^2\\) score is important for assessing model fit because it “measures the proportion of variability in \\(Y\\) that can be explained using \\(X\\)” (James et al. 2021: 70), varying between 0 and 1.\n\\[R^2 = 1-\\frac{TSS}{RSS} = 1-\\frac{\\sum_{i=1}^n (y_i - \\hat{y_i})^2}{\\sum_{i=1}^n (y_i - \\bar{y_i})^2}\n\\tag{8}\\]\n\n\n\n\n\n\n\n\n\n\\(F\\)-statistic\n\n\n\n\n\nIt is used to measure the association between the dependent variable and the independent variable(s). Generally speaking, values greater than 1 indicate a possible correlation. A sufficiently low \\(p\\)-value suggests that the null hypothesis \\(H_0: \\beta_1 = 0\\) can be rejected. The \\(F\\) statistic is computed as shown below (cf. Agresti and Kateri 2022: 232) and follows an \\(F\\)-distribution with two different \\(df\\) values.\n\\[\nF = \\frac{(TSS - SSE) / p}{SSE / [n - (p + 1)]}\n\\tag{9}\\]\n\n\n\n\n\nMultiple linear regression\nIn multiple linear regression, more than one predictor variable is taken into account. For instance, modelling log(RT) as a function of log(Freq), POS and Length requires a more complex model of the form\n\\[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon.\n\\tag{10}\\]\nPredictions are then obtained via the formula\n\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2 + ... + \\hat{\\beta}_px_p.\n\\tag{11}\\]\n\n\nApplication in R\nIn R, a multiple regression model is fitted as in the code example below:\n\n# Fit multiple regression model\nrt.lm2 &lt;- lm(log(RT) ~ log(Freq) + POS + Length, data = ELP)\n\n# View model statistics\nsummary(rt.lm2)\n\n\ntab_model(rt.lm2, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, show.fstat = TRUE, digits = 3)\n\n\n\n\n \nlog(RT)\n\n\nPredictors\nEstimates\nstd. Error\nCI\np\n\n\n(Intercept)\n6.460\n0.017\n6.426 – 6.493\n&lt;0.001\n\n\nFreq [log]\n-0.038\n0.002\n-0.042 – -0.034\n&lt;0.001\n\n\nPOS [NN]\n-0.006\n0.010\n-0.026 – 0.014\n0.539\n\n\nPOS [VB]\n-0.035\n0.012\n-0.059 – -0.011\n0.004\n\n\nLength\n0.023\n0.002\n0.020 – 0.026\n&lt;0.001\n\n\nObservations\n880\n\n\nR2 / R2 adjusted\n0.478 / 0.476\n\n\nDeviance\n10.858\n\n\nAIC\n10356.130",
    "crumbs": [
      "6. Statistical Modelling",
      "6.1 Linear regression"
    ]
  },
  {
    "objectID": "chapters/06-linear_regression.html#descriptive-overview",
    "href": "chapters/06-linear_regression.html#descriptive-overview",
    "title": "6.1 Linear regression",
    "section": "Descriptive overview",
    "text": "Descriptive overview\n\nLog-transformedUntransformed\n\n\n\n\nCode\n# Log-transformed\nggplot(ELP, aes(x = log(RT))) +\n  geom_histogram() +\n  geom_vline(xintercept = mean(log(ELP$RT)), color = \"steelblue\") +\n  geom_vline(xintercept = mean(log(ELP$RT)), color = \"red\") +\n  theme_minimal() +\n  labs(\n    title = \"Histogram of Reaction Times (Log-scale)\",\n    x = \"Reaction time (log)\"\n  ) +\n   annotate(\"text\", x = log(mean(ELP$RT)), y = 0, \n           label = \"mean\", \n           vjust = 1.5, hjust = -0.2, color = \"steelblue\", parse = TRUE) +\n  annotate(\"text\", x = log(mean(ELP$RT)) + -0.2, y = .7, \n           label = \"median\", \n           vjust = 1.5, hjust = -0.2, color = \"red\", parse = TRUE)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Skewed\nggplot(ELP, aes(x = RT)) +\n  geom_histogram() +\n  geom_vline(xintercept = mean(ELP$RT), color = \"steelblue\") +\n  geom_vline(xintercept = median(ELP$RT), color = \"red\") +\n  theme_minimal() +\n  labs(\n    title = \"Histogram of Reaction Times\",\n    x = \"Reaction time\"\n  ) +\nannotate(\"text\", x = mean(ELP$RT) + 10, y = 0, \n           label = \"mean\", \n           vjust = 1.5, hjust = -0.2, color = \"steelblue\", parse = TRUE) +\n  annotate(\"text\", x = median(ELP$RT) -175, y = .7, \n           label = \"median\", \n           vjust = 1.5, hjust = -0.2, color = \"red\", parse = TRUE)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome open questions\n\n\n\n\n\n\nCan word frequencies, word length and POS help us explain variation in reaction times?\nIf it can, then how could we characterise the effects of these independent variables? In other words, do they increase or decrease reaction times?\nWhat reaction times could we expect for new observations?\n\n\n\n\n\nA simple statistical model\nThe dependent variable RT is the response or target that we wish to explain. We generically refer to the response as \\(Y\\). Independent variables, such as Length, are called features or predictors, and are typically denoted by \\(X\\).\nWe can thus summarise our preliminary and fairly general statistical model as\n\\[\nY = f(X) + \\epsilon.\n\\tag{1}\\]\nThe term \\(f(X)\\) describes the contribution of an independent variable \\(X\\) to the explanation of \\(Y\\). Since hardly any model can explain the data perfectly, we expect there to be some degree of error \\(\\epsilon\\).",
    "crumbs": [
      "6. Statistical Modelling",
      "6.1 Linear regression"
    ]
  },
  {
    "objectID": "chapters/06-logistic_regression.html#maximum-likelihood-estimation",
    "href": "chapters/06-logistic_regression.html#maximum-likelihood-estimation",
    "title": "6.2 Logistic Regression",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\nIn contrast to continuous data, the estimation of parameters for discrete response variables is less straightforward in that there is no unique solution. Rather than finding a regression line that minimises the distance to all data points, the default approach of logistic models is to find the parameter values that are most likely, given the data. Hence this procedure is also known as Maximum Likelihood Estimation (MLE).\nThe model first makes an assumption about the probability distribution of the data. For categorical data, the binomial distribution is a common choice. Equation 6 indicates the corresponding probability mass function, which describes the probability \\(\\pi\\) of observing \\(y\\) successes in \\(k\\) independent Bernoulli trials. In other words, if we tossed a coin \\(n = 10\\) times and observed \\(y = 5\\) heads (i.e., 5 successes), what is the probability \\(\\pi\\) of a success?\n\\[\nf(y; k; \\pi) = \\binom{k}{y} \\pi^y (1 - \\pi)^{n-y}\n\\tag{6}\\]\nNow, let \\(\\beta\\) denote some parameter of interest (e.g., the slope coefficient of a logistic regression model). Given some observed data, the likelihood of this parameter can be described in terms of the likelihood function \\(L(\\beta)\\) in Equation 7. It assumes \\(n\\) binomial probability mass functions with trials \\(k = 1\\) and computes their product. Since the binomial coefficient \\(\\binom{n}{y}\\) is a constant term, it is typically dropped. In essence, we’re multiplying successes \\(\\pi^{y_i}_i\\) with failures \\((1 - \\pi_i)^{1-y_i}\\) for each data point.\n\\[\nL(\\beta) = \\prod_{i=1}^n \\pi^{y_i}_i (1 - \\pi_i)^{1-y_i}\n\\tag{7}\\]\nWithin the context of our pronoun data, this can be understood more intuitively as the product of the probability that a subject is a null pronoun with the probability that it is not:\n\\[\nL(\\beta) = \\prod_{i=1}^n \\pi^{\\text{null}}_i (1 - \\pi_i)^{1-\\text{null}}\n\\tag{8}\\]\nConventionally, this expression is log-transformed in order to convert the product into a sum because, for one, sums are easier to handle computationally. The log-likelihood function \\(\\ell(\\beta)\\) in Equation 9 forms the basis for a variety of goodness-of-fit measures used to evaluate logistic regression models.\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n y_i \\log(\\pi_i) + (1 - y_i) \\log(1 - \\pi_i)\n\\tag{9}\\]\nAnd in more concrete terms:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\text{null}_i \\log(\\pi_i) + (1 - \\text{null}_i) \\log(1 - \\pi_i)\n\\tag{10}\\]\nIn a next step, all \\(\\pi_i\\)s would be substituted by their respectively probabilities from the right half of Equation 4.\nThe goal is to find the value that maximises \\(\\ell(\\beta)\\), i.e., the maximum likelihood estimator \\(\\hat{\\beta}\\). Approximate solutions can be attained via iterative optimisation techniques (e.g., Newton-Ralphson or Gradient Descent). Sometimes the algorithm may fail to find an optimal solution, which R may report as a model convergence error. For further technical details, see Wood (2006: 63-66) or Agresti & Kateri (2022: 291-294).",
    "crumbs": [
      "6. Statistical Modelling",
      "6.2 Logistic regression"
    ]
  },
  {
    "objectID": "chapters/05-t_test.html#overview",
    "href": "chapters/05-t_test.html#overview",
    "title": "4.7 t-test",
    "section": "Overview",
    "text": "Overview\nSince the \\(\\chi^2\\) measure exclusively works with categorical variables, a separate test statistic is required if one of them is a continuous variable. The \\(t\\)-statistic is often used for research questions involving differences between sample means. The way \\(t\\) is calculated depends on the sources of \\(X\\) and \\(Y\\): Do they originate from the same sample or from two (in-)dependent ones?\nFirst, we consider two independent samples from a population:\n\nSample \\(X\\) with the observations \\(\\{x_1, x_2, ..., {x_n}_1\\}\\), sample size \\(n_1\\), sample mean \\(\\bar{x}\\) and sample variance \\(s^2_x\\).\nSample \\(Y\\) with the observations \\(\\{y_1, y_2, ..., {y_n}_2\\}\\), sample size \\(n_2\\), sample mean \\(\\bar{y}\\) and sample variance \\(s^2_y\\).",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.7 t-test"
    ]
  },
  {
    "objectID": "chapters/05-t_test.html#overview-and-hypotheses",
    "href": "chapters/05-t_test.html#overview-and-hypotheses",
    "title": "4.7 t-test",
    "section": "Overview and hypotheses",
    "text": "Overview and hypotheses\nSince the \\(\\chi^2\\) measure exclusively works with categorical variables, a separate test statistic is required if one of them is a continuous variable. The \\(t\\)-statistic is often used for research questions involving differences between sample means.\nFor instance, we will compare the mean frequencies of the F1 formants of vowels (in Hz) for male and female speakers of Apache. We forward the following hypotheses:\n\n\\(H_0:\\) \\(\\mu_{\\text{men}} = \\mu_{\\text{women}}\\)\n\\(H_1:\\) \\(\\mu_{\\text{men}} \\neq \\mu_{\\text{women}}\\)\n\nThese are so-called two-sided hypotheses: We are only interested in whether or not the means are different, regardless of one mean being higher or lower than the other. By contrast, a one-sided alternative hypothesis predicts an effect in a specific direction (e.g., \\(\\mu_{\\text{men}} &gt; \\mu_{\\text{women}}\\) or \\(\\mu_{\\text{men}} &lt; \\mu_{\\text{women}}\\)).",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.7 t-test"
    ]
  },
  {
    "objectID": "chapters/05-t_test.html#the-t-test-1",
    "href": "chapters/05-t_test.html#the-t-test-1",
    "title": "4.7 t-test",
    "section": "The \\(t\\)-test",
    "text": "The \\(t\\)-test\n\nTwo-sample \\(t\\)-test\nSuppose that the observations in samples \\(X\\) and \\(Y\\) each follow a normal distribution parametrised by \\(\\mathcal{N}(\\mu, \\sigma^2)\\).\nThe rationale of the \\(t\\)-test is then to check how many standard errors \\(\\mu_{x}\\) is away from \\(\\mu_{y}\\). In short:\n\\[\nt = \\frac{\\hat{\\mu}_{x}-\\hat{\\mu}_{y}}{se}\n\\tag{1}\\]\nThe difficulty lies in selecting an appropriate estimate of the standard error. In the literature, the following scenarios are distinguished (Heumann, Schomaker, and Shalabh 2022: 232–233):\n\nThe population variances are known: This is hardly ever the case; instead, we tend to rely on sample estimates (cf. next point).\nThe population variances are unknown, but equal: As long as the variances (and hence the standard deviations) of the two group samples are not too different, we can estimate the standard error via the pooled variances \\(S^2\\):\n\n\\[\nS^2 = \\frac{(n_x - 1) \\hat{\\sigma}^2_x + (n_y - 1)  \\hat{\\sigma}_y^2}{n_x + n_y - 2}\n\\tag{2}\\]\nThen the standard error corresponds to\n\\[\nse = \\sqrt{\\frac{S^2}{n_1} + \\frac{S^2}{n_2}}\n\\tag{3}\\]\n\nThe population variances are unknown and unequal: According to Agresti & Kateri (2022: 171), it is better to employ a different \\(se\\) estimate “[i]f the data show evidence of greatly different standard deviations (with, say, one being at least 50% larger than the other one).” In short, we replace the pooled variances by the unbiased sample variance estimators \\(\\hat{\\sigma}_x^2\\) and \\(\\hat{\\sigma}_y^2\\):\n\n\\[\nse = \\sqrt{\\frac{\\hat{\\sigma}_x^2}{n_1} + \\frac{\\hat{\\sigma}_y^2}{n_2}}\n\\tag{4}\\]\nThis special case is also known as the \\(t\\)-test after Welch\nFurthermore, it assumes that the underlying continuous random variables \\(X\\) and \\(Y\\) each follow a normal distribution \\(\\mathcal{N}(\\mu, \\sigma^2)\\).\n\n\nPaired \\(t\\)-test\nIf there is more than one observation for a given subject (e.g, before and after an experiment), the samples are called paired (or dependent). Here, the variable \\(d_i\\) denotes the difference between two sample observations, i.e., \\(x_i - y_i\\). The corresponding \\(t\\)-statistic is obtained by dividing the mean difference \\(\\bar{d} = \\frac{1}{n}\\sum_{i=1}^n{d_i}\\) by its standard error:\n\\[\nt(x, y) = t(d) = \\frac{\\bar{d}}{\\hat{\\sigma}_d} \\sqrt{n}.\n\\tag{5}\\]\nThe variance of the mean difference is estimated by\n\\[\n\\hat{\\sigma}_d = \\frac{\\sum_{i=1}^n({d_i} - \\bar{d})^2}{n-1}.\n\\tag{6}\\]\nTraditionally, the \\(t\\)-test is based on the assumptions of …\n\nNormality and\nVariance homogeneity (i.e., equal sample variances). Note that this does not apply to the \\(t\\)-test after Welch, which can handle unequal variances.\n\nThe implementation in R is very straightforward:\n\napache_test &lt;- t.test(data_vowels$HZ_F1 ~ data_vowels$SEX, paired = FALSE)\n\nprint(apache_test)\n\n\n    Welch Two Sample t-test\n\ndata:  data_vowels$HZ_F1 by data_vowels$SEX\nt = 2.4416, df = 112.19, p-value = 0.01619\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n  8.403651 80.758016\nsample estimates:\nmean in group F mean in group M \n       528.8548        484.2740 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf at least one assumption of the \\(t\\)-test has been violated, it is advisable to use a non-parametric test such as the Wilcoxon-Mann-Whitney (WMW) U-Test instead. In essence, this test compares the probabilities of encountering a value \\(x\\) from sample \\(X\\) that is greater than a value \\(y\\) from sample \\(Y\\). For details, see ?wilcox.test().\n\nwilcox.test(data_vowels$HZ_F1 ~ data_vowels$SEX)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  data_vowels$HZ_F1 by data_vowels$SEX\nW = 2270, p-value = 0.01373\nalternative hypothesis: true location shift is not equal to 0",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.7 t-test"
    ]
  },
  {
    "objectID": "chapters/05-t_test.html#descriptive-overview",
    "href": "chapters/05-t_test.html#descriptive-overview",
    "title": "4.7 t-test",
    "section": "Descriptive overview",
    "text": "Descriptive overview\nWe select the variables of interest and proceed calculate the mean F1 frequencies for each level of SEX, requiring a grouped data frame.\n\n\nCode\n# Filter data so as to show only those observations that are relevant\ndata_vowels %&gt;% \n  # Filter columns\n  select(HZ_F1, SEX) %&gt;%\n    # Define grouping variable\n    group_by(SEX) %&gt;% \n      # Compute mean and standard deviation for each sex\n      summarise(mean = mean(HZ_F1),\n                sd = sd(HZ_F1)) -&gt; data_vowels_stats\n\nknitr::kable(data_vowels_stats)\n\n\n\n\n\nSEX\nmean\nsd\n\n\n\n\nF\n528.8548\n110.80099\n\n\nM\n484.2740\n87.90112\n\n\n\n\n\n\n\nCode\n# Plot distributions\ndata_vowels_stats %&gt;% \n  ggplot(aes(x = SEX, y = mean)) +\n    geom_col() +\n    geom_errorbar(aes(x = SEX,\n                    ymin = mean-sd,\n                    ymax = mean+sd), width = .2) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\nCode\n# Plot quartiles\ndata_vowels %&gt;% \n  ggplot(aes(x = SEX, y = HZ_F1)) +\n    geom_boxplot() +\n    theme_classic()\n\n\n\n\n\n\n\n\n\n\nTwo-sample \\(t\\)-test\nSuppose that the observations in samples \\(X\\) and \\(Y\\) each follow a normal distribution parametrised by \\(\\mathcal{N}(\\mu, \\sigma^2)\\).\nThe rationale of the \\(t\\)-test is then to check how many standard errors \\(\\mu_{x}\\) is away from \\(\\mu_{y}\\). In short:\n\\[\nt = \\frac{\\hat{\\mu}_{x}-\\hat{\\mu}_{y}}{se}\n\\tag{1}\\]\nThe difficulty lies in selecting an appropriate estimate of the standard error. In the literature, the following scenarios are distinguished (Heumann, Schomaker, and Shalabh 2022: 232–233):\n\nThe population variances are known: This is hardly ever the case; instead, we tend to rely on sample estimates (cf. next point).\nThe population variances are unknown, but equal: As long as the variances (and hence the standard deviations) of the two group samples are not too different, we can estimate the standard error via the pooled variances \\(\\hat{\\sigma}_{\\text{pooled}}^2\\):\n\n\\[\n\\hat{\\sigma}_{\\text{pooled}}^2 = \\frac{(n_x - 1) \\hat{\\sigma}^2_x + (n_y - 1)  \\hat{\\sigma}_y^2}{n_x + n_y - 2}\n\\tag{2}\\]\nThen the standard error corresponds to\n\\[\nse = \\sqrt{\\frac{\\hat{\\sigma}_{\\text{pooled}}^2}{n_x} + \\frac{\\hat{\\sigma}_{\\text{pooled}}^2}{n_y}}\n\\tag{3}\\]\n\n# Two-sample test with equal variances\napache_test1 &lt;- t.test(data_vowels$HZ_F1 ~ data_vowels$SEX, \n                      var.equal = TRUE,\n                      paired = FALSE)\n\nprint(apache_test1)\n\n\n    Two Sample t-test\n\ndata:  data_vowels$HZ_F1 by data_vowels$SEX\nt = 2.4416, df = 118, p-value = 0.01611\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n  8.423043 80.738624\nsample estimates:\nmean in group F mean in group M \n       528.8548        484.2740 \n\n\n\nThe population variances are unknown and unequal: According to Agresti & Kateri (2022: 171), it is better to employ a different \\(se\\) estimate “[i]f the data show evidence of greatly different standard deviations (with, say, one being at least 50% larger than the other one).” In short, we replace the pooled variances by the unbiased sample variance estimators \\(\\hat{\\sigma}_x^2\\) and \\(\\hat{\\sigma}_y^2\\) (cf. Equation 4). This special case is also known as the \\(t\\)-test after Welch.\n\n\\[\nse = \\sqrt{\\frac{\\hat{\\sigma}_x^2}{n_1} + \\frac{\\hat{\\sigma}_y^2}{n_2}}\n\\tag{4}\\]\n\n# Welch Two Sample t-test\napache_test2 &lt;- t.test(data_vowels$HZ_F1 ~ data_vowels$SEX, \n                       var.eqal = FALSE,\n                       paired = FALSE)\n\nprint(apache_test2)\n\n\n    Welch Two Sample t-test\n\ndata:  data_vowels$HZ_F1 by data_vowels$SEX\nt = 2.4416, df = 112.19, p-value = 0.01619\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n  8.403651 80.758016\nsample estimates:\nmean in group F mean in group M \n       528.8548        484.2740 \n\n\n\n\nPaired \\(t\\)-test\nIf there is more than one observation for a given subject (e.g, before and after an experiment), the samples are called paired (or dependent). Here, the variable \\(d_i\\) denotes the difference between two data points \\(x_i - y_i\\) in the \\(i\\)th observation. The corresponding \\(t\\)-statistic is obtained by relating the mean difference \\(\\bar{d} = \\frac{1}{n}\\sum_{i=1}^n{d_i}\\) to its standard error:\n\\[\nt(x, y) = t(d) = \\frac{\\bar{d}}{\\hat{\\sigma}_d} \\sqrt{n}.\n\\tag{5}\\]\nThe variance of the mean difference is estimated by\n\\[\n\\hat{\\sigma}_d = \\frac{\\sum_{i=1}^n({d_i} - \\bar{d})^2}{n-1}.\n\\tag{6}\\]\n\n\nAssumptions\nTraditionally, the \\(t\\)-test is based on the assumptions of normality and variance homogeneity (i.e., equal sample variances). As long as the sample sizes are more or less comparable, the two-sided \\(t\\)-test remains robust even if there is some evidence of non-normality. The \\(t\\)-test after Welch can additionally handle unequal variances.\nIf the violations are substantial, it is advisable to use a non-parametric test such as the Wilcoxon-Mann-Whitney (WMW) U-Test instead. In essence, this test compares the probabilities of encountering a value \\(x\\) from sample \\(X\\) that is greater than a value \\(y\\) from sample \\(Y\\).\n\n\n\n\n\n\nIs it possible to test for violations?\n\n\n\n\n\nYes, there are formal tests that allow us to test for (non-)normality and (un-)equal variances. However, a visual inspection of some diagnostic plots (e.g., histograms or qq-plots) is usually more than enough!\n\n# Shapiro-Wilk Test of Normality\nshapiro.test(data_vowels$HZ_F1) # H0: data points follow the normal distribution. \n\n\n    Shapiro-Wilk normality test\n\ndata:  data_vowels$HZ_F1\nW = 0.98996, p-value = 0.5311\n\n# Check histogram\nggplot(data_vowels, aes(x = HZ_F1)) +\n  geom_histogram(bins = 30) +\n  theme_classic()\n\n\n\n\n\n\n\n# Check quantile-quantile plot\nqqnorm(data_vowels$HZ_F1, main = \"Q-Q Plot for F1 Frequencies\")\nqqline(data_vowels$HZ_F1, col = \"red\")\n\n\n\n\n\n\n\n# F test to compare two variances\nvar.test(data_vowels$HZ_F1 ~ data_vowels$SEX) # H0: variances are not too different from each other\n\n\n    F test to compare two variances\n\ndata:  data_vowels$HZ_F1 by data_vowels$SEX\nF = 1.5889, num df = 59, denom df = 59, p-value = 0.07789\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.949093 2.660040\nsample estimates:\nratio of variances \n          1.588907 \n\n\n\n\n\n\n\nEffect size\nCohen’s d is a possible effect size measure for continuous data and is obtained by dividing the difference of both sample means by the pooled standard deviation.\n\ncohen.d(data_vowels$HZ_F1, data_vowels$SEX) # see also ?cohen.d for more details\n\n\nCohen's d\n\nd estimate: 0.4457697 (small)\n95 percent confidence interval:\n     lower      upper \n0.07976048 0.81177897 \n\n\nThe documentation of the test (type ?cohen.d into the console for details) lists the following interpretation guidelines:\n\n\\(|d| &lt; 0.2\\): negligible effect\n\\(|d| &lt; 0.5\\): small effect\n\\(|d| &lt; 0.8\\): moderate effect\n\\(|d| \\geq 0.8\\): large effect\n\n\n\nReporting the results\nAccording to a two-sample \\(t\\)-test, there is a significant difference between the mean F1 frequencies of male and female speakers of Apache (\\(t = 2.44\\), \\(df = 112.19\\), \\(p = 0.02\\)), yet the effect is rather small (Cohen’s \\(d\\) = 0.45). This provides sufficient evidence to reject the null hypothesis at \\(\\alpha = 0.05\\).",
    "crumbs": [
      "4. Introduction to Statistics",
      "4.7 t-test"
    ]
  }
]