[
  {
    "objectID": "chapters/05-t_test.html",
    "href": "chapters/05-t_test.html",
    "title": "5.3 t-test",
    "section": "",
    "text": "Load packages and data:\n\n\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\n\ndata_vowels &lt;- read.csv(\"../datasets/Vowels_Apache.csv\", sep = \"\\t\")",
    "crumbs": [
      "Overview",
      "5. Inferential Statistics",
      "5.3 t-test"
    ]
  },
  {
    "objectID": "chapters/05-t_test.html#preparation",
    "href": "chapters/05-t_test.html#preparation",
    "title": "5.3 t-test",
    "section": "",
    "text": "Load packages and data:\n\n\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\n\ndata_vowels &lt;- read.csv(\"../datasets/Vowels_Apache.csv\", sep = \"\\t\")",
    "crumbs": [
      "Overview",
      "5. Inferential Statistics",
      "5.3 t-test"
    ]
  },
  {
    "objectID": "chapters/05-t_test.html#the-t-test",
    "href": "chapters/05-t_test.html#the-t-test",
    "title": "5.3 t-test",
    "section": "2 The \\(t\\)-test",
    "text": "2 The \\(t\\)-test\nSince the \\(\\chi^2\\) measure exclusively works with categorical variables, a separate test statistic is required if one of them is a continuous variable. The \\(t\\) statistic is often used for research questions involving differences between sample means. The way \\(t\\) is calculated depends on the sources of \\(X\\) and \\(Y\\): Do they originate from the same sample or from two (in-)dependent ones?\nFirst, we consider two independent samples from a population:\n\nSample \\(X\\) with the observations \\(\\{x_1, x_2, ..., {x_n}_1\\}\\), sample size \\(n_1\\), sample mean \\(\\bar{x}\\) and sample variance \\(s^2_x\\).\nSample \\(Y\\) with the observations \\(\\{y_1, y_2, ..., {y_n}_2\\}\\), sample size \\(n_2\\), sample mean \\(\\bar{y}\\) and sample variance \\(s^2_y\\).\n\n\n\n\n\n\n\nDefinition of the \\(t\\)-test\n\n\n\n\n\nThe \\(t\\)-statistic after Welch is given by:\n\\[\nt(x, y) = \\frac{|\\bar{x} - \\bar{y}|}{\\sqrt{\\frac{s^2_x}{n_1} + \\frac{s^2_y}{n_2}}}\n\\tag{1}\\]\n\nIf there is more than one observation for a given subject (e.g, before and after an experiment), the samples are called dependent or paired. The paired \\(t\\)-test assumes two continuous variables \\(X\\) and \\(Y\\).\nIn the paired test, the variable \\(d\\) denotes the difference between them, i.e., \\(x - y\\). The corresponding test statistic is obtained via\n\n\\[\nt(x, y) = t(d) = \\frac{\\bar{d}}{s_d} \\sqrt{n}.\n\\tag{2}\\]\nNote the difference \\(\\bar{d} = \\frac{1}{n}\\sum_{i=1}^n{d_i}\\) and the variance\n\\[\ns^2_d = \\frac{\\sum_{i=1}^n({d_i} - \\bar{d})^2}{n-1}.\n\\tag{3}\\]\nTraditionally, the \\(t\\)-test is based on the assumptions of …\n\nNormality and\nVariance homogeneity (i.e., equal sample variances). Note that this does not apply to the \\(t\\)-test after Welch, which can handle unequal variances.\n\n\n\n\nThe implementation in R is very straightforward:\n\nt.test(data_vowels$HZ_F1 ~ data_vowels$SEX, paired = FALSE) # there is a significant difference!\n\n\n\n\n\n\n\nImportant\n\n\n\nIf at least one assumption of the \\(t\\)-test has been violated, it is advisable to use a non-parametric test such as the Wilcoxon-Mann-Whitney (WMW) U-Test instead. In essence, this test compares the probabilities of encountering a value \\(x\\) from sample \\(X\\) that is greater than a value \\(y\\) from sample \\(Y\\). For details, see ?wilcox.test().\n\nwilcox.test(data_vowels$HZ_F1 ~ data_vowels$SEX)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  data_vowels$HZ_F1 by data_vowels$SEX\nW = 2270, p-value = 0.01373\nalternative hypothesis: true location shift is not equal to 0",
    "crumbs": [
      "Overview",
      "5. Inferential Statistics",
      "5.3 t-test"
    ]
  },
  {
    "objectID": "chapters/05-t_test.html#workflow-in-r",
    "href": "chapters/05-t_test.html#workflow-in-r",
    "title": "5.3 t-test",
    "section": "3 Workflow in R",
    "text": "3 Workflow in R\n\n3.1 Define hypotheses\n\n\\(H_0:\\) mean F1 frequency of men \\(=\\) mean F1 frequency of women.\n\\(H_1:\\) mean F1 frequency of men \\(\\ne\\) mean F1 frequency of women.\n\n\n\n3.2 Descriptive overview\nWe select the variables of interest and proceed calculate the mean F1 frequencies for each level of SEX, requiring a grouped data frame.\n\n\nCode\n# Filter data so as to show only those observations that are relevant\ndata_vowels %&gt;% \n  # Filter columns\n  select(HZ_F1, SEX) %&gt;%\n    # Define grouping variable\n    group_by(SEX) %&gt;% \n      # Compute mean and standard deviation for each sex\n      summarise(mean = mean(HZ_F1),\n                sd = sd(HZ_F1)) -&gt; data_vowels_stats\n\nknitr::kable(data_vowels_stats)\n\n\n\n\n\nSEX\nmean\nsd\n\n\n\n\nF\n528.8548\n110.80099\n\n\nM\n484.2740\n87.90112\n\n\n\n\n\n\n\nCode\n# Plot distributions\ndata_vowels_stats %&gt;% \n  ggplot(aes(x = SEX, y = mean)) +\n    geom_col() +\n    geom_errorbar(aes(x = SEX,\n                    ymin = mean-sd,\n                    ymax = mean+sd), width = .2) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\nCode\n# Plot quartiles\ndata_vowels %&gt;% \n  ggplot(aes(x = SEX, y = HZ_F1)) +\n    geom_boxplot() +\n    theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n3.3 Check \\(t\\)-test assumptions\n\n# Normality\nshapiro.test(data_vowels$HZ_F1) # H0: data points follow the normal distribution; however, this test is pretty unreliable!\n\n\n    Shapiro-Wilk normality test\n\ndata:  data_vowels$HZ_F1\nW = 0.98996, p-value = 0.5311\n\n# Check histogram\nggplot(data_vowels, aes(x = HZ_F1)) +\n  geom_histogram(bins = 30) +\n  theme_classic()\n\n\n\n\n\n\n\n# Variance homogeneity\nvar.test(data_vowels$HZ_F1 ~ data_vowels$SEX) # H0: variances are not too different from each other\n\n\n    F test to compare two variances\n\ndata:  data_vowels$HZ_F1 by data_vowels$SEX\nF = 1.5889, num df = 59, denom df = 59, p-value = 0.07789\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.949093 2.660040\nsample estimates:\nratio of variances \n          1.588907 \n\n\n\n\n3.4 Running the test\n\n# t-test for two independent samples \nt.test(data_vowels$HZ_F1 ~ data_vowels$SEX, paired = FALSE) # there is a significant difference between sample means!\n\n\n\n3.5 Effect size\nCohen’s d is a possible effect size measure for continuous data and is obtained by dividing the difference of both sample means by the pooled standard deviation:\n\\[\\frac{\\bar{x} - \\bar{y}}{\\sqrt{\\frac{{(n_1 - 1)s_x^2 + (n_2 - 1)s_y^2}}{{n_1 + n_2 - 2}}}}.\\]\n\ncohen.d(data_vowels$HZ_F1, data_vowels$SEX) # see also ?cohen.d for more details\n\n\nCohen's d\n\nd estimate: 0.4457697 (small)\n95 percent confidence interval:\n     lower      upper \n0.07976048 0.81177897 \n\n\n\n\n3.6 Reporting the results\nAccording to a two-sample \\(t\\)-test, there is a significant difference between the mean F1 frequencies of male and female speakers of Apache (\\(t = 2.44\\), \\(df = 112.19\\), \\(p &lt; 0.05\\)). Therefore, \\(H_0\\) will be rejected.",
    "crumbs": [
      "Overview",
      "5. Inferential Statistics",
      "5.3 t-test"
    ]
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Overview",
    "section": "",
    "text": "This collection of handouts provides a hands-on introduction to data analysis and statistical methods in quantitative corpus linguistics with R. It is designed with accessibility in mind, assuming no prior knowledge of programming or statistics. All you need to get started is a laptop; everything else will be explained within these pages.\nPrimarily, this reader is geared towards students attending the classes Language Variation (BA) and Statistics for Linguistics (MA) at the Catholic University of Eichstätt-Ingolstadt (Germany). However, it is also meant to equip students currently working on their BA/MA/PhD theses with the tools they need to conduct empirical studies on a wide array of linguistic phenomena. The methods presented here reflect the state-of-the-art in corpus-linguistic research, providing readers with current and relevant analytical techniques.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "blog.html#fundamentals-of-corpus-based-research",
    "href": "blog.html#fundamentals-of-corpus-based-research",
    "title": "Overview",
    "section": "1 Fundamentals of Corpus-based Research",
    "text": "1 Fundamentals of Corpus-based Research\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\n1.1 Basics\n\n\nA short introduction to the basic structure of a sociolinguistic study.\n\n\n\n\n1.2 Research questions\n\n\n\n\n\n\n\n1.3 Linguistic variables\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "blog.html#introduction-to-r",
    "href": "blog.html#introduction-to-r",
    "title": "Overview",
    "section": "2 Introduction to R",
    "text": "2 Introduction to R",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "blog.html#corpus-linguistics-with-r",
    "href": "blog.html#corpus-linguistics-with-r",
    "title": "Overview",
    "section": "3 Corpus Linguistics with R",
    "text": "3 Corpus Linguistics with R",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "blog.html#descriptive-statistics",
    "href": "blog.html#descriptive-statistics",
    "title": "Overview",
    "section": "4 Descriptive Statistics",
    "text": "4 Descriptive Statistics\n\n4.1 Chi-squared test",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "blog.html#inferential-statistics",
    "href": "blog.html#inferential-statistics",
    "title": "Overview",
    "section": "5 Inferential Statistics",
    "text": "5 Inferential Statistics",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "blog.html#statistical-modelling-and-machine-learning",
    "href": "blog.html#statistical-modelling-and-machine-learning",
    "title": "Overview",
    "section": "6 Statistical Modelling and Machine Learning",
    "text": "6 Statistical Modelling and Machine Learning",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "chapters/02-first_steps.html",
    "href": "chapters/02-first_steps.html",
    "title": "First steps",
    "section": "",
    "text": "When it comes to data analysis, learning R offers an overwhelming number of short- and long-term advantages over conventional spreadsheet software such as Microsoft Excel or LibreOffice Calc:\n\nFirst of all, it’s completely free. There’s no need to obtain any expensive licenses, as it is the case for commercial software such as SPSS or MS Excel.\nR makes it very easy to document and share every step of the analysis, thereby facilitating reproducible workflows.\nLarge (and by that I mean extremely large!) datasets pose no problems whatsoever. Loading tabular data with hundreds of thousands (or even millions) of rows only takes a few seconds, whereas most other software would crash.\nThere are numerous extensions that provide tailored functions for corpus linguistics that aren’t available in general-purpose spreadsheet software. This allows us to work with corpora, use complex search expression, perform part-of-speech annotation, dependency parsing, and much more – all from within R.\nR’s ggplot2 offers an incredibly powerful framework for data visualisation. Don’t believe it? Check out the ggplot2 gallery.\nThe CRAN repository features more than 20,000 packages that can be installed to expand the functionality of R almost indefinitely. Should none of them meet your needs, R gives you the tools to comfortably write and share your own functions and packages."
  },
  {
    "objectID": "chapters/02-first_steps.html#why-learn-r-to-begin-with",
    "href": "chapters/02-first_steps.html#why-learn-r-to-begin-with",
    "title": "First steps",
    "section": "",
    "text": "When it comes to data analysis, learning R offers an overwhelming number of short- and long-term advantages over conventional spreadsheet software such as Microsoft Excel or LibreOffice Calc:\n\nFirst of all, it’s completely free. There’s no need to obtain any expensive licenses, as it is the case for commercial software such as SPSS or MS Excel.\nR makes it very easy to document and share every step of the analysis, thereby facilitating reproducible workflows.\nLarge (and by that I mean extremely large!) datasets pose no problems whatsoever. Loading tabular data with hundreds of thousands (or even millions) of rows only takes a few seconds, whereas most other software would crash.\nThere are numerous extensions that provide tailored functions for corpus linguistics that aren’t available in general-purpose spreadsheet software. This allows us to work with corpora, use complex search expression, perform part-of-speech annotation, dependency parsing, and much more – all from within R.\nR’s ggplot2 offers an incredibly powerful framework for data visualisation. Don’t believe it? Check out the ggplot2 gallery.\nThe CRAN repository features more than 20,000 packages that can be installed to expand the functionality of R almost indefinitely. Should none of them meet your needs, R gives you the tools to comfortably write and share your own functions and packages."
  },
  {
    "objectID": "chapters/02-first_steps.html#installing-r",
    "href": "chapters/02-first_steps.html#installing-r",
    "title": "First steps",
    "section": "2 Installing R",
    "text": "2 Installing R\nThe first step involves downloading the R programming language itself. The link will take you to the homepage of the Comprehensive R Archive Network (CRAN) where you can download the binary distribution. Choose the one that corresponds to your operating system (Windows/MAC/Linux).\n\n\n\n\n\n\nInstallation instructions for Windows users\n\n\n\n\n\nClick “Download R for Windows” \\(\\rightarrow\\) Select “base” \\(\\rightarrow\\) Click on “Download R-4.4.1 for Windows” (or whatever most recent version is currently displayed).\nOpen the set-up file you’ve just downloaded and simply follow the instructions on screen. It’s fine to go with the default options.\nVideo tutorial on YouTube\n\n\n\n\n\n\n\n\n\nInstallation instructions for MacOS users\n\n\n\n\n\nClick “Download R for macOS” \\(\\rightarrow\\) Select the latest release for your OS\nOpen the downloaded .pkg file and follow the instructions in the installation window.\nVideo tutorial on YouTube"
  },
  {
    "objectID": "chapters/02-first_steps.html#installing-rstudio",
    "href": "chapters/02-first_steps.html#installing-rstudio",
    "title": "First steps",
    "section": "3 Installing RStudio",
    "text": "3 Installing RStudio\nYou can now download and install RStudio. RStudio is a so-called “Integrated Development Environment” (IDE), which will provide us with a variety of helpful tools to write and edit code comfortably. If R was a musical instrument, then RStudio would be the recording studio, so-to-speak."
  },
  {
    "objectID": "chapters/05-hypothesis_testing.html",
    "href": "chapters/05-hypothesis_testing.html",
    "title": "5.1 Hypothesis testing",
    "section": "",
    "text": "For linguists:\n\nGries (2021): Chapter 1.3.2\n\nGeneral:\n\nBaguley (2012): Chapter 4\nAgresti and Kateri (2022): Chapter 5\nDienes (2008)",
    "crumbs": [
      "Overview",
      "5. Inferential Statistics",
      "5.1 Hypothesis testing"
    ]
  },
  {
    "objectID": "chapters/05-hypothesis_testing.html#suggested-reading",
    "href": "chapters/05-hypothesis_testing.html#suggested-reading",
    "title": "5.1 Hypothesis testing",
    "section": "",
    "text": "For linguists:\n\nGries (2021): Chapter 1.3.2\n\nGeneral:\n\nBaguley (2012): Chapter 4\nAgresti and Kateri (2022): Chapter 5\nDienes (2008)",
    "crumbs": [
      "Overview",
      "5. Inferential Statistics",
      "5.1 Hypothesis testing"
    ]
  },
  {
    "objectID": "chapters/05-hypothesis_testing.html#sec-hyp",
    "href": "chapters/05-hypothesis_testing.html#sec-hyp",
    "title": "5.1 Hypothesis testing",
    "section": "2 On scientific inference",
    "text": "2 On scientific inference\nScience begins and ends with theory, and statistics acts as the “go-between”. Regardless of the discipline, solid research is characterised by a robust theoretical foundation that gives rise to substantive hypotheses, i.e., theory-driven predictions about a population of interest. From this rather concrete hypothesis, it should be possible to derive a statistical hypothesis that re-states the prediction in more formal/mathematical terms. After checking it against real-world data, researchers can either confirm or reject their hypothesis, after which they may decide to amend (or even abandon) their theory – or keep it as is.",
    "crumbs": [
      "Overview",
      "5. Inferential Statistics",
      "5.1 Hypothesis testing"
    ]
  },
  {
    "objectID": "chapters/05-hypothesis_testing.html#null-hypothesis-significance-testing-nhst",
    "href": "chapters/05-hypothesis_testing.html#null-hypothesis-significance-testing-nhst",
    "title": "5.1 Hypothesis testing",
    "section": "3 Null hypothesis significance testing (NHST)",
    "text": "3 Null hypothesis significance testing (NHST)\nThe NHST framework offers researchers a convenient way of testing their theoretical assumptions. This chiefly involves setting up a set of (ideally) falsifiable statistical hypotheses, gathering evidence from the observed data and computing the (in)famous ‘\\(p\\)-value’ to determine “statistical significance” – a notion that is frequently misinterpreted in scientific studies.\n\n\n\n\n\n\nIs this the only way of testing hypotheses?\n\n\n\nThe answer is a resounding no. Despite its immense popularity, NHST is problematic in many respects and hence subject to heavy criticism (cf. Dienes (2008): 76; Baguley (2012): 143-144). There are other statistical schools that can remedy many of its shortcomings and come with distinct advantages, such as those relying on likelihood-based inference and Bayesian principles. Although these are also becoming increasingly common in linguistics, they are still restricted to very few sub-disciplines and journals (mostly in the area of psycholinguistics).\n\n\n\n3.1 \\(H_0\\) vs. \\(H_1\\)\nStatistical hypotheses always come in pairs: A null hypothesis is accompanied by an alternative hypothesis. They are set up before (!) seeing the data and justified by previous research.\n\nThe null hypothesis \\(H_0\\) describes the “default state of the world” (James et al. 2021: 555). It claims there is no noteworthy effect to be observed in the data.\nThe alternative hypothesis \\(H_1\\) (or \\(H_a\\)) plainly states that the \\(H_0\\) is false, suggesting that there is an effect of some kind.\n\n\n\n\n\n\n\nExample: Hypotheses for categorical data\n\n\n\n\n\nWe are interested in finding out whether English clause ORDER (‘sc-mc’ or ‘mc-sc’) depends on the type of the subordinate clause (SUBORDTYPE), which can be either temporal (‘temp’) or causal (‘caus’).\nOur hypotheses are:\n\n\\(H_0:\\) The variables ORDER and SUBORDTYPE are independent.\n\\(H_1:\\) The variables ORDER and SUBORDTYPE are not independent.\n\n\n\n\n\n\n\n\n\n\nExample: Hypotheses for continuous data\n\n\n\n\n\nAs part of a phonetic study, we compare the base frequencies of the F1 formants of vowels (in Hz) for male and female speakers of Apache. We forward the following hypotheses:\n\n\\(H_0:\\) mean F1 frequency of men \\(=\\) mean F1 frequency of women.\n\\(H_1:\\) mean F1 frequency of men \\(\\ne\\) mean F1 frequency of women.\n\n\n\n\n\n\n\n\n\n\nIn formal terms\n\n\n\n\n\nTo be precise, we use the hypotheses to make statements about a population parameter \\(\\theta\\), which can be a mean \\(\\mu\\) for continuous data or a proportion \\(\\pi\\) for categorical data, among other things. Mathematically, the null and alternative hypotheses can be restated as in Equation 1.\n\\[\n\\begin{align}\nH_0: \\theta = 0  \\\\\nH_1: \\theta \\neq 0\n\\end{align}\n\\tag{1}\\]\n\n\n\nIn the NHST world, we’re dealing with a “This town ain’t big enough for the both of us” situation: While we have to state both \\(H_0\\) and \\(H_1\\), only one of them can remain at the end of the day. But how do we decide between these two?\n\n\n3.2 Test statistics\nTo facilitate the decision-making process, we proceed to gather statistical evidence from the observed data. Since NHST primarily revolves around \\(H_0\\) (and not \\(H_1\\)!), we need to review the evidence the data provides against or in favour \\(H_0\\). This is done via a test statistic \\(T\\) that characterises the sample at hand. Essentially, you can think of \\(T\\) as one-value summary of your data.\nThere are many possible test statistics out there:\n\nFor instance, if the data are discrete, the \\(\\chi^2\\) measure is used to compute differences between observed and expected frequencies in the entire sample.\nIn the case of continuos data, it is common to rely on \\(t\\) for quantifying differences between sample means.\nOther possible test statistics include the correlation coefficient \\(r\\), \\(z\\)-scores, the \\(F\\)-statistic, and many others.\n\n\n\n3.3 Statistical significance\nThe final rejection of \\(H_0\\) is determined by the significance probability \\(p\\). Due to the frequency and ferocity with which statistical significance is misinterpreted in the research literature, we will begin by reviewing its technical definition:\n\n“The \\(p\\)-value is the probabilty, presuming that \\(H_0\\) is true, that the test statistic equals the observed value or a value even more extreme in the direction predicted by \\(H_a\\)” (Agresti and Kateri 2022: 163).\n\nIn compact notation, it is equivalent to the conditional probability\n\\[\nP(T \\geq \\text{observed value} \\mid H_0 \\text{ is true}).\n\\] If \\(p\\) is lower than a pre-defined threshold (typically \\(0.05\\)), also known as the significance level \\(\\alpha\\), we can reject \\(H_0\\). However, if \\(p \\geq\\) 0.05, this neither justifies rejecting nor accepting the null hypothesis (Baguley 2012: 121).\nFor example, a \\(p\\)-value of \\(0.02\\) means that we would see a test statistic \\(T\\) only 2% of the time if \\(H_0\\) were true. Since \\(0.02\\) lies below our significance level \\(\\alpha\\) = \\(0.05\\), this would suggest a statistically significant relationship in the data, and we could therefore reject \\(H_0\\).\n\n\n3.4 What could go wrong? Type I and Type II errors\nThere is always a chance that we accept or reject the wrong hypothesis; the four possible constellations are summarised in the table below (cf. Heumann, Schomaker, and Shalabh 2022: 223):\n\n\n\n\n\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is not true\n\n\n\n\n\\(H_0\\) is not rejected\n\\(\\color{green}{\\text{Correct decision}}\\)\n\\(\\color{red}{\\text{Type II } (\\beta)\\text{-error}}\\)\n\n\n\\(H_0\\) is rejected\n\\(\\color{red}{\\text{Type I } (\\alpha)\\text{-error}}\\)\n\\(\\color{green}{\\text{Correct decision}}\\)\n\n\n\n\n\n3.5 The mathematics of the \\(p\\)-value\nLet’s say that the statistical analysis of clause ORDER and SUBORDTYPE has returned a test statistic of \\(\\chi^2 = 6.5\\) for 2 \\(df\\). In order to compute the corresponding \\(p\\)-value we need to consult the sampling distribution of this test statistic.\nA sampling distribution is a probability distribution that assigns probabilities to the values of a test statistic. Because most (if not all) of them are continuous, they have characteristic probability density functions (PDFs). Some of them are illustrated below:\n\n\n\n\n\n\n\\(\\chi^2\\) distribution\n\n\n\n\n\n\n\nShow the code\n# Load ggplot2\nlibrary(ggplot2)\n\n# Define the degrees of freedom\ndf &lt;- 2\n\n# Create a sequence of x values\nx &lt;- seq(0, 30, length.out = 1000)\n\n# Compute the chi-squared density\ny &lt;- dchisq(x, df = df)\n\n# Create a data frame\nchi_squared_data &lt;- data.frame(x = x, y = y)\n\n# Generate the plot\nggplot(chi_squared_data, aes(x = x, y = y)) +\n  geom_line(color = \"steelblue\", size = 1) + # Line for the density curve\n  labs(\n    title = \"Chi-Squared Distribution\",\n    subtitle = \"Probability density function with 2 degrees of freedom\",\n    x = \"Chi-squared value\",\n    y = \"Probability density\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(ylim = c(0, 0.05), xlim = c(0, 30)) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.title = element_text(size = 12)\n  )\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(t\\) distribution\n\n\n\n\n\n\n\nShow the code\n# Define the degrees of freedom\ndf_t &lt;- 10\n\n# Create a sequence of x values\nx_t &lt;- seq(-4, 4, length.out = 1000)\n\n# Compute the t-distribution density\ny_t &lt;- dt(x_t, df = df_t)\n\n# Create a data frame\nt_distribution_data &lt;- data.frame(x = x_t, y = y_t)\n\n# Generate the plot\nggplot(t_distribution_data, aes(x = x, y = y)) +\n  geom_line(color = \"steelblue\", size = 1) + # Line for the density curve\n  labs(\n    title = \"t-Distribution\",\n    subtitle = \"Probability density function with 10 degrees of freedom\",\n    x = \"t value\",\n    y = \"Probability density\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(ylim = c(0, 0.4), xlim = c(-4, 4)) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.title = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(F\\) distribution\n\n\n\n\n\n\n\nShow the code\n# Define the degrees of freedom\ndf1 &lt;- 5\ndf2 &lt;- 10\n\n# Create a sequence of x values\nx_f &lt;- seq(0, 5, length.out = 1000)\n\n# Compute the F-distribution density\ny_f &lt;- df(x_f, df1 = df1, df2 = df2)\n\n# Create a data frame\nf_distribution_data &lt;- data.frame(x = x_f, y = y_f)\n\n# Generate the plot\nggplot(f_distribution_data, aes(x = x, y = y)) +\n  geom_line(color = \"steelblue\", size = 1) + # Line for the density curve\n  labs(\n    title = \"F-Distribution\",\n    subtitle = \"Probability density function with 5 and 10 degrees of freedom\",\n    x = \"F value\",\n    y = \"Probability density\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(ylim = c(0, 1), xlim = c(0, 5)) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.title = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nBecause continuous functions have an infinite number of \\(x\\)-values, the probability of any single value must be 0.1 Therefore, if we are interested in obtaining actual probabilities from the PDF, we can only do so for intervals of values. The probability that a value \\(X\\) falls into the interval \\(a &lt; X &lt; b\\) is in fact equivalent to the area under the curve between \\(a\\) and \\(b\\) (cf. Equation 2).\n\\[\nP(a &lt; X &lt; b) = \\int_a^b f(x)dx.\n\\tag{2}\\]\nRecall the PDF \\(f(x)\\) of the \\(\\chi^2\\)-distribution with 2 degrees of freedom. The \\(p\\)-value corresponds to the green area under the curve ranging from \\(x = 6.5\\) up to \\(\\infty\\), which can be restated formally in Equation 3. This brings us back to the definition of the \\(p\\)-value: It is the probability that the \\(\\chi^2\\) score is equal to 6.5 or higher, i.e., \\(P(\\chi^2 \\geq 6.5)\\).\n\\[\nP(6.5 &lt; X &lt; \\infty) = \\int_{6.5}^\\infty f(x)dx.\n\\tag{3}\\]",
    "crumbs": [
      "Overview",
      "5. Inferential Statistics",
      "5.1 Hypothesis testing"
    ]
  },
  {
    "objectID": "chapters/05-hypothesis_testing.html#practical-considerations",
    "href": "chapters/05-hypothesis_testing.html#practical-considerations",
    "title": "5.1 Hypothesis testing",
    "section": "4 Practical considerations",
    "text": "4 Practical considerations\n\n\n\n\n\n\nCommon pitfalls (cf. Agresti and Kateri 2022: 189-190)\n\n\n\n\nStatistical significance is NOT an indication of a causal relationship between the variables of interest (correlation \\(\\neq\\) causation).\n\\(p\\)-values do NOT signify the strength of an effect (\\(\\neq\\) effect size). It only helps identify whether there is an effect to begin with.\n\\(p\\)-values are NOT the probability of the null hypothesis being true.\nStatistical significance is only a starting point for further scientific inquiry, and by no means the end of it.",
    "crumbs": [
      "Overview",
      "5. Inferential Statistics",
      "5.1 Hypothesis testing"
    ]
  },
  {
    "objectID": "chapters/05-hypothesis_testing.html#exercises",
    "href": "chapters/05-hypothesis_testing.html#exercises",
    "title": "5.1 Hypothesis testing",
    "section": "5 Exercises",
    "text": "5 Exercises\n\nExercise 1 Schröter & Kortmann (2016) investigate the relationship between subject realisation (overt vs. null) and the grammatical category Person (1.p. vs. 2.p. vs. 3.p.) in three varieties of English (Great Britain vs. Hong Kong vs. Singapore). They report the following test results (2016: 235):\n\nChi-square test scores: \\[\n\\begin{align}\n\\text{Singapore: \\quad} & \\chi^2 = 3.3245, df = 2, p = 0.1897 \\\\\n\\text{Hong Kong: \\quad} & \\chi^2 = 40.799, df = 2, p &lt; 0.01 \\\\\n\\text{Great Britain: \\quad} & \\chi^2 = 3.6183, df = 2, p = 0.1638 \\\\\n\\end{align}\n\\]\n\n\nWhat hypotheses are the authors testing?\nAssuming a significance level \\(\\alpha = 0.05\\), what statistical conclusions can be drawn from the test results?\nWhat could be the theoretical implications of these results?\n\n\n\nExercise 2 Try to develop statistical hypotheses for a research project you are currently working on!",
    "crumbs": [
      "Overview",
      "5. Inferential Statistics",
      "5.1 Hypothesis testing"
    ]
  },
  {
    "objectID": "chapters/05-hypothesis_testing.html#footnotes",
    "href": "chapters/05-hypothesis_testing.html#footnotes",
    "title": "5.1 Hypothesis testing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe proof for the underlying theorem is given in Heumann et al. (2022: 544).↩︎",
    "crumbs": [
      "Overview",
      "5. Inferential Statistics",
      "5.1 Hypothesis testing"
    ]
  },
  {
    "objectID": "chapters/02-data_frames.html",
    "href": "chapters/02-data_frames.html",
    "title": "Data frames",
    "section": "",
    "text": "Script\n\n\n\nYou can find the full R script associated with this unit here.",
    "crumbs": [
      "Overview",
      "2. Introduction to R",
      "2.2 Data frames I"
    ]
  },
  {
    "objectID": "chapters/02-data_frames.html#preparation",
    "href": "chapters/02-data_frames.html#preparation",
    "title": "Data frames",
    "section": "",
    "text": "Script\n\n\n\nYou can find the full R script associated with this unit here.",
    "crumbs": [
      "Overview",
      "2. Introduction to R",
      "2.2 Data frames I"
    ]
  },
  {
    "objectID": "chapters/02-data_frames.html#recommended-reading",
    "href": "chapters/02-data_frames.html#recommended-reading",
    "title": "Data frames",
    "section": "2 Recommended reading",
    "text": "2 Recommended reading\n\nWinter (2020): Chapter 1.10-1.16\n\nSuggested video tutorial:\n\nUsing the Data Frame in R (DataCamp, 5min)\nLearn How to Subset, Extend & Sort Data Frames in R (DataCamp, 7min)",
    "crumbs": [
      "Overview",
      "2. Introduction to R",
      "2.2 Data frames I"
    ]
  },
  {
    "objectID": "chapters/02-data_frames.html#word-frequencies-ii",
    "href": "chapters/02-data_frames.html#word-frequencies-ii",
    "title": "Data frames",
    "section": "3 Word frequencies II",
    "text": "3 Word frequencies II\nRecall our corpus-linguistic data from the previous unit:\n\n\n\nLemma\nFrequency\n\n\n\n\nstart\n418\n\n\nenjoy\n139\n\n\nbegin\n337\n\n\nhelp\n281\n\n\n\nWe thought of the columns as one-dimensional, indexed lists of elements:\n\nlemma &lt;- c(\"start\", \"enjoy\", \"begin\", \"help\")\n\nfrequency &lt;- c(418, 139, 337, 281)\n\nActually, R allows us to combine these two vectors into something that resembles a real spreadsheet. To this end, we apply the data.frame() function to two vectors of our choice.\n\ndata &lt;- data.frame(lemma, frequency)\n\nprint(data)\n\n  lemma frequency\n1 start       418\n2 enjoy       139\n3 begin       337\n4  help       281\n\n\n\n3.1 Essential R concepts\nThe variable data is no longer a vector, but a data frame (often abbreviated as ‘df’). Once again, each element carries its own label and can, therefore, be accessed or manipulated.\nSince we now have two dimensions, the subsetting notation in square brackets [ ] has to reflect that. This is the general pattern:\n\\[ \\text{df[row, column]}\n\\tag{1}\\]\nSay, we’re looking for the element at the intersection of the first row and first column. Applying the pattern above, we can access it like so:\n\ndata[1,1]\n\n[1] \"start\"\n\n\nBut what if we need the entire first row? We simply omit the column part. Note, however, that the comma , needs to remain:\n\ndata[1,]\n\n  lemma frequency\n1 start       418\n\n\nSubsetting by columns is interesting. We can either use the square bracket notation [ ] or the column operator $:\n\ndata[,1]\n\n[1] \"start\" \"enjoy\" \"begin\" \"help\" \n\ndata$lemma\n\n[1] \"start\" \"enjoy\" \"begin\" \"help\" \n\n\n\n\n3.2 Filtering\nNot all the information contained in a data frame is always relevant for our research. In those cases, it’s important to subset the rows and columns according to certain criteria.\nAssume we only need those observations where the lemma frequencies are greater than 300. We can obtain those by specifying\n\nthe data frame,\nthe column of interest and\nthe condition to apply.\n\nYou can read the code below as\n\nTake the data frame data and subset it according to the column data$frequency. Show me those rows where the values of data$frequency are greater than 300.\n\n\ndata[data$frequency &gt; 300, ]\n\n  lemma frequency\n1 start       418\n3 begin       337\n\n\nWhat if we wanted to filter by lemma instead? Let’s say we’re looking for frequency data on the verbs start and help.\nThis will give us the row associated with start:\n\ndata[data$lemma == \"start\", ]\n\n  lemma frequency\n1 start       418\n\n\nCombining multiple statements requires a logical operator. Here we’re using | , which corresponds to a logical ‘or’ (disjunction).\n\ndata[data$lemma == \"start\" | data$lemma == \"help\", ]\n\n  lemma frequency\n1 start       418\n4  help       281\n\n\n\n\n\n\n\n\nWhy do we need to use “or” (|) and not “and” (&)?\n\n\n\nThe idea of combining statements somewhat naturally suggests a conjunction, which could be achieved via &. How come R doesn’t return anything if we do it that way?\n\ndata[data$lemma == \"start\" & data$lemma == \"help\", ]\n\n[1] lemma     frequency\n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\n\n\n3.3 I don’t like the way this looks – is there another way to filter in R?\nYes, absolutely. The subsections below demonstrate a few popular alternatives. In the end, the exact way you filter doesn’t really matter, so long as you (as well as the people who have to work with your script) can understand what you’re trying to achieve. Always consider adding comments to your filtering operations!\n\n\n\n\n\n\nsubset()\n\n\n\n\n\nAlmost every subsetting operation we perform with square brackets can also be performed using the subset() function. Here are some expressions that are synonymous to the ones above:\n\nsubset(data, frequency &gt; 300)\n\n  lemma frequency\n1 start       418\n3 begin       337\n\nsubset(data, lemma == \"start\" | lemma == \"help\")\n\n  lemma frequency\n1 start       418\n4  help       281\n\n\n\n\n\n\n\n\n\n\n\ntidyverse\n\n\n\n\n\nThe tidyverse-ecosystem is a collection of packages specifically designed for handling typical data science tasks as comfortably and elegantly as possible, supplying countless helper functions for data manipulation, transformation and visualisation. Installation instructions are provided in ?@sec-libraries.\nIt offers some appealing alternatives to the Base R subsetting functions. Let’s generate a tidyverse-style data frame, the tibble:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata2 &lt;- tibble(\n  lemma = c(\"start\", \"enjoy\", \"begin\", \"help\"),\n  frequency = c(418, 139, 337, 281)\n)\n\nprint(data2)\n\n# A tibble: 4 × 2\n  lemma frequency\n  &lt;chr&gt;     &lt;dbl&gt;\n1 start       418\n2 enjoy       139\n3 begin       337\n4 help        281\n\n\nWe can single out certain columns by using select():\n\nselect(data2, lemma)\n\n# A tibble: 4 × 1\n  lemma\n  &lt;chr&gt;\n1 start\n2 enjoy\n3 begin\n4 help \n\n\nIt is very easy to filter the data frame according to certain criteria:\n\nfilter(data2, frequency &gt; 300)\n\n# A tibble: 2 × 2\n  lemma frequency\n  &lt;chr&gt;     &lt;dbl&gt;\n1 start       418\n2 begin       337\n\nfilter(data2, lemma == \"start\" | lemma == \"help\")\n\n# A tibble: 2 × 2\n  lemma frequency\n  &lt;chr&gt;     &lt;dbl&gt;\n1 start       418\n2 help        281\n\n\nA extensive guide to the main tidyverse functions is provided in Chapter 3 of the free eBook R For Data Science (2nd edition).",
    "crumbs": [
      "Overview",
      "2. Introduction to R",
      "2.2 Data frames I"
    ]
  },
  {
    "objectID": "chapters/02-data_frames.html#exercises",
    "href": "chapters/02-data_frames.html#exercises",
    "title": "Data frames",
    "section": "4 Exercises",
    "text": "4 Exercises\n\n\n\n\n\n\nSolutions\n\n\n\nYou can find the solutions to the exercises here.\n\n\n\nExercise 1 Recreate the barplot from the previous unit by subsetting the data variable accordingly.\n\n\nExercise 2 Print the following elements by subsetting the data frame data accordingly.\n\n337\nbegin\nenjoy\nenjoy 139\nthe entire frequency column\n\n\n\nExercise 3 Extension of ?@exr-v-3. Verify that the following verbs are represented in the lemma column: enjoy, hit, find, begin. If they are in the data frame, print their frequency information.\n\n\nExercise 4 Extension of ?@exr-v-4. Use which() to find the rows where the frequency is greater than 200, and then print the lemma and frequency of only those rows.",
    "crumbs": [
      "Overview",
      "2. Introduction to R",
      "2.2 Data frames I"
    ]
  },
  {
    "objectID": "chapters/06-mixed_effects_regression.html",
    "href": "chapters/06-mixed_effects_regression.html",
    "title": "Mixed-effects regression",
    "section": "",
    "text": "For linguists:\n\nSchäfer (2020)\n\nGeneral:\n\nGelman and Hill (2007)",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.3 Mixed-effects regression"
    ]
  },
  {
    "objectID": "chapters/06-mixed_effects_regression.html#recommended-reading",
    "href": "chapters/06-mixed_effects_regression.html#recommended-reading",
    "title": "Mixed-effects regression",
    "section": "",
    "text": "For linguists:\n\nSchäfer (2020)\n\nGeneral:\n\nGelman and Hill (2007)",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.3 Mixed-effects regression"
    ]
  },
  {
    "objectID": "chapters/06-mixed_effects_regression.html#preparation",
    "href": "chapters/06-mixed_effects_regression.html#preparation",
    "title": "Mixed-effects regression",
    "section": "2 Preparation",
    "text": "2 Preparation\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(tidyr)\nlibrary(lme4) # for linear mixed-effects models\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nlibrary(sjPlot)\n\nInstall package \"strengejacke\" from GitHub (`devtools::install_github(\"strengejacke/strengejacke\")`) to load all sj-packages at once!\n\nlibrary(ggeffects)\nlibrary(ggpubr)\n\n# Load data\nvarmorph &lt;- read.csv(\"../datasets/varmorph_data.csv\", header = TRUE)\n\n# Reduce data\nvarmorph %&gt;%\n  select(rt, target, prime_type, subj_id) %&gt;%\n  filter(prime_type != \"filler\") %&gt;% \n  drop_na() -&gt; varmorph2\n\n# Overview\nglimpse(varmorph2)\n\nRows: 7,038\nColumns: 4\n$ rt         &lt;dbl&gt; 599.63, 885.39, 1124.94, 568.68, 726.24, 1095.37, 492.96, 6…\n$ target     &lt;chr&gt; \"print\", \"defend\", \"tempt\", \"hunt\", \"staple\", \"pose\", \"kick…\n$ prime_type &lt;chr&gt; \"derived\", \"derived\", \"derived\", \"derived\", \"derived\", \"der…\n$ subj_id    &lt;chr&gt; \"3202\", \"3202\", \"3202\", \"3202\", \"3202\", \"3202\", \"3202\", \"32…",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.3 Mixed-effects regression"
    ]
  },
  {
    "objectID": "chapters/06-mixed_effects_regression.html#sec-mer",
    "href": "chapters/06-mixed_effects_regression.html#sec-mer",
    "title": "Mixed-effects regression",
    "section": "3 Multilevel models",
    "text": "3 Multilevel models\nAt their core, mixed-effects models “are extensions of regression in which data are structured in groups and coefficients can vary by group” (Gelman and Hill 2007: 237). Typical grouping structures found in linguistic data include speakers, regions, or lexical stimuli for which multiple observations are attested. Normally, such structures would violate the assumption of independence, but can be controlled for by capturing group-wise tendencies.\nFor illustration, a simple example of a hierarchical dataset is presented in Figure 1. If one were to, for instance, measure test scores for every student, it may be of interest how their performance varies not only from student to student but also from school to school. After all, the students are nested within their schools.\n\n\n\n\n\n\n\n\ngraph LR\n    A1[School 1] --&gt; B11(Student 1)\n    A1[School 1] --&gt; B12(Student 2)\n    A1[School 1] --&gt; B13(Student 3)\n    \n    A2[School 2] --&gt; B21(Student 4)\n    A2[School 2] --&gt; B22(Student 5)\n    A2[School 2] --&gt; B23(Student 6)\n    \n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nRead the following (partial) description of the experiments conducted by Ciaccio & Veríssimo on the morphological processing of complex lexical items (2022):\n\nSixty-nine intermediate to advanced non-native speakers of English (54 women; 15 men) took part in the experiment in exchange for payment or course credits. […] The experiment included 102 English monomorphemic verbs used as targets (e.g., print). These were preceded by their -ed past-tense form (e.g., printed) as the inflected prime, their -er nominalization (e.g., printer) as the derived prime, or by an unrelated prime. Unrelated primes were dissimilar in form and meaning from their corresponding targets; half of them were -ed inflected forms and half of them were -er derived words. (Ciaccio and Veríssimo 2022: 2267)\n\nInspect varmorph2 and characterise its multilevel structure.\n\n\n\n3.1 Types of mixed-effects models\nVariance across groups can be captured by varying-intercept and/or varying-slope models. These varying coefficients also known as random effects (cf. Gelman and Hill (2007): 245). In the model equation, the intercept \\(\\alpha\\) and/or the slope \\(\\beta\\) is additionally indexed for the grouping factor. Let \\(J\\) denote the number of groups for \\(j = 1, ..., J\\).\n\n\n\n\n\n\nVarying-intercept model\n\n\n\nWe allow group-wise variation in the intercept by replacing \\(\\alpha\\) with \\(\\alpha_{j}\\) to indicate the intercept for the \\(j\\)-th group. It is defined as a random variable and follows the normal distribution. For instance, each participant in the aforementioned psycholinguistic would receive its own intercept rather than a global one for all participants.\n\\[\nY = \\alpha_{j} + \\beta_1X_{1} + \\beta_2X_{2} + ... + \\epsilon \\qquad \\alpha_{j} \\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha}^2)\n\\tag{1}\\]\n\n\n\n\n\n\n\n\nVarying-slope model\n\n\n\nWe will allow group-wise variation in the slope coefficients by replacing them with \\(\\beta_{ij}\\) to indicate the slope for the \\(j\\)-th group. The slope now functions as a random variable and is normally distributed. In the psycholinguistic study, each participant would be assigned its own slope coefficient.\n\\[\nY = \\alpha + \\beta_{1j}X_{1} + \\beta_{2j}X_{2} + ... + \\epsilon \\qquad \\beta_{j} \\sim N(\\mu_{\\beta}, \\sigma_{\\beta}^2)\n\\tag{2}\\]\n\n\n\n\n3.2 Example\nAssume we are predicting test performance by School. Using simulated data, the following series of plots plots illustrate …\n\n… School as a fixed effect,\n… random intercepts for each School,\n… random slopes for each School, and\n… random intercepts and slopes for each School.\n\n\n\n\n\n\n\nPlot: (1) Fixed-effects model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot: (2) Varying-intercept model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot: (3) Varying-slope model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot: (4) Varying-intercept and varying-slope model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3 Linear mixed-effects models\n\n\n3.4 Application in R\n\n3.4.1 Varying-intercept model\n\n# Varying intercept model\n\n# Define reference level for \"prime_type\"\nvarmorph2$prime_type &lt;- factor(varmorph2$prime_type, levels = c(\"unrelated\", \"derived\", \"inflected\"))\n\n# Fit mixed-effects models\nvarmorph.me &lt;- lmer(rt ~ prime_type + # fixed effect\n                      (1 | subj_id) + # let intercept vary by subject\n                      (1 | target), # # let intercept vary by target word\n                      data = varmorph2)\n\n# Summarise results\nsummary(varmorph.me)\n\n\ntab_model(varmorph.me, show.se = TRUE, show.aic = TRUE, show.dev = TRUE)\n\n\n\n \nrt\n\n\nPredictors\nEstimates\nstd. Error\nCI\np\n\n\n(Intercept)\n715.70\n12.66\n690.89 – 740.52\n&lt;0.001\n\n\nprime type [derived]\n-31.31\n5.01\n-41.14 – -21.48\n&lt;0.001\n\n\nprime type [inflected]\n-33.96\n5.01\n-43.78 – -24.13\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n29483.89\n\n\n\nτ00 target\n3690.56\n\n\nτ00 subj_id\n7693.61\n\n\nICC\n0.28\n\n\nN subj_id\n69\n\n\nN target\n102\n\nObservations\n7038\n\n\nMarginal R2 / Conditional R2\n0.006 / 0.283\n\n\nDeviance\n92860.321\n\n\nAIC\n92855.624\n\n\n\n\n\n\n\n\n\n\n\n\nICC\n\n\n\nThe intraclass correlation coefficient (ICC) “ranges from \\(0\\) if the grouping conveys no information to \\(1\\) if all members of a group are identical” (Gelman and Hill 2007: 258). In other words, it indicates how much of the variance in the outcome can be explained by the grouping factor (e.g. school or participant).\n\n\n\n\nShow the code\n# Extract random effects and their standard errors\nranef_obj &lt;- ranef(varmorph.me)  # Extract random effects with conditional variance\nse_ranef &lt;- arm::se.ranef(varmorph.me)           # Extract standard errors for random effects\n\n# Prepare a data frame for 'subj_id' random intercepts with confidence intervals\nsubj_ranef &lt;- ranef_obj$subj_id  # Random effects for subjects\nsubj_se &lt;- se_ranef$subj_id      # Standard errors for subjects\n\n# Combine random effects and standard errors into a data frame\nsubj_df &lt;- data.frame(\n  subj_id = rownames(subj_ranef),\n  intercept = subj_ranef[, \"(Intercept)\"],\n  se = subj_se[, \"(Intercept)\"],\n  conf.low = subj_ranef[, \"(Intercept)\"] - 1.96 * subj_se[, \"(Intercept)\"],\n  conf.high = subj_ranef[, \"(Intercept)\"] + 1.96 * subj_se[, \"(Intercept)\"]\n)\n\n# Create the waterfall plot\nggplot(subj_df, aes(x = reorder(subj_id, intercept), y = intercept)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", col = \"grey10\") +\n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\n  coord_flip() +\n  geom_vline(xintercept = 0, linetype = \"dashed\", col = \"grey10\") +\n  labs(title = \"Random Intercepts by Subject\",\n       x = \"Subject ID\",\n       y = \"Random Intercept\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n3.4.2 Varying-slope model\n\n# Varying-slope model; replace 0 with 1 if you want the intercept to vary too\nvarmorph.me2 &lt;- lmer(rt ~ prime_type +\n                      (0 + prime_type | subj_id),\n                      data = varmorph2)\n\nsummary(varmorph.me2)\n\n\ntab_model(varmorph.me2, show.se = TRUE, show.aic = TRUE, show.dev = TRUE)\n\n\n\n \nrt\n\n\nPredictors\nEstimates\nstd. Error\nCI\np\n\n\n(Intercept)\n715.70\n11.11\n693.92 – 737.49\n&lt;0.001\n\n\nprime type [derived]\n-31.31\n5.57\n-42.23 – -20.39\n&lt;0.001\n\n\nprime type [inflected]\n-33.96\n5.32\n-44.38 – -23.53\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n33126.00\n\n\n\nτ00\n \n\n\nτ00\n \n\n\nτ11 subj_id.prime_typeunrelated\n7545.01\n\n\nτ11 subj_id.prime_typederived\n8193.86\n\n\nτ11 subj_id.prime_typeinflected\n7385.08\n\n\nρ01\n \n\n\nρ01\n \n\n\nICC\n0.14\n\n\nN subj_id\n69\n\nObservations\n7038\n\n\nMarginal R2 / Conditional R2\n0.006 / 0.141\n\n\nDeviance\n93452.160\n\n\nAIC\n93455.369\n\n\n\n\n\n\n\n\nShow the code\n# Extract random slopes\nranef_data1 &lt;- ranef(varmorph.me2)$subj_id\n\n# Extract standard errors\nranef_data1_se &lt;- arm::se.ranef(varmorph.me2)$subj_id\n\n# Convert data into long format\nrandom_effects_df &lt;- ranef_data1 %&gt;%\n  as.data.frame() %&gt;% \n  rownames_to_column(var = \"subj_id\") %&gt;%\n  pivot_longer(cols = -subj_id, \n               names_to = \"prime_type\", \n               values_to = \"random_effect\")\n\n\n# Create a data frame for standard errors\nse_df &lt;- as.data.frame(ranef_data1_se) %&gt;%\n  rownames_to_column(var = \"subj_id\") %&gt;%\n  pivot_longer(cols = -subj_id, \n               names_to = \"prime_type\", \n               values_to = \"se\")\n\n\n# Combine random effects with standard errors\ncombined_df &lt;- random_effects_df %&gt;%\n  left_join(se_df, by = c(\"subj_id\", \"prime_type\"))\n\n# Calculate confidence intervals\ncombined_df &lt;- combined_df %&gt;%\n  mutate(lower_ci = random_effect - 1.96 * se,\n         upper_ci = random_effect + 1.96 * se)\n\n\n# Dotplots with confidence intervals\nggplot(combined_df, aes(x = subj_id, y = random_effect, col = prime_type)) +\n  coord_flip() +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", col = \"grey10\") +\n  facet_wrap(~ prime_type) +\n  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), width = 0.2) +\n  labs(title = \"Random Effect of Prime Type by Subject ID\",\n       x = \"Random Slope\",\n       y = \"Subject ID\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.3 Mixed-effects regression"
    ]
  },
  {
    "objectID": "chapters/02-libraries.html",
    "href": "chapters/02-libraries.html",
    "title": "Libraries",
    "section": "",
    "text": "Winter (2020): Chapter 1.13",
    "crumbs": [
      "Overview",
      "2. Introduction to R",
      "2.3 Libraries"
    ]
  },
  {
    "objectID": "chapters/02-libraries.html#recommended-reading",
    "href": "chapters/02-libraries.html#recommended-reading",
    "title": "Libraries",
    "section": "",
    "text": "Winter (2020): Chapter 1.13",
    "crumbs": [
      "Overview",
      "2. Introduction to R",
      "2.3 Libraries"
    ]
  },
  {
    "objectID": "chapters/02-libraries.html#working-with-packages-in-r",
    "href": "chapters/02-libraries.html#working-with-packages-in-r",
    "title": "Libraries",
    "section": "2 Working with packages in R",
    "text": "2 Working with packages in R\nPackages expand the basic functionality of R by providing numerous quality-of-life improvements that not only considerably simplify common data wrangling tasks but which also provide frameworks for state-of-the-art methods for statistical analysis and natural language processing (NLP), among many other things.\n\n2.1 Installation\n\n\n\n\n\n\nHow do I install a library?\n\n\n\n\n\nNavigate to Packages &gt; Install and verify that the pop-up window says Install from: Repository (CRAN). You can now type in the name of the package you would like to install under Packages.\nVideo tutorial on YouTube\n\n\n\nThis reader will use functions from a variety of R packages. Please install the following ones:\n\n\n\nPackage\nPurpose\nSession\n\n\n\n\nreadxl\nImporting Microsoft Excel files\nImporting/exporting\n\n\nwritexl\nExporting Microsoft Excel files\nImporting/exporting\n\n\nquanteda\nAnalysis of text data\nConcordancing\n\n\nlattice\nData visualisation\nConcordancing\n\n\ntidyverse\nFramework for data manipulation and visualisation\nCategorical data, Continuous data\n\n\ncrosstable\nCreating contingency tables\nCategorical data\n\n\nflextable\nExporting contingency tables\nCategorical data\n\n\nconfintr\nEffect size measure for categorical data\nChi-squared test\n\n\n\n\n\n2.2 Loading packages\nOnce the installation has been completed, you can proceed to load the libraries using the code below. You can ignore the warning messages.\n\nlibrary(readxl)\nlibrary(writexl)\nlibrary(quanteda)\n\nPackage version: 3.3.1\nUnicode version: 14.0\nICU version: 70.1\n\n\nParallel computing: 8 of 8 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(lattice)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(crosstable)\n\n\nAttaching package: 'crosstable'\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\nlibrary(flextable)\n\n\nAttaching package: 'flextable'\n\nThe following object is masked from 'package:purrr':\n\n    compose\n\nlibrary(confintr)\n\n\n\n\n\n\n\nActivating libraries\n\n\n\n\n\nWhenever you start a new R session (i.e., open RStudio), your libraries and their respective functions will be inactive. To re-activate a library, either use the library() function or simply select it in the Packages tab.\n\n\n\nIt is good practice to only activate those packages that are necessary for your analysis. While it won’t be a problem for the small set of packages as shown here, loading dozens of packages increases the risk of obtaining “homonymous” functions which have the same name but perform different operations. In this case, it might be helpful to “disambiguate” them by directly indicating which package a function is from:\n\nreadxl::read_xlsx(...)\n\n\n\n2.3 Citing R and R packages\nWhenever we draw on ideas other than our own, we give credit to the respective source by citing it appropriately. The same applies to R, RStudio as well as all the packages we rely on throughout our analyses.\nFor R, an up-to-date citation can be generated as follows:\n\ncitation()\n\n\nTo cite R in publications use:\n\n  R Core Team (2023). R: A language and environment for statistical\n  computing. R Foundation for Statistical Computing, Vienna, Austria.\n  URL https://www.R-project.org/.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2023},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nTo cite a specific package, simply supply the package name as an argument.\n\ncitation(\"quanteda\")\n\n\nTo cite package 'quanteda' in publications use:\n\n  Benoit K, Watanabe K, Wang H, Nulty P, Obeng A, Müller S, Matsuo A\n  (2018). \"quanteda: An R package for the quantitative analysis of\n  textual data.\" _Journal of Open Source Software_, *3*(30), 774.\n  doi:10.21105/joss.00774 &lt;https://doi.org/10.21105/joss.00774&gt;,\n  &lt;https://quanteda.io&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {quanteda: An R package for the quantitative analysis of textual data},\n    journal = {Journal of Open Source Software},\n    author = {Kenneth Benoit and Kohei Watanabe and Haiyan Wang and Paul Nulty and Adam Obeng and Stefan Müller and Akitaka Matsuo},\n    doi = {10.21105/joss.00774},\n    url = {https://quanteda.io},\n    volume = {3},\n    number = {30},\n    pages = {774},\n    year = {2018},\n  }",
    "crumbs": [
      "Overview",
      "2. Introduction to R",
      "2.3 Libraries"
    ]
  },
  {
    "objectID": "chapters/01-basics.html",
    "href": "chapters/01-basics.html",
    "title": "1.1 Basics",
    "section": "",
    "text": "New quotatives in American English: be like, go, be all:\n\n“I was like ‘well if it’s not leaking, I don’t care.’”\n“Do you, I asked I was running out and I go, ‘you’re trying to deliver a pizza here?’”\n“My sister’s all ‘excuse me would you mind if I gave you, if I want your autograph.’”\n\n\n\n\n\nPrevious results with regard to age: Be like is known to be preferred by younger speakers.\nPrevious results with regard to sex are inconclusive.\n\n\n\n\n\n“The study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all” (Barbieri 2007: 26).\n\n\n\n\n\nChoice of corpus: 450,000 words taken from the Conversation component of the Longman Spoken and Written English Corpus (all files where the age and gender of speakers were available), i.e., about 50 hours of casual conversation coded for speakers’ age and sex.\nData extraction: All forms of be like, be all, go, say extracted using corpus linguistic software; data entered into spreadsheet software; word counts of texts added via Delphi program.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuotative\nAge group\nMale use (per 100,000 words)\nFemale use (per 100,000 words)\n\n\n\n\nsay\n16–26\n115\n149\n\n\n\n27–40\n101\n117\n\n\n\n41–54\n55\n214\n\n\n\n55+\n67\n433\n\n\nbe like\n16–26\n32\n329\n\n\n\n27–40\n66\n25\n\n\n\n41–54\n-\n1\n\n\n\n55+\n-\n19\n\n\ngo\n16–26\n25\n113\n\n\n\n27–40\n61\n18\n\n\n\n41–54\n-\n15\n\n\n\n55+\n-\n7\n\n\nbe all\n16–26\n14\n33\n\n\n\n\n\n\n\n\nReproduction of Barbieri’s Figure 5 ‘Proportional quotative use by men and women aged 16–26 and 27–40’\n\n\n\n\n\n\n\n\n“In sum, the patterns of proportional use and the results of the statistical analyses show that there are striking differences in the way that men and women under the age of forty use quotative verbs. In contrast, in conforming to the traditional quotative say, men and women over forty displayed similar behavior” (Barbieri 2007: 38–39).\n\n\n\n\n\n“In favoring be like over other quotatives, men in their late 20s and in their 30s display an ‘affinity’ with slightly younger women, women in their early 20s, rather than with women of their same age” (Barbieri 2007: 41).",
    "crumbs": [
      "Overview",
      "1. Theory",
      "1.1 Basics"
    ]
  },
  {
    "objectID": "chapters/01-basics.html#barbierioldermenyounger2007-older-men-and-younger-women",
    "href": "chapters/01-basics.html#barbierioldermenyounger2007-older-men-and-younger-women",
    "title": "1.1 Basics",
    "section": "",
    "text": "New quotatives in American English: be like, go, be all:\n\n“I was like ‘well if it’s not leaking, I don’t care.’”\n“Do you, I asked I was running out and I go, ‘you’re trying to deliver a pizza here?’”\n“My sister’s all ‘excuse me would you mind if I gave you, if I want your autograph.’”\n\n\n\n\n\nPrevious results with regard to age: Be like is known to be preferred by younger speakers.\nPrevious results with regard to sex are inconclusive.\n\n\n\n\n\n“The study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all” (Barbieri 2007: 26).\n\n\n\n\n\nChoice of corpus: 450,000 words taken from the Conversation component of the Longman Spoken and Written English Corpus (all files where the age and gender of speakers were available), i.e., about 50 hours of casual conversation coded for speakers’ age and sex.\nData extraction: All forms of be like, be all, go, say extracted using corpus linguistic software; data entered into spreadsheet software; word counts of texts added via Delphi program.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuotative\nAge group\nMale use (per 100,000 words)\nFemale use (per 100,000 words)\n\n\n\n\nsay\n16–26\n115\n149\n\n\n\n27–40\n101\n117\n\n\n\n41–54\n55\n214\n\n\n\n55+\n67\n433\n\n\nbe like\n16–26\n32\n329\n\n\n\n27–40\n66\n25\n\n\n\n41–54\n-\n1\n\n\n\n55+\n-\n19\n\n\ngo\n16–26\n25\n113\n\n\n\n27–40\n61\n18\n\n\n\n41–54\n-\n15\n\n\n\n55+\n-\n7\n\n\nbe all\n16–26\n14\n33\n\n\n\n\n\n\n\n\nReproduction of Barbieri’s Figure 5 ‘Proportional quotative use by men and women aged 16–26 and 27–40’\n\n\n\n\n\n\n\n\n“In sum, the patterns of proportional use and the results of the statistical analyses show that there are striking differences in the way that men and women under the age of forty use quotative verbs. In contrast, in conforming to the traditional quotative say, men and women over forty displayed similar behavior” (Barbieri 2007: 38–39).\n\n\n\n\n\n“In favoring be like over other quotatives, men in their late 20s and in their 30s display an ‘affinity’ with slightly younger women, women in their early 20s, rather than with women of their same age” (Barbieri 2007: 41).",
    "crumbs": [
      "Overview",
      "1. Theory",
      "1.1 Basics"
    ]
  },
  {
    "objectID": "chapters/01-basics.html#principles-of-empirical-linguistics",
    "href": "chapters/01-basics.html#principles-of-empirical-linguistics",
    "title": "1.1 Basics",
    "section": "Principles of Empirical Linguistics",
    "text": "Principles of Empirical Linguistics\n\nObjectivity — Independence from researchers or devices (→ replicability!)\nReliability — Studies should be replicable.\nValidity — A study must actually address the problem formulated in the research question.",
    "crumbs": [
      "Overview",
      "1. Theory",
      "1.1 Basics"
    ]
  },
  {
    "objectID": "chapters/01-basics.html#exercises",
    "href": "chapters/01-basics.html#exercises",
    "title": "1.1 Basics",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1 As you read through the sections above, think about what might go wrong at each stage of the study!\n\n\nExercise 2 Read another sociolinguistic study (e.g., Unuabonah and Gut 2018). As you read, identify similar key sections or “building blocks” used in this study (e.g., introduction, research question etc.). What purpose does each section serve in the study’s overall structure?",
    "crumbs": [
      "Overview",
      "1. Theory",
      "1.1 Basics"
    ]
  },
  {
    "objectID": "chapters/06-ordinal_regression.html",
    "href": "chapters/06-ordinal_regression.html",
    "title": "Ordinal regression",
    "section": "",
    "text": "General:\n\nBaguley (2012): Chapter 17.4.5\nO’Connell (2006)\nPowers and Xie (2008): Chapter 7\nDocumentation of Cumulative Link Models",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.5 Ordinal regression"
    ]
  },
  {
    "objectID": "chapters/06-ordinal_regression.html#suggested-reading",
    "href": "chapters/06-ordinal_regression.html#suggested-reading",
    "title": "Ordinal regression",
    "section": "",
    "text": "General:\n\nBaguley (2012): Chapter 17.4.5\nO’Connell (2006)\nPowers and Xie (2008): Chapter 7\nDocumentation of Cumulative Link Models",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.5 Ordinal regression"
    ]
  },
  {
    "objectID": "chapters/06-ordinal_regression.html#introduction",
    "href": "chapters/06-ordinal_regression.html#introduction",
    "title": "Ordinal regression",
    "section": "2 Introduction",
    "text": "2 Introduction\nIn her recent contribution, Glass (2021) examines possible reasons why certain transitive verbs have a stronger affinity towards object omission compared to others, placing special emphasis on the routinisation of the actions denoted by the verbs. Specifically, she assesses how high/low-routine contexts affect the acceptability of object omission for transitive verbs from different frequency bins.\nWe will replicate her findings using her survey data Glass_2021_survey_processed.csv1:\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ordinal)\n\n\nAttaching package: 'ordinal'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(sjPlot)\n\nLearn more about sjPlot with 'browseVignettes(\"sjPlot\")'.\n\nlibrary(effects)\n\nLoading required package: carData\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nlibrary(ggeffects)\nlibrary(ggpubr)\n\n# For additional tests\nlibrary(DescTools)\nlibrary(generalhoslem)\n\nLoading required package: reshape\n\nAttaching package: 'reshape'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nThe following object is masked from 'package:dplyr':\n\n    rename\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, smiths\n\nlibrary(brant)\n\n# Load data\nsurvey &lt;- read.csv(\"../datasets/Glass_2021_survey_processed.csv\")\n\n# Inspect dataset\nstr(survey)\n\n'data.frame':   784 obs. of  5 variables:\n $ rating       : int  3 4 3 3 2 4 3 2 3 2 ...\n $ freq         : chr  \"hi\" \"lo\" \"lo\" \"hi\" ...\n $ verb         : chr  \"pick\" \"catch\" \"throw\" \"break\" ...\n $ ParticipantID: chr  \"Participant1\" \"Participant1\" \"Participant1\" \"Participant1\" ...\n $ routine      : chr  \"hi\" \"lo\" \"lo\" \"lo\" ...\n\nhead(survey)\n\n  rating freq   verb ParticipantID routine\n1      3   hi   pick  Participant1      hi\n2      4   lo  catch  Participant1      lo\n3      3   lo  throw  Participant1      lo\n4      3   hi  break  Participant1      lo\n5      2   hi  taste  Participant1      hi\n6      4   hi bottle  Participant1      hi\n\n\n\n\n\n\n\n\nShort breakdown of the variables\n\n\n\n\n\n\nroutine: In Glass’s study, transitive verbs were randomly assigned to one of the following conditions:\n\n\n\n(High routine condition:) I worked at my poultry farm. Just like I always do, I butchered some chickens. Then I gathered some eggs.\n(Low-routine condition:) I visited a friend’s job. Just because people wanted me to try it, I butchered some chickens. Then I went for a walk.\n\nCf. Glass (2021: 66)\n\n\nunique(survey$routine)\n\n[1] \"hi\" \"lo\"\n\n\n\nrating records the responses of participants to a follow-up question regarding the acceptability of object omission. The answers are recorded on a 1-5 Likert scale.\n\n\nThe next time Caroline talks about butchering chickens the day before, how likely do you think she is to say the following?\n‘I butchered yesterday’\nCf. Glass (2021: 66)\n\n\nunique(survey$rating)\n\n[1] 3 4 2 5 1\n\n\n\nverb contains the items to be rated for the conditions in routine\n\n\nunique(survey$verb)\n\n[1] \"pick\"   \"catch\"  \"throw\"  \"break\"  \"taste\"  \"bottle\" \"sell\"   \"chop\"  \n\n\n\nfrequency relates to the frequency bins of the verbs:\n\n\nunique(survey$freq)\n\n[1] \"hi\" \"lo\"\n\n\n\nParticipantID identifies each of the 98 subjects who provided ratings",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.5 Ordinal regression"
    ]
  },
  {
    "objectID": "chapters/06-ordinal_regression.html#descriptive-overview",
    "href": "chapters/06-ordinal_regression.html#descriptive-overview",
    "title": "Ordinal regression",
    "section": "3 Descriptive overview",
    "text": "3 Descriptive overview\n\n\nShow the code\nggplot(survey, aes(x = rating, fill = freq)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~freq) +\n  theme_minimal() +\n  labs(title = \"Density of Ratings by Frequency\",\n       x = \"Rating\", y = \"Density\", fill = \"Frequency\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(survey, aes(x = rating, fill = routine)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~routine) +\n  theme_minimal() +\n  labs(title = \"Density of Ratings by Routine\",\n       x = \"Rating\", y = \"Density\", fill = \"Routine\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggridges)\nggplot(survey, aes(x = rating, y = verb, fill = verb)) +\n  geom_density_ridges(scale = 5, rel_min_height = 0.01, alpha = 0.6) +\n  theme_ridges() +\n  #theme(legend.position = \"none\") +\n  labs(title = \"Distribution of Ratings by Verb\",\n       x = \"Rating\", y = \"Verb\")\n\n\nPicking joint bandwidth of 0.381",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.5 Ordinal regression"
    ]
  },
  {
    "objectID": "chapters/06-ordinal_regression.html#modelling-ordinal-data",
    "href": "chapters/06-ordinal_regression.html#modelling-ordinal-data",
    "title": "Ordinal regression",
    "section": "4 Modelling ordinal data",
    "text": "4 Modelling ordinal data\nOur task is clear: We need to measure how routine and freq affect the variability in the acceptability ratings, while controlling for repeated measurements for verb and ParticipantID, which impose a hierarchical structure on the dataset.\nFormally speaking, we have \\(p\\) explanatory variables \\(X_1, X_2, ..., X_p\\) for \\(1, ..., p\\). The target variable, i.e. our \\(Y\\), is rating with the ordered, discrete outcomes \\(y \\in \\{1, 2, 3, 4, 5\\}\\).\nThe goal is to find a model \\(f\\) that describes the relationship between \\(Y\\) and \\(X_p\\) as accurately as possible and minimises the error term \\(\\epsilon\\):\n\\[\nY = f(X_1, X_2, ..., X_p) + \\epsilon\n\\tag{1}\\]\n\n4.1 Ordered logistic regression\nOne family of models that respects the ordered, yet categorical nature of \\(Y\\) is ordered (or ordinal) logistic regression. Other terms include proportional odds models and cumulative logit/link models.\n\n\n\n\n\n\nRecap: Logistic regression\n\n\n\n\n\nLogistic regression is used to model categorical response variables with two or more levels. For instance, let’s assume our \\(Y\\) is dichotomous with the following two outcomes:\n\\[\nY =\n\\begin{cases}\n\\text{yes} \\\\\n\\text{no}\n\\end{cases}\n\\tag{2}\\]\nUsing the logistic function, we can estimate the probability of one outcome versus the other given the predictors \\(X_p\\). Their log-transformed odds ratio (log odds) is equivalent of the all-too-familiar linear model:\n\\[\n\\log\\left(\\frac{P(Y = yes \\mid X_1, X_2, ..., X_p)}{1 - P(Y = yes \\mid X_1, X_2, ..., X_p)}\\right) = \\beta_0 + \\sum_{i=1}^p \\beta_iX_i\n\\tag{3}\\]\n\n\n\nCore to this approach is the notion of cumulative probabilities. Let \\(J\\) denote the number of ordered categories in \\(Y\\). In Glass’s case study, the estimated cumulative probabilities for each ordered outcome (= acceptability rating) would have the forms in Equation 4.\n\\[\n\\begin{array}{rcl}\nP(Y \\leq 1) & = & P(Y = 1) \\\\\nP(Y \\leq 2) & = & P(Y = 1) + P(Y = 2) \\\\\nP(Y \\leq 3) & = & P(Y = 1) + P(Y = 2) + P(Y = 3) \\\\\n& \\vdots & \\\\\nP(Y \\leq j) & = & P_1 + ... + P_j\n\\end{array}\n\\tag{4}\\]\nWe can now update our logistic regression model to take into account cumulative probabilities for \\(j = 1, ..., J-1\\).\n\\[\n\\log\\left(\\frac{P(Y \\leq j \\mid X_1, X_2, ..., X_p)}{1 - P(Y \\leq j \\mid X_1, X_2, ..., X_p)}\\right) = \\beta_0 + \\sum_{i=1}^p \\beta_iX_i\n\\tag{5}\\]\nThe intercepts \\(\\beta_{0_j}\\) serve as cutpoints between the adjacent ordinal categories. For \\(J = 5\\) categories, there are \\(J - 1 = 4\\) cutpoints, i.e.,\n\n1|2 for \\(P(Y \\leq 1)\\)\n2|3 for \\(P(Y \\leq 2)\\)\n3|4 for \\(P(Y \\leq 3)\\)\n4|5 for \\(P(Y \\leq 4)\\).\n\nGiven a change in predictor values, the slope coefficients \\(\\beta_pX_p\\) indicate how the probability of being in a higher rating category changes (Baguley 2012: 691–2).\nWe can obtain “regular” probabilities from the cumulative ones by drawing on the equivalence in Equation 6.\n\\[\nP(Y = j) = P(Y \\leq j) - P(Y \\leq j - 1)\n\\tag{6}\\]\nFor instance, the probability \\(P(Y = 3)\\) is equivalent to\n\\[\nP(Y = 3) = P(Y \\leq 3) - P(Y \\leq 2).\n\\tag{7}\\]\n\n\n\n\n\n\nAssumptions of proportional odds models\n\n\n\n\n\nThe proportional odds assumption stipulates a stable effect of the predictors on the (log) odds of the ordinal outcomes across all possible cutpoints (O’Connell 2006: 29). In case of violation, it is better to rely on partial proportional odds models or multinomial logistic regression instead.",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.5 Ordinal regression"
    ]
  },
  {
    "objectID": "chapters/06-ordinal_regression.html#application-in-r",
    "href": "chapters/06-ordinal_regression.html#application-in-r",
    "title": "Ordinal regression",
    "section": "5 Application in R",
    "text": "5 Application in R\nThere are several R packages that support ordinal logistic regression models. This section provides an overview of some of the more common (as well as well-documented) implementations.\n\n5.1 Using polr() from the MASS library\n\n# Convert to factor and determine ordering\nsurvey$rating &lt;- factor(survey$rating, ordered = TRUE, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n# Fit polr model\nsurvey.polr &lt;- polr(rating ~ \n                      freq +\n                      routine,\n                      data = survey)\n\n# Model summary\nsummary(survey.polr)\n\n\nRe-fitting to get Hessian\n\n\nCall:\npolr(formula = rating ~ freq + routine, data = survey)\n\nCoefficients:\n             Value Std. Error  t value\nfreqlo    -0.01095     0.1291 -0.08483\nroutinelo -0.55521     0.1302 -4.26449\n\nIntercepts:\n    Value   Std. Error t value\n1|2 -0.8704  0.1228    -7.0859\n2|3  0.1342  0.1188     1.1290\n3|4  1.2528  0.1293     9.6856\n4|5  2.8915  0.2003    14.4345\n\nResidual Deviance: 2246.662 \nAIC: 2258.662 \n\n# R-squared and AIC\nPseudoR2(survey.polr, c(\"Nagelkerke\", \"AIC\"))\n\n  Nagelkerke          AIC \n   0.0244793 2258.6617419 \n\n\n\ntab_model(survey.polr, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, transform = NULL)\n\n\nRe-fitting to get Hessian\n\n\n\n\n\n \nrating\n\n\nPredictors\nLog-Odds\nstd. Error\nCI\np\n\n\n1|2\n-0.87\n0.12\n-1.11 – -0.63\n&lt;0.001\n\n\n2|3\n0.13\n0.12\n-0.10 – 0.37\n0.259\n\n\n3|4\n1.25\n0.13\n1.00 – 1.51\n&lt;0.001\n\n\n4|5\n2.89\n0.20\n2.50 – 3.28\n&lt;0.001\n\n\nfreq [lo]\n-0.01\n0.13\n-0.26 – 0.24\n0.932\n\n\nroutine [lo]\n-0.56\n0.13\n-0.81 – -0.30\n&lt;0.001\n\n\nObservations\n784\n\n\nR2 Nagelkerke\n0.024\n\n\nDeviance\n2246.662\n\n\nAIC\n2258.662\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the model parameters\n\n\n\n\n\n\nCoefficients: The conditions freqlo (low frequency) and routinelo (low-routine context) both have negative values, which means that both of them decrease the probability of obtaining a higher acceptability rating (compared to freqhi and routinehi).\nIntercepts: These represent the cutpoints between the ordinal categories, which are necessary for calculating the probabilities of each ordinal category.\n\n\n\n\n\n\n5.2 Testing assumptions and goodness of fit\n\nTest proportional odds assumption:\n\n\nbrant(survey.polr) # p &lt; 0.05 is a violation of the assumption\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     14.45   6   0.02\nfreqlo      6.5 3   0.09\nroutinelo   8.14    3   0.04\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\n\nHosmer-Lemeshow test, which is essentially a \\(\\chi^2\\)-test:\n\n\nlogitgof(survey$rating, # observed\n         fitted(survey.polr), # expected\n         ord = TRUE) # respect ordering\n\n\n    Hosmer and Lemeshow test (ordinal model)\n\ndata:  survey$rating, fitted(survey.polr)\nX-squared = 51.173, df = 35, p-value = 0.03808\n\n\n\nThe Lipsitz test is an extension of the Hosmer-Lemeshow test. Note that it requires the response to be a factor.\n\n\nlipsitz.test(survey.polr)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  rating ~ freq + routine\nLR statistic = 20.261, df = 9, p-value = 0.01637\n\n\n\nPart of the same family of tests is the Pulkstenis-Robinson test, which also relies on the \\(\\chi^2\\)-distribution:\n\n\npulkrob.chisq(survey.polr, catvars = c(\"freq\", \"routine\"))\n\n\n    Pulkstenis-Robinson chi-squared test\n\ndata:  formula:  rating ~ freq + routine\nX-squared = 21.476, df = 9, p-value = 0.0107\n\n\n\n\n5.3 Visualisation\n\n5.3.1 With effects\n\n# Routine effect plot\nplot(Effect(focal.predictors = c(\"routine\"), mod = survey.polr), rug = FALSE, style=\"stacked\")\n\n\n\n5.3.2 With ggeffects and ggplot2\n\n\nShow the code\n# Get the ggeffects data\neff &lt;- ggeffects::ggeffect(survey.polr, \"freq\")\n\n# Convert to a data frame\nplot_data &lt;- as.data.frame(eff)\n\n# Ensure the response.level has the desired levels\nplot_data$response.level &lt;- factor(plot_data$response.level, \n                                   levels = c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"),\n                                   labels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n# Create the plot with confidence intervals\np1 &lt;- ggplot(plot_data, aes(x = x, y = predicted, color = response.level, group = response.level)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1, linewidth = 0.5) +\n  scale_color_viridis_d() +\n  labs(\n    x = \"Frequency\",\n    y = \"Predicted Probability\",\n    color = \"Rating\"\n  ) +\n  facet_grid(~ response.level) +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    panel.grid.minor = element_blank(),\n    axis.title = element_text(face = \"bold\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(margin = margin(b = 10))\n  ) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1))\n\np1\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Get the ggeffects data for \"routine\"\neff_routine &lt;- ggeffects::ggeffect(survey.polr, \"routine\")\n\n# Convert to a data frame\nplot_data_routine &lt;- as.data.frame(eff_routine)\n\n# Ensure the response.level has the desired levels for \"routine\"\nplot_data_routine$response.level &lt;- factor(plot_data_routine$response.level, \n                                           levels = c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"),\n                                           labels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n# Create the second plot for \"routine\"\np2 &lt;- ggplot(plot_data_routine, aes(x = x, y = predicted, color = response.level, group = response.level)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1, linewidth = 0.5) +\n  scale_color_viridis_d() +\n  labs(\n    x = \"Routine\",\n    y = \"Predicted Probability\",\n    color = \"Rating\"\n  ) +\n  facet_grid(~ response.level) +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    panel.grid.minor = element_blank(),\n    axis.title = element_text(face = \"bold\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(margin = margin(b = 10))\n  ) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1))\n\np2\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Generate the interaction effects for \"freq\" and \"routine\"\neff_interaction &lt;- ggeffect(survey.polr, terms = c(\"freq\", \"routine\"))\n\n# Convert to a data frame\nplot_data_interaction &lt;- as.data.frame(eff_interaction)\n\n# Ensure the response.level has the desired levels\nplot_data_interaction$response.level &lt;- factor(plot_data_interaction$response.level, \n                                               levels = c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"),\n                                               labels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n\n# Create the interaction plot with facet by 'x' and color by 'response.level'\np_interaction &lt;- ggplot(plot_data_interaction, aes(x = group, y = predicted, color = response.level, group = response.level)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1, linewidth = 0.5) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Predicted Probabilities for Interaction of Frequency and Routine\",\n    x = \"Routine\",\n    y = \"Predicted Probability\",\n    color = \"Rating\"\n  ) +\n  facet_wrap(~ x, labeller = labeller(x = c(\"hi\" = \"High Frequency\", \"lo\" = \"Low Frequency\"))) +  # Facet by \"freq\"\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    panel.grid.minor = element_blank(),\n    axis.title = element_text(face = \"bold\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(margin = margin(b = 10))\n  ) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1))\n\n# Display the interaction plot\np_interaction\n\n\n\n\n\n\n\n\n\n\n\n\n5.4 Using clm() from the ordinal library\n\n# Convert to factor and determine ordering\nsurvey$rating &lt;- factor(survey$rating, ordered = TRUE, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n# Fit cumulative link model\nclm.1 &lt;- ordinal::clm(rating ~ \n                    freq +\n                    routine,\n                    data = survey, Hess=TRUE)\n\n# Model summary\nsummary(clm.1)\n\n\ntab_model(clm.1, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, transform = NULL)\n\n\n\n\n \nrating\n\n\nPredictors\nLog-Odds\nstd. Error\nCI\np\n\n\n1|2\n-0.87\n0.12\n-1.11 – -0.63\n&lt;0.001\n\n\n2|3\n0.13\n0.12\n-0.10 – 0.37\n0.259\n\n\n3|4\n1.25\n0.13\n1.00 – 1.51\n&lt;0.001\n\n\n4|5\n2.89\n0.20\n2.50 – 3.28\n&lt;0.001\n\n\nfreq [lo]\n-0.01\n0.13\n-0.26 – 0.24\n0.932\n\n\nroutine [lo]\n-0.56\n0.13\n-0.81 – -0.30\n&lt;0.001\n\n\nObservations\n784\n\n\nR2 Nagelkerke\n0.024\n\n\nAIC\n2258.662\n\n\n\n\n\n\n\n\n\n5.5 Mixed-effects ordinal regression\n\n\n\n\n\n\nRecap: Mixed-effects models\n\n\n\n\n\nIf the data is nested according to some grouping factor with \\(1, ..., k\\) groups, we can let the intercept and/or slopes vary by group. For instance, recall the varying-intercept model:\n\\[\nY = \\alpha_{k} + \\beta_1X_{1} + \\beta_2X_{2} + ... + \\epsilon \\qquad \\alpha_{k} \\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha}^2).\n\\] In this case we also speak of random effects.\n\n\n\n\n# Convert to factor and determine ordering\nsurvey$rating &lt;- factor(survey$rating, ordered = TRUE, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n\n# Fit mixed model with random intercepts for \"verb\" and \"ParticipantID\"\nclm.2 &lt;- ordinal::clmm(rating ~ \n                    freq * routine +\n                    (1 | verb) +\n                    (1 | ParticipantID),\n                    data = survey, Hess=TRUE)\n\n# Model summary\nsummary(clm.2)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: rating ~ freq * routine + (1 | verb) + (1 | ParticipantID)\ndata:    survey\n\n link  threshold nobs logLik  AIC     niter     max.grad cond.H \n logit flexible  784  -991.65 2001.30 598(2386) 3.81e-04 1.5e+02\n\nRandom effects:\n Groups        Name        Variance Std.Dev.\n ParticipantID (Intercept) 2.0628   1.4363  \n verb          (Intercept) 0.9059   0.9518  \nNumber of groups:  ParticipantID 98,  verb 8 \n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \nfreqlo            -0.1567     0.2138  -0.733 0.463437    \nroutinelo         -0.7473     0.2103  -3.553 0.000381 ***\nfreqlo:routinelo  -0.1406     0.2981  -0.472 0.637046    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -1.3562     0.4020  -3.374\n2|3   0.2062     0.3988   0.517\n3|4   1.8329     0.4053   4.523\n4|5   3.8639     0.4401   8.780\n\n\n\ntab_model(clm.2, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, transform = NULL)\n\n\n\n \nrating\n\n\nPredictors\nLog-Odds\nstd. Error\nCI\np\n\n\n1|2\n-1.36\n0.40\n-2.14 – -0.57\n0.001\n\n\n2|3\n0.21\n0.40\n-0.58 – 0.99\n0.605\n\n\n3|4\n1.83\n0.41\n1.04 – 2.63\n&lt;0.001\n\n\n4|5\n3.86\n0.44\n3.00 – 4.73\n&lt;0.001\n\n\nfreq [lo]\n-0.16\n0.21\n-0.58 – 0.26\n0.463\n\n\nroutine [lo]\n-0.75\n0.21\n-1.16 – -0.34\n&lt;0.001\n\n\nfreq [lo] × routine [lo]\n-0.14\n0.30\n-0.72 – 0.44\n0.637\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 ParticipantID\n2.06\n\n\nτ00 verb\n0.91\n\n\nICC\n0.47\n\n\nN verb\n8\n\n\nN ParticipantID\n98\n\nObservations\n784\n\n\nMarginal R2 / Conditional R2\n0.028 / 0.489\n\n\nAIC\n2001.305\n\n\n\n\n\n\n\n\nShow the code\n# Extract random effects\nre_verb &lt;- ranef(clm.2)$verb\nre_participant &lt;- ranef(clm.2)$ParticipantID\n\n# Create dataframes for random effects\ndf_verb &lt;- data.frame(verb = rownames(re_verb), re = re_verb[,1])\ndf_participant &lt;- data.frame(ParticipantID = rownames(re_participant), re = re_participant[,1])\n\n# Get predictions for an average case\npred_avg &lt;- ggpredict(clm.2, terms = c(\"freq\", \"routine\"))\n\n\nYou are calculating adjusted predictions on the population-level (i.e.\n  `type = \"fixed\"`) for a *generalized* linear mixed model.\n  This may produce biased estimates due to Jensen's inequality. Consider\n  setting `bias_correction = TRUE` to correct for this bias.\n  See also the documentation of the `bias_correction` argument.\n\n\nSome of the focal terms are of type `character`. This may lead to\n  unexpected results. It is recommended to convert these variables to\n  factors before fitting the model.\n  The following variables are of type character: `freq`, `routine`\nSome of the focal terms are of type `character`. This may lead to\n  unexpected results. It is recommended to convert these variables to\n  factors before fitting the model.\n  The following variables are of type character: `freq`, `routine`\n\n\nShow the code\n# Add random effects to predictions\npred_verb &lt;- crossing(pred_avg, df_verb) %&gt;%\n  mutate(predicted = predicted + re)\n\npred_participant &lt;- crossing(pred_avg, df_participant) %&gt;%\n  mutate(predicted = predicted + re)\n\n# Create a horizontal dot plot for random effects of participants\np_caterpillar &lt;- ggplot(df_participant, aes(x = re, y = reorder(ParticipantID, re))) +\n  geom_point(size = 3, color = \"steelblue3\") +  # Dots representing the random effects\n  labs(title = \"Random Effects for Participants\", \n       x = \"Random Effect Estimate (log odds)\", \n       y = \"Participant ID\") +\n   geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey20\") +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank())  # Removes the gridlines for y-axis\n\np_caterpillar2 &lt;- ggplot(df_verb, aes(x = re, y = reorder(verb, re))) +\n  geom_point(size = 3, color = \"steelblue3\") +  # Dots representing the random effects\n  labs(title = \"Random Effects for Verbs\", \n       x = \"Random Effect Estimate (log odds)\", \n       y = \"Verb\") +\n   geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey20\") +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank())  # Removes the gridlines for y-axis\n\nggarrange(p_caterpillar, p_caterpillar2, ncol = 2, common.legend = TRUE, legend = \"right\")",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.5 Ordinal regression"
    ]
  },
  {
    "objectID": "chapters/06-ordinal_regression.html#generalised-additive-mixed-effects-models-gamms",
    "href": "chapters/06-ordinal_regression.html#generalised-additive-mixed-effects-models-gamms",
    "title": "Ordinal regression",
    "section": "6 Generalised Additive Mixed-effects Models (GAMMs)",
    "text": "6 Generalised Additive Mixed-effects Models (GAMMs)\n\n6.1 Suggested reading\nFor linguists:\n\nBaayen & Linke (2020)\n\nGeneral:\n\nHastie & Tibshirani (1991)\nWood (2006)\n\n\n\n6.2 Rationale\nA core assumption of Generalised Linear Models (GLMs) is a linear relationship between predictor(s) and response. If, however, one is interested in exploring potential non-linear trends without the risk of extreme overfitting, GAMs offer an elegant solution: Instead of relying on the linear sum of model coefficients, GAMs estimate more flexible smooth terms \\(f_k\\) for \\(k = 1, ..., p\\). For illustration, Equation 8 shows a linear additive model for a continuous target variable with \\(p\\) predictors.\n\\[\nY = \\beta_0 + \\sum\\limits_{k = 1}^p f_k(X_k)\n\\tag{8}\\]\n\n\n6.3 Application in R\n\n# Load libraries\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n\nlibrary(itsadug)\n\nLoading required package: plotfunctions\n\n\n\nAttaching package: 'plotfunctions'\n\n\nThe following object is masked from 'package:ggpubr':\n\n    get_palette\n\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\n\nLoaded package itsadug 2.4 (see 'help(\"itsadug\")' ).\n\n\n\nAttaching package: 'itsadug'\n\n\nThe following object is masked from 'package:ggeffects':\n\n    get_predictions\n\nlibrary(gratia)\n\n\nAttaching package: 'gratia'\n\n\nThe following object is masked from 'package:itsadug':\n\n    dispersion\n\n\nThe following object is masked from 'package:stringr':\n\n    boundary\n\n# Convert predictors to factors\nsurvey$ParticipantID &lt;- as.factor(survey$ParticipantID)\nsurvey$verb &lt;- as.factor(survey$verb)\n\n# Fit GAMM\ngam1 &lt;- bam(as.numeric(rating) ~ # treated as numeric term\n              freq + # linear term\n              routine + # linear term\n              s(ParticipantID, bs = \"re\") + # smooth term\n              s(verb, bs = \"re\"), # smooth term\n              data = survey, \n              family = ocat(R = 5) # number of ordinal categories\n            )\n\n# Model summary\nsummary(gam1)\n\n\nFamily: Ordered Categorical(-1,0.51,2.09,4.06) \nLink function: identity \n\nFormula:\nas.numeric(rating) ~ freq + routine + s(ParticipantID, bs = \"re\") + \n    s(verb, bs = \"re\")\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.3613     0.3880   0.931    0.352    \nfreqlo       -0.2195     0.1431  -1.534    0.125    \nroutinelo    -0.7752     0.1428  -5.430 7.79e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                    edf Ref.df      F p-value    \ns(ParticipantID) 77.654     97  4.677  &lt;2e-16 ***\ns(verb)           6.728      7 25.860  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDeviance explained = 27.3%\nfREML =   1631  Scale est. = 1         n = 784\n\n# Extract the intercepts for plotting\nthresh &lt;- gratia::theta(gam1) %&gt;% \n  tibble::as_tibble() %&gt;% \n  setNames(c(\"threshold\"))\n\n# Extract predictions for \"routine\"\nroutine_pred &lt;- ggpredict(gam1, terms = \"routine\")\n\nSome of the focal terms are of type `character`. This may lead to\n  unexpected results. It is recommended to convert these variables to\n  factors before fitting the model.\n  The following variables are of type character: `routine`\n\n# Plot predictions\nroutine_pred %&gt;% \n  ggplot(aes(x = x, y = predicted)) +\n  geom_point(col = \"steelblue\") +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = 0), col = \"steelblue\") +\n  geom_hline(data = thresh, aes(yintercept = threshold), linetype = \"dashed\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Extract random effects for \"verb\"\nverb_pred &lt;- ggpredict(gam1, terms = \"verb\")\n\n# Plot random effect\nverb_pred %&gt;% \n  ggplot(aes(x = x, y = predicted)) +\n  geom_point(col = \"steelblue\") +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = 0), col = \"steelblue\") +\n  geom_hline(data = thresh, aes(yintercept = threshold), linetype = \"dashed\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Extract random effects for \"ParticipantID\"\nsubj_pred &lt;- ggpredict(gam1, terms = \"ParticipantID\")\n\n# Plot random effect\nsubj_pred |&gt;\n  ggplot(aes(x = x, y = predicted)) +\n  geom_point(col = \"steelblue\") +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high, width = 0), col = \"steelblue\") +\n  #geom_line() +\n  geom_hline(data = thresh, aes(yintercept = threshold), linetype = \"dashed\") +\n  theme_minimal()",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.5 Ordinal regression"
    ]
  },
  {
    "objectID": "chapters/06-ordinal_regression.html#footnotes",
    "href": "chapters/06-ordinal_regression.html#footnotes",
    "title": "Ordinal regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe original dataset can be retrieved from Lelia Glass’s OSF repository: https://osf.io/t6zw5 [Last accessed: 27th September, 2024].↩︎",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.5 Ordinal regression"
    ]
  },
  {
    "objectID": "chapters/02-exploring_rstudio.html",
    "href": "chapters/02-exploring_rstudio.html",
    "title": "Exploring RStudio",
    "section": "",
    "text": "Once you’ve opened RStudio, you will see several empty panes similar to this:\n\n\n\n\n\n\n\nLet’s focus on the console window on the left. This is where we can directly communicate with R by entering “code”. The way this code is written follows certain arbitrary conventions – just like natural languages such as English or German. Here is an example in the R programming language:\n\nprint(\"Hello world!\")\n\n[1] \"Hello world!\"\n\n\nEntering this command into your console and pressing ENTER will display the sentence “Hello world!” in a new line.\nAs we can see, anything that R should understand as a simple sequence of letters or words must be enclosed by quotation marks \"...\". Anything inside them will be interpreted as a so-called string. Their counterpart are numbers or integers, as illustrated here:\n\n2 + 2 - 1\n\n[1] 3\n\n\nNaturally, you can use R for more sophisticated computations:\n\n\\(2 + (3 \\times 3)\\)\n\n2 + (3 * 3)\n\n[1] 11\n\n\n\\(\\sqrt{9}\\)\n\nsqrt(9)\n\n[1] 3\n\n\n\\(\\frac{16}{2^3}\\)\n\n16 / 2 ^ 3\n\n[1] 2\n\n\n\n\n\n\nWhile it is certainly useful to print text or numbers to your console, it may sometimes make more sense to (at least temporally) store them somewhere, so you can re-use them later. In fact, R gives us a way of storing data in virtual, container-like objects: variables. We can assign strings or numbers to a variable by using the assignment operator &lt;-.\nWhen you run the commands below, you will (hopefully) notice two items popping up in your Environment/Workspace tab in the top right corner.\n\ngreeting &lt;- \"Hello world!\"\n\nquick_math &lt;- 2 + 2 - 1\n\nIf we now want to display the content in the console, we can simply apply the print() function to the variable:\n\nprint(greeting)\n\n[1] \"Hello world!\"\n\nprint(quick_math)\n\n[1] 3\n\n\nWe can also embed variables in other statements. For example, let’s take the content of quick_math and multiply it with 2.\n\nhard_math &lt;- quick_math * 2\n\nprint(hard_math)\n\n[1] 6\n\n\n\n\n\nWorking with the console has a very plain, yet important disadvantage: Once we close RStudio, the console is wiped clean, erasing everything we’ve typed into it during our precious R session.\nThe remedy for this nuisance are scripts. Essentially, a script is the programmer’s equivalent of a Word document: It allows you to save all the code you’ve written to a file, which you can seamlessly continue working on the next time you open it.\n\nTo create a new R script you can either go to:\n\n“File” \\(\\rightarrow\\) “New” \\(\\rightarrow\\) “R Script” or …\nclick on the icon with the + sign and select “R Script” …\nor simply press Ctrl+Shift+N (MacOS: Cmd+Shift+N)\n\n\nDon’t forget to save your script with Ctrl+S (Cmd+S)! It is good practice to save your files regularly."
  },
  {
    "objectID": "chapters/02-exploring_rstudio.html#the-rstudio-interface",
    "href": "chapters/02-exploring_rstudio.html#the-rstudio-interface",
    "title": "Exploring RStudio",
    "section": "",
    "text": "Once you’ve opened RStudio, you will see several empty panes similar to this:\n\n\n\n\n\n\n\nLet’s focus on the console window on the left. This is where we can directly communicate with R by entering “code”. The way this code is written follows certain arbitrary conventions – just like natural languages such as English or German. Here is an example in the R programming language:\n\nprint(\"Hello world!\")\n\n[1] \"Hello world!\"\n\n\nEntering this command into your console and pressing ENTER will display the sentence “Hello world!” in a new line.\nAs we can see, anything that R should understand as a simple sequence of letters or words must be enclosed by quotation marks \"...\". Anything inside them will be interpreted as a so-called string. Their counterpart are numbers or integers, as illustrated here:\n\n2 + 2 - 1\n\n[1] 3\n\n\nNaturally, you can use R for more sophisticated computations:\n\n\\(2 + (3 \\times 3)\\)\n\n2 + (3 * 3)\n\n[1] 11\n\n\n\\(\\sqrt{9}\\)\n\nsqrt(9)\n\n[1] 3\n\n\n\\(\\frac{16}{2^3}\\)\n\n16 / 2 ^ 3\n\n[1] 2\n\n\n\n\n\n\nWhile it is certainly useful to print text or numbers to your console, it may sometimes make more sense to (at least temporally) store them somewhere, so you can re-use them later. In fact, R gives us a way of storing data in virtual, container-like objects: variables. We can assign strings or numbers to a variable by using the assignment operator &lt;-.\nWhen you run the commands below, you will (hopefully) notice two items popping up in your Environment/Workspace tab in the top right corner.\n\ngreeting &lt;- \"Hello world!\"\n\nquick_math &lt;- 2 + 2 - 1\n\nIf we now want to display the content in the console, we can simply apply the print() function to the variable:\n\nprint(greeting)\n\n[1] \"Hello world!\"\n\nprint(quick_math)\n\n[1] 3\n\n\nWe can also embed variables in other statements. For example, let’s take the content of quick_math and multiply it with 2.\n\nhard_math &lt;- quick_math * 2\n\nprint(hard_math)\n\n[1] 6\n\n\n\n\n\nWorking with the console has a very plain, yet important disadvantage: Once we close RStudio, the console is wiped clean, erasing everything we’ve typed into it during our precious R session.\nThe remedy for this nuisance are scripts. Essentially, a script is the programmer’s equivalent of a Word document: It allows you to save all the code you’ve written to a file, which you can seamlessly continue working on the next time you open it.\n\nTo create a new R script you can either go to:\n\n“File” \\(\\rightarrow\\) “New” \\(\\rightarrow\\) “R Script” or …\nclick on the icon with the + sign and select “R Script” …\nor simply press Ctrl+Shift+N (MacOS: Cmd+Shift+N)\n\n\nDon’t forget to save your script with Ctrl+S (Cmd+S)! It is good practice to save your files regularly."
  },
  {
    "objectID": "chapters/01-rqs.html",
    "href": "chapters/01-rqs.html",
    "title": "1.2 Research questions",
    "section": "",
    "text": "Example: I am studying the pronunciation of [r] in New York because I want to find out whether different social groups differ with respect to their pronunciation, in order to help my readers understand how social factors influence our linguistic behavior.\n\nTopic: I am studying...\nIndirect question: Because I want to find out what, why, how...\n(General) linguistic significance: ... in order to help the readers understand how, why, or whether...\n\n\nCf. Booth, Colomb, and Williams (2008): 45–48\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nA research question must be simple, specific, and doable. It is better to make a small contribution to a particular problem than to aim at covering a whole subfield of sociolinguistics.\nKeep a research question stable in your paper, even if you come across interesting new ideas as you work on it.\nThe research question must be an explicit part of your paper. Mention it in the introduction and keep referring back to it from time to time.\nKeep in mind that the research question will determine both methodology and data.\n\n\nCf. Hazen (2015): 9–10\n\n\n\n\n\n\nResearch questions must be complemented by a set of falsifiable hypotheses. These will be covered in-depth in the chapter on hypothesis testing. In short,\n\nHypothesis \\(H_1\\) predicts a specific relationship between a dependent variable and an independent variable. If \\(H_1\\) holds, studies often speak of a correlation (or association) between variables.\nits opposite, Hypothesis \\(H_0\\), describes the state of affairs where the predicted relationship between the variables does not hold.\n\nThe predictions made by the hypotheses are based on previous research. Here is an example:\nResearch question: I am studying the pronunciation of [r] in New York because I want to find out whether different social groups differ with respect to their pronunciation, in order to help my readers understand how social factors influence our linguistic behavior.\n\nHypothesis \\(H_1\\): There is a difference in the use of postvocalic /r/ between upper-class and lower-class speakers.\nHypothesis \\(H_0\\): There is no difference in the use of postvocalic /r/ between upper-class and lower-class speakers.\n\n\n\n\n\nExercise 1 Assess the following research questions!\n\nI will try to find the specific linguistic features which a Shetlander uses when speaking Scottish standard English, i.e., Shetland accent in Scottish Standard English.\nIn this paper, I am going to analyze the use of American and Indian dialect features among Indian immigrants to the US because I want to find out whether they correlate with their willingness to integrate into American society; this will show the degree to which linguistic adaptation depends on cultural orientation.\nIn this paper, I am going to address the question of how gender influences language.\nThe study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all.\nIn this paper, adverts by Coca-Cola and Pepsi will be analyzed for the use of verb forms.\n\n\n\nExercise 2 Develop valid research questions on the basis of the following linguistic variables:\n\nSpeakers in bilingual conversations are especially likely to code-switch when there is a significant change of topic.\nWomen use more tentative language in conversations than men.",
    "crumbs": [
      "Overview",
      "1. Theory",
      "1.2 Research Questions"
    ]
  },
  {
    "objectID": "chapters/01-rqs.html#what-makes-a-good-research-question",
    "href": "chapters/01-rqs.html#what-makes-a-good-research-question",
    "title": "1.2 Research questions",
    "section": "",
    "text": "Example: I am studying the pronunciation of [r] in New York because I want to find out whether different social groups differ with respect to their pronunciation, in order to help my readers understand how social factors influence our linguistic behavior.\n\nTopic: I am studying...\nIndirect question: Because I want to find out what, why, how...\n(General) linguistic significance: ... in order to help the readers understand how, why, or whether...\n\n\nCf. Booth, Colomb, and Williams (2008): 45–48\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nA research question must be simple, specific, and doable. It is better to make a small contribution to a particular problem than to aim at covering a whole subfield of sociolinguistics.\nKeep a research question stable in your paper, even if you come across interesting new ideas as you work on it.\nThe research question must be an explicit part of your paper. Mention it in the introduction and keep referring back to it from time to time.\nKeep in mind that the research question will determine both methodology and data.\n\n\nCf. Hazen (2015): 9–10\n\n\n\n\n\n\nResearch questions must be complemented by a set of falsifiable hypotheses. These will be covered in-depth in the chapter on hypothesis testing. In short,\n\nHypothesis \\(H_1\\) predicts a specific relationship between a dependent variable and an independent variable. If \\(H_1\\) holds, studies often speak of a correlation (or association) between variables.\nits opposite, Hypothesis \\(H_0\\), describes the state of affairs where the predicted relationship between the variables does not hold.\n\nThe predictions made by the hypotheses are based on previous research. Here is an example:\nResearch question: I am studying the pronunciation of [r] in New York because I want to find out whether different social groups differ with respect to their pronunciation, in order to help my readers understand how social factors influence our linguistic behavior.\n\nHypothesis \\(H_1\\): There is a difference in the use of postvocalic /r/ between upper-class and lower-class speakers.\nHypothesis \\(H_0\\): There is no difference in the use of postvocalic /r/ between upper-class and lower-class speakers.\n\n\n\n\n\nExercise 1 Assess the following research questions!\n\nI will try to find the specific linguistic features which a Shetlander uses when speaking Scottish standard English, i.e., Shetland accent in Scottish Standard English.\nIn this paper, I am going to analyze the use of American and Indian dialect features among Indian immigrants to the US because I want to find out whether they correlate with their willingness to integrate into American society; this will show the degree to which linguistic adaptation depends on cultural orientation.\nIn this paper, I am going to address the question of how gender influences language.\nThe study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all.\nIn this paper, adverts by Coca-Cola and Pepsi will be analyzed for the use of verb forms.\n\n\n\nExercise 2 Develop valid research questions on the basis of the following linguistic variables:\n\nSpeakers in bilingual conversations are especially likely to code-switch when there is a significant change of topic.\nWomen use more tentative language in conversations than men.",
    "crumbs": [
      "Overview",
      "1. Theory",
      "1.2 Research Questions"
    ]
  },
  {
    "objectID": "chapters/06-poisson_regression.html",
    "href": "chapters/06-poisson_regression.html",
    "title": "Poisson regression",
    "section": "",
    "text": "Winter (2020): Chapter 13\nBaguley (2012): Chapter 17.5",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.4 Poisson regression"
    ]
  },
  {
    "objectID": "chapters/06-poisson_regression.html#recommended-reading",
    "href": "chapters/06-poisson_regression.html#recommended-reading",
    "title": "Poisson regression",
    "section": "",
    "text": "Winter (2020): Chapter 13\nBaguley (2012): Chapter 17.5",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.4 Poisson regression"
    ]
  },
  {
    "objectID": "chapters/06-poisson_regression.html#preparation",
    "href": "chapters/06-poisson_regression.html#preparation",
    "title": "Poisson regression",
    "section": "2 Preparation",
    "text": "2 Preparation\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggeffects)\nlibrary(readxl)\nlibrary(car)\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n# Load datasets\nverbs &lt;- read_xlsx(\"../datasets/winter_2020_visual.xlsx\")",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.4 Poisson regression"
    ]
  },
  {
    "objectID": "chapters/06-poisson_regression.html#the-poisson-family",
    "href": "chapters/06-poisson_regression.html#the-poisson-family",
    "title": "Poisson regression",
    "section": "3 The Poisson family",
    "text": "3 The Poisson family\nFrequency data is ubiquitous in corpus linguistics. Given its numeric nature, it seems tempting to model such data using linear regression, but doing so is bound to cause problems. Partially owing to the fact that count data is always positive, the residuals more often than not deviate from normality and additionally display non-constant variance. The figures below illustrate these issues for a linear model fitted to simulated count data.\n\n\n\n\n\n\n\n\n\nA probability distribution that is much better equipped for this special case of discrete, yet numeric data is the Poisson distribution. Assuming a Poisson-distributed variable \\(X\\), its exact shape is determined by a single parameter, \\(\\lambda\\) (lambda), which is both its mean and its variance. In other words,\n\\[\nX \\sim Pois(\\lambda).\n\\tag{1}\\]\nIts probability mass function has a notable negative skew, which becomes less conspicuous for increasingly higher parameter values.",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.4 Poisson regression"
    ]
  },
  {
    "objectID": "chapters/06-poisson_regression.html#poisson-regression",
    "href": "chapters/06-poisson_regression.html#poisson-regression",
    "title": "Poisson regression",
    "section": "4 Poisson regression",
    "text": "4 Poisson regression\nRecall the linear regression ?@eq-multreg, where the dependent variable \\(Y\\) was modelled as a function of the linear sum of predictor terms \\(\\beta_pX_p\\) for \\(p\\) independent variables.\nThe Poisson model is very similar to the linear regression model, with the main differences being the logarithmic transformation of the response and the exclusion of the error term:\n\\[ \\ln(Y) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p.\n\\tag{2}\\]\nTo remove the logarithm and obtain the model output on a more intuitive scale, we simply exponentiate both sides:\n\\[ Y = e^{\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p}.\n\\tag{3}\\]\n\n4.1 Application in R\nThe data provided by Winter (2020) contains frequency data for hundreds of verbs (Freq column) as well as a variety of psycholinguistic ratings (Sight, Touch, Sound etc.), which were originally compiled by Lynott & Connell (2009).\n\nhead(verbs)\n\n# A tibble: 6 × 9\n  Word      DominantModality Sight Touch Sound  Taste  Smell Log10Freq  Freq\n  &lt;chr&gt;     &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 abrasive  Haptic            2.89 3.68  1.68  0.579  0.579      1.36     23\n2 absorbent Visual            4.14 3.14  0.714 0.476  0.476      0.903     8\n3 aching    Haptic            2.05 3.67  0.667 0.0476 0.0952     2.02    105\n4 acidic    Gustatory         2.19 1.14  0.476 4.19   2.90       1.04     11\n5 acrid     Olfactory         1.12 0.625 0.375 3      3.5        0.301     2\n6 adhesive  Haptic            3.67 4.33  1.19  0.905  1.76       1.67     47\n\n\nWe will examine of the effects of different sensory ratings on the frequency of a verb. When fitting the generalized linear model, it’s important to indicate the argument family = \"poisson\" to apply the correct (logarithmic) link function.\n\n# Fit Poisson regression model\nfreq.m1 &lt;- glm(Freq ~ Sight + Touch + Sound + Taste + Smell, data = verbs, family = \"poisson\")\n\n# Summarise model statistics\nsummary(freq.m1)\n\n\nCall:\nglm(formula = Freq ~ Sight + Touch + Sound + Taste + Smell, family = \"poisson\", \n    data = verbs)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.1201624  0.0118785  262.67   &lt;2e-16 ***\nSight        0.8726608  0.0024550  355.46   &lt;2e-16 ***\nTouch        0.1490792  0.0009801  152.10   &lt;2e-16 ***\nSound        0.1692741  0.0011662  145.15   &lt;2e-16 ***\nTaste        0.0790246  0.0019106   41.36   &lt;2e-16 ***\nSmell       -0.0963918  0.0019746  -48.82   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1890910  on 361  degrees of freedom\nResidual deviance: 1633858  on 356  degrees of freedom\n  (61 observations deleted due to missingness)\nAIC: 1636345\n\nNumber of Fisher Scoring iterations: 7\n\n\nThe model has identified numerous significant effects, i.e., \\(\\beta\\)-values that are significantly different from 0. The low standard errors hint at very robust estimates, resulting in 95% confidence intervals that are barely visible in the effect plots.\n\n# Get predicted values for each predictor\npred_sight &lt;- ggpredict(freq.m1, terms = \"Sight\")\npred_touch &lt;- ggpredict(freq.m1, terms = \"Touch\")\npred_sound &lt;- ggpredict(freq.m1, terms = \"Sound\")\npred_taste &lt;- ggpredict(freq.m1, terms = \"Taste\")\npred_smell &lt;- ggpredict(freq.m1, terms = \"Smell\")\n\n# For plotting individual predictions, simply use:\n# plot(pred_sight) \n\n# Combine all predictions into one data frame\npred_all &lt;- rbind(\n  data.frame(pred_sight, predictor = \"Sight\"),\n  data.frame(pred_touch, predictor = \"Touch\"),\n  data.frame(pred_sound, predictor = \"Sound\"),\n  data.frame(pred_taste, predictor = \"Taste\"),\n  data.frame(pred_smell, predictor = \"Smell\")\n)\n\n# Create faceted plot\nggplot(pred_all, aes(x = x, y = predicted)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.5) +\n  facet_wrap(~predictor, scales = \"free_x\") +\n  labs(x = \"Rating\", y = \"Predicted verb frequency\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n4.2 Interpretation\nAs we can see, verbs with higher Sight ratings (i.e., highly visual verbs) are associated with the greatest increase in frequency of occurrence. Given a one-unit increase in \\(X\\), we can obtain the percentage increase (or decrease, respectively) using the formula \\(100(e^b - 1)\\). The predictor Sight has an estimate of approximately \\(0.87\\) (\\(p&lt; 0.001\\), 95% CI [0.87, 0.88]), so the proportional increase is \\(100(e^{0.87} - 1) = 138.69\\%\\).\nBy contrast, Smell is a associated with a lower frequency: \\(100(e^{0.10} -1) = -9.51\\%\\) is the drop in frequency for each one-unit increase in smell ratings. In essence: Smelly verbs are unpopular!",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.4 Poisson regression"
    ]
  },
  {
    "objectID": "chapters/06-linear_regression.html",
    "href": "chapters/06-linear_regression.html",
    "title": "Linear regression",
    "section": "",
    "text": "For linguists:\n\nLevshina (2015: Chapter 7)\nWinter (2020: Chapter 4)\n\nGeneral:\n\nHeumann et al. (2022: Chapter 11)\nJames et al. (2021: Chapter 3)",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.1 Linear regression"
    ]
  },
  {
    "objectID": "chapters/06-linear_regression.html#recommended-reading",
    "href": "chapters/06-linear_regression.html#recommended-reading",
    "title": "Linear regression",
    "section": "",
    "text": "For linguists:\n\nLevshina (2015: Chapter 7)\nWinter (2020: Chapter 4)\n\nGeneral:\n\nHeumann et al. (2022: Chapter 11)\nJames et al. (2021: Chapter 3)",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.1 Linear regression"
    ]
  },
  {
    "objectID": "chapters/06-linear_regression.html#preparation",
    "href": "chapters/06-linear_regression.html#preparation",
    "title": "Linear regression",
    "section": "2 Preparation",
    "text": "2 Preparation\n\n# Load libraries\nlibrary(readxl) # for reading in Excel data\nlibrary(tidyverse) # data manipulation and visualisation framework\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom) # converting models to data frames\nlibrary(sjPlot) # exporting regression tables\n\nInstall package \"strengejacke\" from GitHub (`devtools::install_github(\"strengejacke/strengejacke\")`) to load all sj-packages at once!\n\nlibrary(effects) # plot marginal effects\n\nLoading required package: carData\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nlibrary(ggeffects) # generating predictions\nlibrary(car) # model diagnostics\n\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n# Load data\nELP &lt;- read_xlsx(\"../datasets/ELP.xlsx\")\n\n# Inspect data structure\nstr(ELP)\n\ntibble [880 × 5] (S3: tbl_df/tbl/data.frame)\n $ Word  : chr [1:880] \"rackets\" \"stepmother\" \"delineated\" \"swimmers\" ...\n $ Length: num [1:880] 7 10 10 8 6 5 5 8 8 6 ...\n $ Freq  : num [1:880] 0.96 4.24 0.04 1.49 1.06 3.33 0.1 0.06 0.43 5.41 ...\n $ POS   : chr [1:880] \"NN\" \"NN\" \"VB\" \"NN\" ...\n $ RT    : num [1:880] 791 693 960 771 882 ...",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.1 Linear regression"
    ]
  },
  {
    "objectID": "chapters/06-linear_regression.html#introduction",
    "href": "chapters/06-linear_regression.html#introduction",
    "title": "Linear regression",
    "section": "3 Introduction",
    "text": "3 Introduction\nConsider the distribution of the variables RT (reaction times) and Freq from the ELP (English Lexicon Project) dataset (Balota et al. 2007).\nWe will apply a \\(\\log\\)-transformation to both variables in order to even out the differences between extremely high and extremely low frequency counts (Winter 2020: 90-94).\n\nLog-transformedUntransformed\n\n\n\n\nCode\n# Log-transformed\nggplot(ELP, aes(x = log(RT))) +\n  geom_histogram() +\n  geom_vline(xintercept = mean(log(ELP$RT)), color = \"steelblue\") +\n  geom_vline(xintercept = mean(log(ELP$RT)), color = \"red\") +\n  theme_minimal() +\n  labs(\n    x = \"Reaction time (log)\"\n  ) +\n   annotate(\"text\", x = log(mean(ELP$RT)), y = 0, \n           label = \"mean\", \n           vjust = 1.5, hjust = -0.2, color = \"steelblue\", parse = TRUE) +\n  annotate(\"text\", x = log(mean(ELP$RT)) + -0.2, y = .7, \n           label = \"median\", \n           vjust = 1.5, hjust = -0.2, color = \"red\", parse = TRUE)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Skewed\nggplot(ELP, aes(x = RT)) +\n  geom_histogram() +\n  geom_vline(xintercept = mean(ELP$RT), color = \"steelblue\") +\n  geom_vline(xintercept = median(ELP$RT), color = \"red\") +\n  theme_minimal() +\n  labs(\n    x = \"Reaction time\"\n  ) +\nannotate(\"text\", x = mean(ELP$RT) + 10, y = 0, \n           label = \"mean\", \n           vjust = 1.5, hjust = -0.2, color = \"steelblue\", parse = TRUE) +\n  annotate(\"text\", x = median(ELP$RT) -175, y = .7, \n           label = \"median\", \n           vjust = 1.5, hjust = -0.2, color = \"red\", parse = TRUE)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\nWe are particularly interested in the relationship between reaction times RT and the (log-)frequency Freq of a lexical stimulus. What kind of pattern does the scatter plot below suggest?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome open questions\n\n\n\n\n\n\nCan word frequency help us explain variation in reaction times?\nIf it can, then how could we characterise the effect of word frequency? In other words, does it increase or decrease reaction times?\nWhat reaction times should we expect for new observations?\n\n\n\n\n\n3.1 A simple statistical model\nRT is the response or target that we wish to explain. We generically refer to the response as \\(Y\\).\nFreq is the feature, input, or predictor, which we will call \\(X\\).\nWe can thus summarise our preliminary and fairly general statistical model as\n\\[Y = f(X) + \\epsilon.\n\\tag{1}\\]\nThe term \\(f(X)\\) describes the contribution of \\(X\\) to the explanation of \\(Y\\). Since no model can explain everything perfectly, we expect there to be some degree of error \\(\\epsilon\\).",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.1 Linear regression"
    ]
  },
  {
    "objectID": "chapters/06-linear_regression.html#linear-regression",
    "href": "chapters/06-linear_regression.html#linear-regression",
    "title": "Linear regression",
    "section": "4 Linear regression",
    "text": "4 Linear regression\n\nLinear regression is a simple approach to supervised machine learning where the response variable is known.1\nIt assumes that the dependence of \\(Y\\) on \\(X\\) is linear, i.e., their relationship is a straight line.\nThis approach is suitable for numerical response variables. The predictors, however, can be either continuous or discrete.\n\nAlthough it may seem overly simplistic, linear regression is extremely useful both conceptually and practically.\n\n4.1 Model with a single predictor \\(X\\)\nThe simple linear model has the general form\n\\[ Y = \\beta_0 + \\beta_1X + \\epsilon.\n\\tag{2}\\]\n\nThe model parameters (or coefficients) \\(\\beta_0\\) and \\(\\beta_1\\) specify the functional relationship \\(f\\) between \\(Y\\) and \\(X\\).\nThe first parameter \\(\\beta_0\\) determines the intercept of the regression line, and \\(\\beta_1\\) indicates the slope.\nOnce again, \\(\\epsilon\\) captures the model error, which is equivalent to the sum of all distances of the data points from the regression line.\n\n\n\nWarning in geom_segment(aes(x = large_residual$x, xend = large_residual$x, : All aesthetics have length 1, but the data has 50 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\n\n\n\n\nApplying the model formula to our dataset, we get the following updated regression equation:\n\\[ \\text{Reaction time} = \\beta_0 + \\beta_1\\text{Frequency} + \\text{Model Error}. \\]\nBut how do we find the exact values of the intercept and the slope? In short: We can’t! We are dealing with population parameters and can, therefore, only provide an approximation of the true relationship between RT and Freq.\nTo reflect the tentative nature of the model coefficients, we use the hat symbol ^ (e.g., \\(\\hat{\\beta_0}\\)) to indicate estimations rather than true values. The estimation procedure requires training data, based on which the algorithm “learns” the relationship between \\(Y\\) and \\(X\\) (hence the term “Machine Learning”).\n\n\n\n\n\n\nHow exactly do you estimate the model parameters?\n\n\n\n\n\nThe most common way of estimating parameters for linear models is the Least Squares approach. In essence, the parameters are chosen such that the residual sum of squares, i.e., the sum of the differences between observed and predicted values, is as low as possible. In other words, we are trying to minimise the distances between the data points and the regression line.\nIt can be computed using the equivalence in Equation 3.\n\\[ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i- \\bar{y})}{\\sum_{i=1}^n(x_i- \\bar{x})^2}.\n\\tag{3}\\]\nWe can then obtain the intercept via Equation 4.\n\\[\n\\hat{\\beta_0}= \\bar{y}- \\hat{\\beta}_1\\bar{x}\n\\tag{4}\\]\n\n\n\nOnce we’ve fitted the model, we can then predict reaction times if we know the frequency of a lexical stimulus:\n\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x,\n\\tag{5}\\]\nwhere \\(\\hat{y}\\) indicates a prediction of \\(Y\\) on the basis of the predictor values \\(X = x\\).\n\n\n4.2 Application in R\nIn R, we can fit a linear model with the lm() function.\n\n# Fit linear model\nrt.lm1 = lm(log(RT) ~ log(Freq), data = ELP)\n\n# View model data\nsummary(rt.lm1)\n\n\ntab_model(rt.lm1, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, show.fstat = TRUE, digits = 3)\n\n\n\n\n \nlog(RT)\n\n\nPredictors\nEstimates\nstd. Error\nCI\np\n\n\n(Intercept)\n6.633\n0.004\n6.625 – 6.642\n&lt;0.001\n\n\nFreq [log]\n-0.049\n0.002\n-0.053 – -0.044\n&lt;0.001\n\n\nObservations\n880\n\n\nR2 / R2 adjusted\n0.357 / 0.356\n\n\nDeviance\n13.385\n\n\nAIC\n10534.255\n\n\n\n\n\n\n\nThe model statistics comprise the following elements:\n\n\n\n\n\n\nCall\n\n\n\n\n\ni.e., the model formula.\n\n\n\n\n\n\n\n\n\nResiduals\n\n\n\n\n\nThese indicate the difference between the observed values in the data set and the values predicted by the model (= the fitted values). These correspond to the error term \\(\\epsilon\\). The lower the residuals, the better the model describes the data.\n\n# Show fitted values (= predictions) for the first six observations\nhead(rt.lm1$fitted.values)\n\n       1        2        3        4        5        6 \n6.635345 6.563152 6.789806 6.613980 6.630529 6.574894 \n\n# Show deviation of the fitted values from the observed values\nhead(rt.lm1$residuals)\n\n          1           2           3           4           5           6 \n 0.03778825 -0.02277165  0.07759556  0.03387713  0.15222949 -0.10432670 \n\n\n\n\n\n\n\n\n\n\n\nCoefficients\n\n\n\n\n\nThe regression coefficients correspond to \\(\\hat{\\beta}_0\\) (“Intercept”) and \\(\\hat{\\beta}_1\\) (“log(Freq)”), respectively. The model shows that for a one-unit increase in log-frequency the log-reaction time decreases by approx. 0.05.\n\n# Convert coefficients to a tibble (= tidyverse-style data frame)\ntidy_model &lt;- tidy(rt.lm1)\n\ntidy_model\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   6.63     0.00429    1548.  0       \n2 log(Freq)    -0.0486   0.00220     -22.1 2.88e-86\n\n\n\n\n\n\n\n\n\n\n\n\\(p\\)-values and \\(t\\)-statistic\n\n\n\n\n\n\\(p\\)-values and \\(t\\)-statistic: Given the null hypothesis \\(H_0\\) that there is no correlation between log(RT) and log(Freq) (i.e., \\(H_0: \\beta_1 = 0\\)), a \\(p\\)-value lower than 0.05 indicates that \\(\\beta_1\\) considerably deviates from 0, thus providing evidence for the alternative hypothesis \\(H_1: \\beta_1 \\ne 0\\). Since \\(p &lt; 0.001\\), we can reject \\(H_0\\).\nThe \\(p\\)-value itself crucially depends on the \\(t\\)-statistic2, which measures “the number of standard deviations that \\(\\hat{\\beta_1}\\) is away from 0” (James et al. 2021: 67). The standard error (SE) reflects how much an estimated coefficient differs on average from the true values of \\(\\beta_0\\) and \\(\\beta_1\\). They can be used to compute the 95% confidence interval \\[[\\hat{\\beta}_1 - 2 \\cdot SE(\\hat{β}_1), \\hat{\\beta}_1 + 2 \\cdot SE(\\hat{\\beta}_1)].\n\\tag{6}\\]\nThe true value of the parameter \\(\\beta_1\\) lies within the specified range 95% of the time.\n\n# Compute confidence intervals for intercept and log(Freq)\ntidy_model_ci &lt;- tidy(rt.lm1, conf.int = TRUE)\n\ntidy_model_ci\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   6.63     0.00429    1548.  0          6.62      6.64  \n2 log(Freq)    -0.0486   0.00220     -22.1 2.88e-86  -0.0529   -0.0443\n\n\nThe estimated parameter for log(Freq), which is -0.049, thus has the 95% confidence interval [-0.053, -0.044].\n\n\n\n\n\n\n\n\n\nResidual standard error (RSE)\n\n\n\n\n\nThis is an estimation of the average deviation of the predictions from the observed values.\n\\[RSE = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n (y_i - \\hat{y_i}})^2\n\\tag{7}\\]\n\n\n\n\n\n\n\n\n\n\\(R^2\\)\n\n\n\n\n\nThe \\(R^2\\) score is important for assessing model fit because it “measures the proportion of variability in \\(Y\\) that can be explained using \\(X\\)” (James et al. 2021: 70), varying between 0 and 1.\n\\[R^2 = 1-\\frac{TSS}{RSS} = 1-\\frac{\\sum_{i=1}^n (y_i - \\hat{y_i})^2}{\\sum_{i=1}^n (y_i - \\bar{y_i})^2}\n\\tag{8}\\]\n\n\n\n\n\n\n\n\n\n\\(F\\)-statistic\n\n\n\n\n\nIt is used to measure the association between the dependent variable and the independent variable(s). Generally speaking, values greater than 1 indicate a possible correlation. A sufficiently low \\(p\\)-value suggests that the null hypothesis \\(H_0: \\beta_1 = 0\\) can be rejected. The \\(F\\) statistic is computed as shown below (cf. Agresti and Kateri 2022: 232) and follows an \\(F\\)-distribution with two different \\(df\\) values.\n\\[\nF = \\frac{(TSS - SSE) / p}{SSE / [n - (p + 1)]}\n\\tag{9}\\]\n\n\n\n\n\n4.3 Multiple linear regression\nIn multiple linear regression, more than one predictor variable is taken into account. For instance, modelling log(RT) as a function of log(Freq), POS and Length requires a more complex model of the form\n\\[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon.\n\\tag{10}\\]\nPredictions are then obtained via the formula\n\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2 + ... + \\hat{\\beta}_px_p.\n\\tag{11}\\]\n\n\n4.4 Application in R\nIn R, a multiple regression model is fitted as in the code example below:\n\n# Fit multiple regression model\nrt.lm2 &lt;- lm(log(RT) ~ log(Freq) + POS + Length, data = ELP)\n\n# View model statistics\nsummary(rt.lm2)\n\n\ntab_model(rt.lm2, show.se = TRUE, show.aic = TRUE, show.dev = TRUE, show.fstat = TRUE, digits = 3)\n\n\n\n\n \nlog(RT)\n\n\nPredictors\nEstimates\nstd. Error\nCI\np\n\n\n(Intercept)\n6.460\n0.017\n6.426 – 6.493\n&lt;0.001\n\n\nFreq [log]\n-0.038\n0.002\n-0.042 – -0.034\n&lt;0.001\n\n\nPOS [NN]\n-0.006\n0.010\n-0.026 – 0.014\n0.539\n\n\nPOS [VB]\n-0.035\n0.012\n-0.059 – -0.011\n0.004\n\n\nLength\n0.023\n0.002\n0.020 – 0.026\n&lt;0.001\n\n\nObservations\n880\n\n\nR2 / R2 adjusted\n0.478 / 0.476\n\n\nDeviance\n10.858\n\n\nAIC\n10356.130",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.1 Linear regression"
    ]
  },
  {
    "objectID": "chapters/06-linear_regression.html#visualising-regression-models",
    "href": "chapters/06-linear_regression.html#visualising-regression-models",
    "title": "Linear regression",
    "section": "5 Visualising regression models",
    "text": "5 Visualising regression models\n\nPlot coefficient estimates:\n\n\n\nCode\n# Tidy the model output\ntidy_model &lt;- tidy(rt.lm2, conf.int = TRUE)\n\n# Remove intercept\ntidy_model &lt;- tidy_model %&gt;% filter(term != \"(Intercept)\")\n\n# Create the coefficient plot\nggplot(tidy_model, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient Estimate\",\n    y = \"Predictor\",\n    title = \"Coefficient Estimates with Confidence Intervals\"\n  )\n\n\n\n\n\n\n\n\n\n\nPlot predictions:\n\n\n\nCode\nplot(ggeffect(rt.lm2, \"Freq\")) + geom_line(col = \"steelblue\") + labs(subtitle = \"Untransformed frequencies\", y = \"log(RT)\")\n\n\n\n\n\n\n\n\n\nCode\nplot(ggeffect(rt.lm2, \"Freq [log]\")) + geom_line(col = \"steelblue\") + labs(subtitle = \"Log-transformed frequencies\", y = \"log(RT)\")\n\n\nWarning: Removed 50 rows containing missing values or values outside the scale range\n(`geom_line()`).\nRemoved 50 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nCode\nplot(ggeffect(rt.lm2, \"POS\")) + geom_line(col = \"steelblue\") + labs(y = \"log(RT)\")\n\n\n\n\n\n\n\n\n\nCode\nplot(ggeffect(rt.lm2, \"Length\")) + geom_line(col = \"steelblue\") + labs(y = \"log(RT)\")",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.1 Linear regression"
    ]
  },
  {
    "objectID": "chapters/06-linear_regression.html#model-assumptions-and-diagnostics",
    "href": "chapters/06-linear_regression.html#model-assumptions-and-diagnostics",
    "title": "Linear regression",
    "section": "6 Model assumptions and diagnostics",
    "text": "6 Model assumptions and diagnostics\nAs a parametric method, linear regression makes numerous assumptions about the training data. It is, therefore, essential to run further tests to rule out possible violations. Among other things, the model assumptions include:\n\nA linear relationship between the response and the quantitative predictors: The residuals should not display a clear pattern. For this reason, it is recommended to use component residual plots (e.g., crPlot() from the car library) for the visual identification of potentially non-linear trends.\n\n\n\nCode\n# pink line = main tendency vs. blue line = slope coefficients;\n# some minor non-linearity can be observed\n\ncrPlot(rt.lm2, var = \"log(Freq)\") \n\n\n\n\n\n\n\n\n\nCode\ncrPlot(rt.lm2, var = \"POS\")\n\n\n\n\n\n\n\n\n\nCode\ncrPlot(rt.lm2, var = \"Length\") # potentially problematic\n\n\n\n\n\n\n\n\n\n\nNo heteroscedasticity (i.e, non-constant variance of error terms): Visually, a violation of this assumption becomes apparent if the residuals form a funnel-like shape. It is also possible to conduct a non-constant variance test ncvTest(): If it returns \\(p\\)-values &lt; 0.05, it suggests non-constant variance.\n\n\n\nCode\nplot(rt.lm2, which = 1)\n\n\n\n\n\n\n\n\n\nCode\nncvTest(rt.lm2) # significant, meaning that errors do not vary constantly\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 30.0101, Df = 1, p = 4.298e-08\n\n\n\nNo multicollinearity: Predictors should not be correlated with each other. In the model data, correlated variables have unusually high standard errors, thereby decreasing the explanatory power of both the coefficients and the model as a whole. Another diagnostic measure are variance inflation factors (VIF-scores); predictors with VIF scores &gt; 5 are potentially collinear. They can be computed using the vif() function.\n\n\n\nCode\nvif(rt.lm2) # vif &lt; 5 indicates that predictors are not correlated\n\n\n              GVIF Df GVIF^(1/(2*Df))\nlog(Freq) 1.150140  1        1.072446\nPOS       1.026925  2        1.006664\nLength    1.151054  1        1.072872\n\n\n\nNormally distributed residuals: The residuals should follow the normal distribution and be centered around 0:\n\n\\[\n\\epsilon \\sim N(0, \\sigma^2)\n\\tag{12}\\]\nUsually, a visual inspection using qqnorm() is sufficient, but the Shapiro-Wilke test shapiro.test() can also be run on the model residuals. Note that a \\(p\\)-value below 0.05 provides evidence for non-normality.\n\n\nCode\nplot(rt.lm2, which = 2)\n\n\n\n\n\n\n\n\n\nCode\nshapiro.test(residuals(rt.lm2)) # residuals are not normally distributed because p &lt; 0.05\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(rt.lm2)\nW = 0.99062, p-value = 2.139e-05\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBeside the points mentioned above, it is always recommend to examine the model with regard to\n\noutliers that might skew the regression estimates,\n\n\n\nCode\ninfluencePlot(rt.lm2, id.method = \"identify\")\n\n\nWarning in plot.window(...): \"id.method\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"id.method\" is not a graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"id.method\" is not\na graphical parameter\nWarning in axis(side = side, at = at, labels = labels, ...): \"id.method\" is not\na graphical parameter\n\n\nWarning in box(...): \"id.method\" is not a graphical parameter\n\n\nWarning in title(...): \"id.method\" is not a graphical parameter\n\n\nWarning in plot.xy(xy.coords(x, y), type = type, ...): \"id.method\" is not a\ngraphical parameter\n\n\n\n\n\n\n\n\n\n       StudRes         Hat       CookD\n16   0.4910868 0.027128347 0.001346143\n207 -0.7674391 0.030963411 0.003765568\n452  3.3366052 0.009338916 0.020749639\n498  3.5794954 0.004047706 0.010275907\n660  3.0847082 0.008750676 0.016638370\n\n\n\ninteractions, i.e., combined effects of predictors, and\n\n\n\nCode\nrt.lm.int &lt;- lm(log(RT) ~ log(Freq) + POS + Length + log(Freq):Length, data = ELP)\n\nsummary(rt.lm.int)\n\n\n\nCall:\nlm(formula = log(RT) ~ log(Freq) + POS + Length + log(Freq):Length, \n    data = ELP)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27355 -0.07723 -0.00651  0.06623  0.39971 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       6.4669304  0.0171836 376.343  &lt; 2e-16 ***\nlog(Freq)        -0.0243748  0.0062631  -3.892 0.000107 ***\nPOSNN            -0.0054973  0.0101366  -0.542 0.587735    \nPOSVB            -0.0344559  0.0120992  -2.848 0.004506 ** \nLength            0.0217620  0.0018007  12.085  &lt; 2e-16 ***\nlog(Freq):Length -0.0017681  0.0007606  -2.325 0.020319 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1111 on 874 degrees of freedom\nMultiple R-squared:  0.4816,    Adjusted R-squared:  0.4786 \nF-statistic: 162.4 on 5 and 874 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# ANOVA (analysis of variance)\n\n## Compare interaction model with main effects model\n\nanova(rt.lm.int, rt.lm2) # interaction term improves the model\n\n\nAnalysis of Variance Table\n\nModel 1: log(RT) ~ log(Freq) + POS + Length + log(Freq):Length\nModel 2: log(RT) ~ log(Freq) + POS + Length\n  Res.Df    RSS Df Sum of Sq     F  Pr(&gt;F)  \n1    874 10.792                             \n2    875 10.858 -1 -0.066726 5.404 0.02032 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\noverfitting, which results in poor model performance outside the training data.\n\n\n\nCode\nlibrary(\"rms\")\n\n\nLoading required package: Hmisc\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\n\nAttaching package: 'rms'\n\n\nThe following objects are masked from 'package:car':\n\n    Predict, vif\n\n\nCode\n# Refit the model with ols(), which is equivalent to lm()\nols.rt &lt;- ols(log(RT) ~ log(Freq) + POS + Length, data = ELP, x = TRUE, y = TRUE)\n\n# Cross-validate\nols.val &lt;- validate(ols.rt, bw = TRUE, B = 200) # Perform 200 random resampling iterations (= bootstrapping); compare model performance on training vs. test (= new) data. The slope optimism should be below 0.05 to rule out overfitting.\n\n\n\n        Backwards Step-down - Original Model\n\nNo Factors Deleted\n\nFactors in Final Model\n\n[1] Freq   POS    Length\n\n\nCode\nols.val[,1:5] # The model does not overfit.\n\n\n          index.orig  training       test      optimism index.corrected\nR-square  0.47839094 0.4796005 0.47569234  0.0039081611      0.47448277\nMSE       0.01233904 0.0122713 0.01240287 -0.0001315717      0.01247061\ng         0.11875152 0.1187133 0.11856458  0.0001486979      0.11860283\nIntercept 0.00000000 0.0000000 0.01917023 -0.0191702320      0.01917023\nSlope     1.00000000 1.0000000 0.99706609  0.0029339079      0.99706609",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.1 Linear regression"
    ]
  },
  {
    "objectID": "chapters/06-linear_regression.html#footnotes",
    "href": "chapters/06-linear_regression.html#footnotes",
    "title": "Linear regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf the response variable is unknown or irrelevant, we speak of unsupervised machine learning. Unsupervised models are mostly concerned with finding patterns in high-dimensional datasets with dozens or even hundreds of variables.↩︎\nIf the response variable is unknown or irrelevant, we speak of unsupervised machine learning. Unsupervised models are mostly concerned with finding patterns in high-dimensional datasets with dozens or even hundreds of variables.↩︎",
    "crumbs": [
      "Overview",
      "6. Statistical Modelling",
      "6.1 Linear regression"
    ]
  },
  {
    "objectID": "Exploring_RStudio.html",
    "href": "Exploring_RStudio.html",
    "title": "Exploring RStudio",
    "section": "",
    "text": "Once you’ve opened RStudio, you will see several empty panes similar to this:\n\n\n\n\n\n\n\nLet’s focus on the console window on the left. This is where we can directly communicate with R by entering “code”. The way this code is written follows certain arbitrary conventions – just like natural languages such as English or German. Here is an example in the R programming language:\n\nprint(\"Hello world!\")\n\n[1] \"Hello world!\"\n\n\nEntering this command into your console and pressing ENTER will display the sentence “Hello world!” in a new line.\nAs we can see, anything that R should understand as a simple sequence of letters or words must be enclosed by quotation marks \"...\". Anything inside them will be interpreted as a so-called string. Their counterpart are numbers or integers, as illustrated here:\n\n2 + 2 - 1\n\n[1] 3\n\n\nNaturally, you can use R for more sophisticated computations:\n\n\\(2 + (3 \\times 3)\\)\n\n2 + (3 * 3)\n\n[1] 11\n\n\n\\(\\sqrt{9}\\)\n\nsqrt(9)\n\n[1] 3\n\n\n\\(\\frac{16}{2^3}\\)\n\n16 / 2 ^ 3\n\n[1] 2\n\n\n\n\n\n\nWhile it is certainly useful to print text or numbers to your console, it may sometimes make more sense to (at least temporally) store them somewhere, so you can re-use them later. In fact, R gives us a way of storing data in virtual, container-like objects: variables. We can assign strings or numbers to a variable by using the assignment operator &lt;-.\nWhen you run the commands below, you will (hopefully) notice two items popping up in your Environment/Workspace tab in the top right corner.\n\ngreeting &lt;- \"Hello world!\"\n\nquick_math &lt;- 2 + 2 - 1\n\nIf we now want to display the content in the console, we can simply apply the print() function to the variable:\n\nprint(greeting)\n\n[1] \"Hello world!\"\n\nprint(quick_math)\n\n[1] 3\n\n\nWe can also embed variables in other statements. For example, let’s take the content of quick_math and multiply it with 2.\n\nhard_math &lt;- quick_math * 2\n\nprint(hard_math)\n\n[1] 6\n\n\n\n\n\nWorking with the console has a very plain, yet important disadvantage: Once we close RStudio, the console is wiped clean, erasing everything we’ve typed into it during our precious R session.\nThe remedy for this nuisance are scripts. Essentially, a script is the programmer’s equivalent of a Word document: It allows you to save all the code you’ve written to a file, which you can seamlessly continue working on the next time you open it.\n\nTo create a new R script you can either go to:\n\n“File” \\(\\rightarrow\\) “New” \\(\\rightarrow\\) “R Script” or …\nclick on the icon with the + sign and select “R Script” …\nor simply press Ctrl+Shift+N (MacOS: Cmd+Shift+N)\n\n\nDon’t forget to save your script with Ctrl+S (Cmd+S)! It is good practice to save your files regularly."
  },
  {
    "objectID": "Exploring_RStudio.html#the-rstudio-interface",
    "href": "Exploring_RStudio.html#the-rstudio-interface",
    "title": "Exploring RStudio",
    "section": "",
    "text": "Once you’ve opened RStudio, you will see several empty panes similar to this:\n\n\n\n\n\n\n\nLet’s focus on the console window on the left. This is where we can directly communicate with R by entering “code”. The way this code is written follows certain arbitrary conventions – just like natural languages such as English or German. Here is an example in the R programming language:\n\nprint(\"Hello world!\")\n\n[1] \"Hello world!\"\n\n\nEntering this command into your console and pressing ENTER will display the sentence “Hello world!” in a new line.\nAs we can see, anything that R should understand as a simple sequence of letters or words must be enclosed by quotation marks \"...\". Anything inside them will be interpreted as a so-called string. Their counterpart are numbers or integers, as illustrated here:\n\n2 + 2 - 1\n\n[1] 3\n\n\nNaturally, you can use R for more sophisticated computations:\n\n\\(2 + (3 \\times 3)\\)\n\n2 + (3 * 3)\n\n[1] 11\n\n\n\\(\\sqrt{9}\\)\n\nsqrt(9)\n\n[1] 3\n\n\n\\(\\frac{16}{2^3}\\)\n\n16 / 2 ^ 3\n\n[1] 2\n\n\n\n\n\n\nWhile it is certainly useful to print text or numbers to your console, it may sometimes make more sense to (at least temporally) store them somewhere, so you can re-use them later. In fact, R gives us a way of storing data in virtual, container-like objects: variables. We can assign strings or numbers to a variable by using the assignment operator &lt;-.\nWhen you run the commands below, you will (hopefully) notice two items popping up in your Environment/Workspace tab in the top right corner.\n\ngreeting &lt;- \"Hello world!\"\n\nquick_math &lt;- 2 + 2 - 1\n\nIf we now want to display the content in the console, we can simply apply the print() function to the variable:\n\nprint(greeting)\n\n[1] \"Hello world!\"\n\nprint(quick_math)\n\n[1] 3\n\n\nWe can also embed variables in other statements. For example, let’s take the content of quick_math and multiply it with 2.\n\nhard_math &lt;- quick_math * 2\n\nprint(hard_math)\n\n[1] 6\n\n\n\n\n\nWorking with the console has a very plain, yet important disadvantage: Once we close RStudio, the console is wiped clean, erasing everything we’ve typed into it during our precious R session.\nThe remedy for this nuisance are scripts. Essentially, a script is the programmer’s equivalent of a Word document: It allows you to save all the code you’ve written to a file, which you can seamlessly continue working on the next time you open it.\n\nTo create a new R script you can either go to:\n\n“File” \\(\\rightarrow\\) “New” \\(\\rightarrow\\) “R Script” or …\nclick on the icon with the + sign and select “R Script” …\nor simply press Ctrl+Shift+N (MacOS: Cmd+Shift+N)\n\n\nDon’t forget to save your script with Ctrl+S (Cmd+S)! It is good practice to save your files regularly."
  },
  {
    "objectID": "Basics.html",
    "href": "Basics.html",
    "title": "An example from sociolinguistics",
    "section": "",
    "text": "New quotatives in American English: be like, go, be all:\n\n“I was like ‘well if it’s not leaking, I don’t care.’”\n“Do you, I asked I was running out and I go, ‘you’re trying to deliver a pizza here?’”\n“My sister’s all ‘excuse me would you mind if I gave you, if I want your autograph.’”\n\n\n\n\n\nPrevious results with regard to age: Be like is known to be preferred by younger speakers.\nPrevious results with regard to sex are inconclusive.\n\n\n\n\n\n“The study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all” (Barbieri 2007: 26).\n\n\n\n\n\nChoice of corpus: 450,000 words taken from the Conversation component of the Longman Spoken and Written English Corpus (all files where the age and gender of speakers were available), i.e., about 50 hours of casual conversation coded for speakers’ age and sex.\nData extraction: All forms of be like, be all, go, say extracted using corpus linguistic software; data entered into spreadsheet software; word counts of texts added via Delphi program.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuotative\nAge group\nMale use (per 100,000 words)\nFemale use (per 100,000 words)\n\n\n\n\nsay\n16–26\n115\n149\n\n\n\n27–40\n101\n117\n\n\n\n41–54\n55\n214\n\n\n\n55+\n67\n433\n\n\nbe like\n16–26\n32\n329\n\n\n\n27–40\n66\n25\n\n\n\n41–54\n-\n1\n\n\n\n55+\n-\n19\n\n\ngo\n16–26\n25\n113\n\n\n\n27–40\n61\n18\n\n\n\n41–54\n-\n15\n\n\n\n55+\n-\n7\n\n\nbe all\n16–26\n14\n33\n\n\n\n\n\n\n\n\nReproduction of Barbieri’s Figure 5 ‘Proportional quotative use by men and women aged 16–26 and 27–40’\n\n\n\n\n\n\n\n\n“In sum, the patterns of proportional use and the results of the statistical analyses show that there are striking differences in the way that men and women under the age of forty use quotative verbs. In contrast, in conforming to the traditional quotative say, men and women over forty displayed similar behavior” (Barbieri 2007: 38–39).\n\n\n\n\n\n“In favoring be like over other quotatives, men in their late 20s and in their 30s display an ‘affinity’ with slightly younger women, women in their early 20s, rather than with women of their same age” (Barbieri 2007: 41)."
  },
  {
    "objectID": "Basics.html#barbierioldermenyounger2007-older-men-and-younger-women",
    "href": "Basics.html#barbierioldermenyounger2007-older-men-and-younger-women",
    "title": "An example from sociolinguistics",
    "section": "",
    "text": "New quotatives in American English: be like, go, be all:\n\n“I was like ‘well if it’s not leaking, I don’t care.’”\n“Do you, I asked I was running out and I go, ‘you’re trying to deliver a pizza here?’”\n“My sister’s all ‘excuse me would you mind if I gave you, if I want your autograph.’”\n\n\n\n\n\nPrevious results with regard to age: Be like is known to be preferred by younger speakers.\nPrevious results with regard to sex are inconclusive.\n\n\n\n\n\n“The study aims to contribute to the investigation of the controversial question of the effect of speaker’s age and sex on quotative use in American English, focusing on the newer quotatives be like, go, be all” (Barbieri 2007: 26).\n\n\n\n\n\nChoice of corpus: 450,000 words taken from the Conversation component of the Longman Spoken and Written English Corpus (all files where the age and gender of speakers were available), i.e., about 50 hours of casual conversation coded for speakers’ age and sex.\nData extraction: All forms of be like, be all, go, say extracted using corpus linguistic software; data entered into spreadsheet software; word counts of texts added via Delphi program.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuotative\nAge group\nMale use (per 100,000 words)\nFemale use (per 100,000 words)\n\n\n\n\nsay\n16–26\n115\n149\n\n\n\n27–40\n101\n117\n\n\n\n41–54\n55\n214\n\n\n\n55+\n67\n433\n\n\nbe like\n16–26\n32\n329\n\n\n\n27–40\n66\n25\n\n\n\n41–54\n-\n1\n\n\n\n55+\n-\n19\n\n\ngo\n16–26\n25\n113\n\n\n\n27–40\n61\n18\n\n\n\n41–54\n-\n15\n\n\n\n55+\n-\n7\n\n\nbe all\n16–26\n14\n33\n\n\n\n\n\n\n\n\nReproduction of Barbieri’s Figure 5 ‘Proportional quotative use by men and women aged 16–26 and 27–40’\n\n\n\n\n\n\n\n\n“In sum, the patterns of proportional use and the results of the statistical analyses show that there are striking differences in the way that men and women under the age of forty use quotative verbs. In contrast, in conforming to the traditional quotative say, men and women over forty displayed similar behavior” (Barbieri 2007: 38–39).\n\n\n\n\n\n“In favoring be like over other quotatives, men in their late 20s and in their 30s display an ‘affinity’ with slightly younger women, women in their early 20s, rather than with women of their same age” (Barbieri 2007: 41)."
  },
  {
    "objectID": "Basics.html#principles-of-empirical-linguistics",
    "href": "Basics.html#principles-of-empirical-linguistics",
    "title": "An example from sociolinguistics",
    "section": "2 Principles of Empirical Linguistics",
    "text": "2 Principles of Empirical Linguistics\n\nObjectivity — Independence from researchers or devices (→ replicability!)\nReliability — Studies should be replicable.\nValidity — A study must actually address the problem formulated in the research question."
  },
  {
    "objectID": "Basics.html#exercises",
    "href": "Basics.html#exercises",
    "title": "An example from sociolinguistics",
    "section": "3 Exercises",
    "text": "3 Exercises\n\nExercise 1 As you read through the sections above, think about what might go wrong at each stage of the study!\n\n\nExercise 2 Read another sociolinguistic study (e.g., Unuabonah and Gut 2018). As you read, identify similar key sections or “building blocks” used in this study (e.g., introduction, research question etc.). What purpose does each section serve in the study’s overall structure?"
  }
]